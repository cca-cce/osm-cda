<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.55">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>object recognition – osm-pyws</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./pyws01-0-getting-started.html">workshop</a></li><li class="breadcrumb-item"><a href="./pyws04-0-image-analysis.html">part 4: image analysis</a></li><li class="breadcrumb-item"><a href="./pyws04-2-image-analysis.html">object recognition</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">osm-pyws</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">start</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">about</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">workshop</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./pyws01-0-getting-started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">part 1: getting started</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pyws01-1-getting-started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">python environments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pyws01-2-getting-started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">basic python syntax</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./pyws02-0-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">part 2: data analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pyws02-1-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">dataframe manipulation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pyws02-2-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">result visualizations</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./pyws03-0-text-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">part 3: text analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pyws03-1-text-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">reading text content</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pyws03-2-text-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">text tokenization</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./pyws04-0-image-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">part 4: image analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pyws04-1-image-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">reading image content</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pyws04-2-image-analysis.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">object recognition</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./pyws05-0-data-collection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">part 5: data collection</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pyws05-1-data-collection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">web scraping</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pyws05-2-data-collection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">web experiments</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resources.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">resources</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#images-human-content" id="toc-images-human-content" class="nav-link active" data-scroll-target="#images-human-content">images, human content</a>
  <ul class="collapse">
  <li><a href="#import-opencv-library" id="toc-import-opencv-library" class="nav-link" data-scroll-target="#import-opencv-library">Import OpenCV Library</a></li>
  <li><a href="#load-the-haar-cascade-classifier-for-face-detection" id="toc-load-the-haar-cascade-classifier-for-face-detection" class="nav-link" data-scroll-target="#load-the-haar-cascade-classifier-for-face-detection">Load the Haar Cascade Classifier for Face Detection</a></li>
  <li><a href="#load-the-image" id="toc-load-the-image" class="nav-link" data-scroll-target="#load-the-image">Load the Image</a></li>
  <li><a href="#convert-the-image-to-grayscale" id="toc-convert-the-image-to-grayscale" class="nav-link" data-scroll-target="#convert-the-image-to-grayscale">Convert the Image to Grayscale</a></li>
  <li><a href="#detect-faces-in-the-image" id="toc-detect-faces-in-the-image" class="nav-link" data-scroll-target="#detect-faces-in-the-image">Detect Faces in the Image</a></li>
  <li><a href="#draw-rectangles-around-detected-faces" id="toc-draw-rectangles-around-detected-faces" class="nav-link" data-scroll-target="#draw-rectangles-around-detected-faces">Draw Rectangles Around Detected Faces</a></li>
  <li><a href="#display-the-image-with-detected-faces" id="toc-display-the-image-with-detected-faces" class="nav-link" data-scroll-target="#display-the-image-with-detected-faces">Display the Image with Detected Faces</a></li>
  </ul></li>
  <li><a href="#image-classification" id="toc-image-classification" class="nav-link" data-scroll-target="#image-classification">image classification</a>
  <ul class="collapse">
  <li><a href="#install-the-deepface-package" id="toc-install-the-deepface-package" class="nav-link" data-scroll-target="#install-the-deepface-package">Install the DeepFace Package</a></li>
  <li><a href="#import-required-libraries" id="toc-import-required-libraries" class="nav-link" data-scroll-target="#import-required-libraries">Import Required Libraries</a></li>
  <li><a href="#load-the-image-1" id="toc-load-the-image-1" class="nav-link" data-scroll-target="#load-the-image-1">Load the Image</a></li>
  <li><a href="#convert-the-image-to-grayscale-1" id="toc-convert-the-image-to-grayscale-1" class="nav-link" data-scroll-target="#convert-the-image-to-grayscale-1">Convert the Image to Grayscale</a></li>
  <li><a href="#load-the-haar-cascade-classifier-for-face-detection-1" id="toc-load-the-haar-cascade-classifier-for-face-detection-1" class="nav-link" data-scroll-target="#load-the-haar-cascade-classifier-for-face-detection-1">Load the Haar Cascade Classifier for Face Detection</a></li>
  <li><a href="#detect-faces-in-the-image-1" id="toc-detect-faces-in-the-image-1" class="nav-link" data-scroll-target="#detect-faces-in-the-image-1">Detect Faces in the Image</a></li>
  <li><a href="#analyze-emotions-for-each-detected-face" id="toc-analyze-emotions-for-each-detected-face" class="nav-link" data-scroll-target="#analyze-emotions-for-each-detected-face">Analyze Emotions for Each Detected Face</a></li>
  <li><a href="#display-the-image-with-detected-faces-and-emotions" id="toc-display-the-image-with-detected-faces-and-emotions" class="nav-link" data-scroll-target="#display-the-image-with-detected-faces-and-emotions">Display the Image with Detected Faces and Emotions</a></li>
  <li><a href="#other-natural-objects" id="toc-other-natural-objects" class="nav-link" data-scroll-target="#other-natural-objects">other natural objects</a></li>
  </ul></li>
  <li><a href="#object-recognition" id="toc-object-recognition" class="nav-link" data-scroll-target="#object-recognition">object recognition</a></li>
  <li><a href="#image-video-to-text" id="toc-image-video-to-text" class="nav-link" data-scroll-target="#image-video-to-text">image, video to text</a></li>
  <li><a href="#pixtral-cutting-edge.." id="toc-pixtral-cutting-edge.." class="nav-link" data-scroll-target="#pixtral-cutting-edge..">pixtral, cutting edge..</a></li>
  <li><a href="#testing..-on-your-own-risk" id="toc-testing..-on-your-own-risk" class="nav-link" data-scroll-target="#testing..-on-your-own-risk">testing.. on your own risk :)</a>
  <ul class="collapse">
  <li><a href="#extract-frames-from-the-video" id="toc-extract-frames-from-the-video" class="nav-link" data-scroll-target="#extract-frames-from-the-video">1. Extract Frames from the Video</a></li>
  <li><a href="#use-an-image-captioning-model-to-generate-descriptions" id="toc-use-an-image-captioning-model-to-generate-descriptions" class="nav-link" data-scroll-target="#use-an-image-captioning-model-to-generate-descriptions">2. Use an Image Captioning Model to Generate Descriptions</a></li>
  <li><a href="#combine-captions-into-a-coherent-summary" id="toc-combine-captions-into-a-coherent-summary" class="nav-link" data-scroll-target="#combine-captions-into-a-coherent-summary">3. Combine Captions into a Coherent Summary</a></li>
  <li><a href="#complete-code-example" id="toc-complete-code-example" class="nav-link" data-scroll-target="#complete-code-example">Complete Code Example</a></li>
  <li><a href="#additional-considerations" id="toc-additional-considerations" class="nav-link" data-scroll-target="#additional-considerations">Additional Considerations</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./pyws01-0-getting-started.html">workshop</a></li><li class="breadcrumb-item"><a href="./pyws04-0-image-analysis.html">part 4: image analysis</a></li><li class="breadcrumb-item"><a href="./pyws04-2-image-analysis.html">object recognition</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">object recognition</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<ul>
<li>code examples <a href="https://huggingface.co/tasks" target="_blank">huggingface</a></li>
<li>download <a href="pyws04-2-image-analysis.ipynb" target="_blank">jupyter notebook</a></li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get github repo.. run inside google colab</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">#!git clone https://github.com/cca-cce/osm-cca-cv.git</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install dependencies</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">#!pip install -q timm</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="images-human-content" class="level2">
<h2 class="anchored" data-anchor-id="images-human-content">images, human content</h2>
<section id="import-opencv-library" class="level3">
<h3 class="anchored" data-anchor-id="import-opencv-library">Import OpenCV Library</h3>
<p><em>We start by importing the OpenCV library (<code>cv2</code>), which provides functions for image processing and computer vision tasks.</em></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="load-the-haar-cascade-classifier-for-face-detection" class="level3">
<h3 class="anchored" data-anchor-id="load-the-haar-cascade-classifier-for-face-detection">Load the Haar Cascade Classifier for Face Detection</h3>
<p><em>In this step, we load the pre-trained Haar cascade classifier for frontal face detection. OpenCV comes with these classifiers stored in the <code>haarcascades</code> directory. We use the <code>cv2.CascadeClassifier</code> function to load the classifier file <code>haarcascade_frontalface_default.xml</code>.</em></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Haar cascade classifier for face detection</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>face_cascade <span class="op">=</span> cv2.CascadeClassifier(cv2.data.haarcascades <span class="op">+</span> <span class="st">'haarcascade_frontalface_default.xml'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="load-the-image" class="level3">
<h3 class="anchored" data-anchor-id="load-the-image">Load the Image</h3>
<p><em>We load the image from the specified file path using <code>cv2.imread()</code>. This function reads the image into a NumPy array in BGR color format.</em></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the image</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">'/content/osm-cca-cv/res/img/image_prefix-003.png'</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> cv2.imread(image_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="convert-the-image-to-grayscale" class="level3">
<h3 class="anchored" data-anchor-id="convert-the-image-to-grayscale">Convert the Image to Grayscale</h3>
<p><em>Face detection algorithms often perform better on grayscale images. We convert the loaded color image to grayscale using <code>cv2.cvtColor()</code> with the <code>cv2.COLOR_BGR2GRAY</code> conversion code.</em></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the image to grayscale</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>gray <span class="op">=</span> cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="detect-faces-in-the-image" class="level3">
<h3 class="anchored" data-anchor-id="detect-faces-in-the-image">Detect Faces in the Image</h3>
<p><em>We use the <code>detectMultiScale()</code> method of the Haar cascade classifier to detect faces in the grayscale image. This method returns a list of rectangles, where each rectangle contains the coordinates <code>(x, y)</code> and the width and height <code>(w, h)</code> of a detected face. The parameters <code>scaleFactor</code>, <code>minNeighbors</code>, and <code>minSize</code> control the accuracy and speed of the detection.</em></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Detect faces in the image</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>faces <span class="op">=</span> face_cascade.detectMultiScale(</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    gray,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    scaleFactor<span class="op">=</span><span class="fl">1.1</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    minNeighbors<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    minSize<span class="op">=</span>(<span class="dv">30</span>, <span class="dv">30</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="draw-rectangles-around-detected-faces" class="level3">
<h3 class="anchored" data-anchor-id="draw-rectangles-around-detected-faces">Draw Rectangles Around Detected Faces</h3>
<p><em>We iterate over the list of detected faces and draw rectangles around each face on the original image using <code>cv2.rectangle()</code>. The rectangle is drawn using the top-left corner <code>(x, y)</code> and the bottom-right corner <code>(x + w, y + h)</code>. We set the color to blue <code>(255, 0, 0)</code> and the thickness to <code>2</code> pixels.</em></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw rectangles around the detected faces</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (x, y, w, h) <span class="kw">in</span> faces:</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    cv2.rectangle(image, (x, y), (x <span class="op">+</span> w, y <span class="op">+</span> h), (<span class="dv">255</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="display-the-image-with-detected-faces" class="level3">
<h3 class="anchored" data-anchor-id="display-the-image-with-detected-faces">Display the Image with Detected Faces</h3>
<p><em>Finally, we display the image with the detected faces highlighted. Since we are in a Google Colab environment, we use <code>cv2_imshow()</code> from <code>google.colab.patches</code> to display the image within the notebook.</em></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image with detected faces</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> google.colab.patches <span class="im">import</span> cv2_imshow</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>cv2_imshow(image)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Summary:</strong></p>
<ul>
<li><strong>Import OpenCV</strong>: Provides functions for image processing.</li>
<li><strong>Load Haar Cascade Classifier</strong>: Loads a pre-trained model for face detection.</li>
<li><strong>Load Image</strong>: Reads the image file into a NumPy array.</li>
<li><strong>Convert to Grayscale</strong>: Prepares the image for detection by simplifying color channels.</li>
<li><strong>Detect Faces</strong>: Uses the classifier to find faces in the image.</li>
<li><strong>Draw Rectangles</strong>: Highlights detected faces on the original image.</li>
<li><strong>Display Image</strong>: Shows the processed image within the notebook.</li>
</ul>
<hr>
<p><em>By following these steps, you can use OpenCV’s Haar cascades to detect and localize faces in an image. This process involves loading the necessary libraries and classifiers, preprocessing the image, performing face detection, and visualizing the results.</em></p>
</section>
</section>
<section id="image-classification" class="level2">
<h2 class="anchored" data-anchor-id="image-classification">image classification</h2>
<section id="install-the-deepface-package" class="level3">
<h3 class="anchored" data-anchor-id="install-the-deepface-package">Install the DeepFace Package</h3>
<p><em>We begin by installing the <code>deepface</code> package using pip. This package provides easy-to-use face recognition and facial attribute analysis functionalities.</em></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>q deepface</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="import-required-libraries" class="level3">
<h3 class="anchored" data-anchor-id="import-required-libraries">Import Required Libraries</h3>
<p><em>We import the necessary libraries: <code>DeepFace</code> from the <code>deepface</code> package for emotion analysis and <code>cv2</code> from OpenCV for image processing tasks.</em></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> deepface <span class="im">import</span> DeepFace</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="load-the-image-1" class="level3">
<h3 class="anchored" data-anchor-id="load-the-image-1">Load the Image</h3>
<p><em>We load the image from the specified path using OpenCV’s <code>imread</code> function. The image is read into a NumPy array for processing.</em></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the image</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">'/content/osm-cca-cv/res/img/image_prefix-003.png'</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> cv2.imread(image_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="convert-the-image-to-grayscale-1" class="level3">
<h3 class="anchored" data-anchor-id="convert-the-image-to-grayscale-1">Convert the Image to Grayscale</h3>
<p><em>We convert the loaded image to grayscale using OpenCV’s <code>cvtColor</code> function. Grayscale images simplify the computation required for face detection.</em></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the image to grayscale</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>gray <span class="op">=</span> cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="load-the-haar-cascade-classifier-for-face-detection-1" class="level3">
<h3 class="anchored" data-anchor-id="load-the-haar-cascade-classifier-for-face-detection-1">Load the Haar Cascade Classifier for Face Detection</h3>
<p><em>We load the pre-trained Haar Cascade classifier for frontal face detection provided by OpenCV. This classifier will help us detect faces in the image.</em></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Haar cascade classifier for face detection</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>face_cascade <span class="op">=</span> cv2.CascadeClassifier(cv2.data.haarcascades <span class="op">+</span> <span class="st">'haarcascade_frontalface_default.xml'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="detect-faces-in-the-image-1" class="level3">
<h3 class="anchored" data-anchor-id="detect-faces-in-the-image-1">Detect Faces in the Image</h3>
<p><em>We use the Haar Cascade classifier’s <code>detectMultiScale</code> method to detect faces within the grayscale image. The method returns a list of bounding boxes around detected faces.</em></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Detect faces in the image</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>faces <span class="op">=</span> face_cascade.detectMultiScale(</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    gray, </span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    scaleFactor<span class="op">=</span><span class="fl">1.1</span>, </span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    minNeighbors<span class="op">=</span><span class="dv">5</span>, </span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    minSize<span class="op">=</span>(<span class="dv">30</span>, <span class="dv">30</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="analyze-emotions-for-each-detected-face" class="level3">
<h3 class="anchored" data-anchor-id="analyze-emotions-for-each-detected-face">Analyze Emotions for Each Detected Face</h3>
<p><em>We iterate over each detected face, extract the region of interest (ROI), and use DeepFace’s <code>analyze</code> function to predict the dominant emotion. We handle exceptions to catch any errors during the analysis.</em></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Analyze emotions for each detected face</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (x, y, w, h) <span class="kw">in</span> faces:</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    face_roi <span class="op">=</span> image[y:y<span class="op">+</span>h, x:x<span class="op">+</span>w]  <span class="co"># Extract the face region of interest</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Analyze emotions using DeepFace</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> DeepFace.analyze(face_roi, actions<span class="op">=</span>[<span class="st">'emotion'</span>], enforce_detection<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        dominant_emotion <span class="op">=</span> result[<span class="st">'dominant_emotion'</span>]</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Detected emotion: </span><span class="sc">{</span>dominant_emotion<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Optionally, draw the dominant emotion on the image</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        cv2.putText(</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>            image, </span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>            dominant_emotion, </span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>            (x, y <span class="op">-</span> <span class="dv">10</span>), </span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>            cv2.FONT_HERSHEY_SIMPLEX, </span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>            <span class="fl">0.9</span>, </span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>            (<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">0</span>), </span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>            <span class="dv">2</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Error analyzing emotions: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Analyze emotions for each detected face</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (x, y, w, h) <span class="kw">in</span> faces:</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    face_roi <span class="op">=</span> image[y:y<span class="op">+</span>h, x:x<span class="op">+</span>w]  <span class="co"># Extract the face region of interest</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Analyze emotions using DeepFace</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> DeepFace.analyze(face_roi, actions<span class="op">=</span>[<span class="st">'emotion'</span>], enforce_detection<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check if result is a list or a dictionary</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(result, <span class="bu">list</span>):</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>            dominant_emotion <span class="op">=</span> result[<span class="dv">0</span>][<span class="st">'dominant_emotion'</span>]</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>            dominant_emotion <span class="op">=</span> result[<span class="st">'dominant_emotion'</span>]</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Detected emotion: </span><span class="sc">{</span>dominant_emotion<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Optionally, draw the dominant emotion on the image</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>        cv2.putText(</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>            image, </span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>            dominant_emotion, </span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>            (x, y <span class="op">-</span> <span class="dv">10</span>), </span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>            cv2.FONT_HERSHEY_SIMPLEX, </span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>            <span class="fl">0.9</span>, </span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>            (<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">0</span>), </span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>            <span class="dv">2</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Error analyzing emotions: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="display-the-image-with-detected-faces-and-emotions" class="level3">
<h3 class="anchored" data-anchor-id="display-the-image-with-detected-faces-and-emotions">Display the Image with Detected Faces and Emotions</h3>
<p><em>Finally, we display the image with the detected faces and their corresponding emotions using <code>cv2_imshow</code>, which is available in Google Colab for displaying images.</em></p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image with detected faces and emotions</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> google.colab.patches <span class="im">import</span> cv2_imshow</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>cv2_imshow(image)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><strong>Summary:</strong></p>
<ul>
<li><strong>Installation and Imports:</strong> We installed the <code>deepface</code> package and imported necessary libraries for image processing and emotion analysis.</li>
<li><strong>Image Loading and Preprocessing:</strong> The image was loaded and converted to grayscale to prepare for face detection.</li>
<li><strong>Face Detection:</strong> Used OpenCV’s Haar Cascade classifier to detect faces in the image.</li>
<li><strong>Emotion Analysis:</strong> For each detected face, we used DeepFace to analyze and predict the dominant emotion.</li>
<li><strong>Visualization:</strong> Displayed the image with bounding boxes around faces and the predicted emotions annotated.</li>
</ul>
<p>This code effectively combines OpenCV for face detection and DeepFace for emotion analysis, allowing you to identify and display emotions detected in faces within an image.</p>
</section>
<section id="other-natural-objects" class="level3">
<h3 class="anchored" data-anchor-id="other-natural-objects">other natural objects</h3>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> pipeline(<span class="st">"image-classification"</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>clf(<span class="st">"/content/osm-cca-cv/res/img/image_prefix-000.png"</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co">#[{'label': 'tabby cat', 'score': 0.731},</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co">#...</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co">#]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="object-recognition" class="level2">
<h2 class="anchored" data-anchor-id="object-recognition">object recognition</h2>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> pipeline(<span class="st">"object-detection"</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>model(<span class="st">"/content/osm-cca-cv/res/img/image_prefix-000.png"</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co"># [{'label': 'blanket',</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">#  'mask': mask_string,</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co">#  'score': 0.917},</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co">#...]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Original image path</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">"/content/osm-cca-cv/res/img/image_prefix-000.png"</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Read the image</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> cv2.imread(image_path)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if the image was successfully loaded</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> image <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Error: Could not read image."</span>)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of detections</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    detections <span class="op">=</span> [</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>            <span class="st">'score'</span>: <span class="fl">0.8914425373077393</span>,</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>            <span class="st">'label'</span>: <span class="st">'dog'</span>,</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>            <span class="st">'box'</span>: {<span class="st">'xmin'</span>: <span class="dv">265</span>, <span class="st">'ymin'</span>: <span class="dv">187</span>, <span class="st">'xmax'</span>: <span class="dv">282</span>, <span class="st">'ymax'</span>: <span class="dv">204</span>}</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>            <span class="st">'score'</span>: <span class="fl">0.999186098575592</span>,</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>            <span class="st">'label'</span>: <span class="st">'person'</span>,</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>            <span class="st">'box'</span>: {<span class="st">'xmin'</span>: <span class="dv">237</span>, <span class="st">'ymin'</span>: <span class="dv">136</span>, <span class="st">'xmax'</span>: <span class="dv">303</span>, <span class="st">'ymax'</span>: <span class="dv">218</span>}</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>            <span class="st">'score'</span>: <span class="fl">0.9989272952079773</span>,</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">'label'</span>: <span class="st">'person'</span>,</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">'box'</span>: {<span class="st">'xmin'</span>: <span class="dv">352</span>, <span class="st">'ymin'</span>: <span class="dv">126</span>, <span class="st">'xmax'</span>: <span class="dv">385</span>, <span class="st">'ymax'</span>: <span class="dv">184</span>}</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>            <span class="st">'score'</span>: <span class="fl">0.999570906162262</span>,</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>            <span class="st">'label'</span>: <span class="st">'person'</span>,</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>            <span class="st">'box'</span>: {<span class="st">'xmin'</span>: <span class="dv">481</span>, <span class="st">'ymin'</span>: <span class="dv">150</span>, <span class="st">'xmax'</span>: <span class="dv">589</span>, <span class="st">'ymax'</span>: <span class="dv">344</span>}</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over detections and draw bounding boxes</span></span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> detection <span class="kw">in</span> detections:</span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> detection[<span class="st">'score'</span>]</span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> detection[<span class="st">'label'</span>]</span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>        box <span class="op">=</span> detection[<span class="st">'box'</span>]</span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>        xmin <span class="op">=</span> box[<span class="st">'xmin'</span>]</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>        ymin <span class="op">=</span> box[<span class="st">'ymin'</span>]</span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>        xmax <span class="op">=</span> box[<span class="st">'xmax'</span>]</span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>        ymax <span class="op">=</span> box[<span class="st">'ymax'</span>]</span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Draw the bounding box</span></span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a>        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">0</span>), thickness<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Prepare label with score</span></span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate text size for background rectangle</span></span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a>        (text_width, text_height), baseline <span class="op">=</span> cv2.getTextSize(</span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a>            text, cv2.FONT_HERSHEY_SIMPLEX, fontScale<span class="op">=</span><span class="fl">0.5</span>, thickness<span class="op">=</span><span class="dv">1</span></span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Position the text above the bounding box if possible</span></span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a>        text_x <span class="op">=</span> xmin</span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true" tabindex="-1"></a>        text_y <span class="op">=</span> ymin <span class="op">-</span> <span class="dv">10</span> <span class="cf">if</span> ymin <span class="op">-</span> <span class="dv">10</span> <span class="op">&gt;</span> text_height <span class="cf">else</span> ymin <span class="op">+</span> text_height <span class="op">+</span> <span class="dv">10</span></span>
<span id="cb21-62"><a href="#cb21-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-63"><a href="#cb21-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Draw background rectangle for text</span></span>
<span id="cb21-64"><a href="#cb21-64" aria-hidden="true" tabindex="-1"></a>        cv2.rectangle(</span>
<span id="cb21-65"><a href="#cb21-65" aria-hidden="true" tabindex="-1"></a>            image,</span>
<span id="cb21-66"><a href="#cb21-66" aria-hidden="true" tabindex="-1"></a>            (text_x, text_y <span class="op">-</span> text_height <span class="op">-</span> baseline),</span>
<span id="cb21-67"><a href="#cb21-67" aria-hidden="true" tabindex="-1"></a>            (text_x <span class="op">+</span> text_width, text_y <span class="op">+</span> baseline),</span>
<span id="cb21-68"><a href="#cb21-68" aria-hidden="true" tabindex="-1"></a>            color<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">0</span>),</span>
<span id="cb21-69"><a href="#cb21-69" aria-hidden="true" tabindex="-1"></a>            thickness<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb21-70"><a href="#cb21-70" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb21-71"><a href="#cb21-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-72"><a href="#cb21-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Put the text on the image</span></span>
<span id="cb21-73"><a href="#cb21-73" aria-hidden="true" tabindex="-1"></a>        cv2.putText(</span>
<span id="cb21-74"><a href="#cb21-74" aria-hidden="true" tabindex="-1"></a>            image,</span>
<span id="cb21-75"><a href="#cb21-75" aria-hidden="true" tabindex="-1"></a>            text,</span>
<span id="cb21-76"><a href="#cb21-76" aria-hidden="true" tabindex="-1"></a>            (text_x, text_y),</span>
<span id="cb21-77"><a href="#cb21-77" aria-hidden="true" tabindex="-1"></a>            cv2.FONT_HERSHEY_SIMPLEX,</span>
<span id="cb21-78"><a href="#cb21-78" aria-hidden="true" tabindex="-1"></a>            fontScale<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb21-79"><a href="#cb21-79" aria-hidden="true" tabindex="-1"></a>            color<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb21-80"><a href="#cb21-80" aria-hidden="true" tabindex="-1"></a>            thickness<span class="op">=</span><span class="dv">1</span></span>
<span id="cb21-81"><a href="#cb21-81" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb21-82"><a href="#cb21-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-83"><a href="#cb21-83" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert BGR to RGB for displaying with matplotlib</span></span>
<span id="cb21-84"><a href="#cb21-84" aria-hidden="true" tabindex="-1"></a>    image_rgb <span class="op">=</span> cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span>
<span id="cb21-85"><a href="#cb21-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-86"><a href="#cb21-86" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display the image using matplotlib</span></span>
<span id="cb21-87"><a href="#cb21-87" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb21-88"><a href="#cb21-88" aria-hidden="true" tabindex="-1"></a>    plt.imshow(image_rgb)</span>
<span id="cb21-89"><a href="#cb21-89" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb21-90"><a href="#cb21-90" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb21-91"><a href="#cb21-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-92"><a href="#cb21-92" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Alternatively, save the image to a file</span></span>
<span id="cb21-93"><a href="#cb21-93" aria-hidden="true" tabindex="-1"></a>    output_path <span class="op">=</span> <span class="st">"/content/osm-cca-cv/res/img/image_with_detections.png"</span></span>
<span id="cb21-94"><a href="#cb21-94" aria-hidden="true" tabindex="-1"></a>    cv2.imwrite(output_path, image)</span>
<span id="cb21-95"><a href="#cb21-95" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Output image saved to </span><span class="sc">{</span>output_path<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Explanation:</strong></p>
<ul>
<li><strong>Import Libraries:</strong>
<ul>
<li><code>cv2</code> for image processing.</li>
<li><code>matplotlib.pyplot</code> for displaying the image inline if using a Jupyter notebook or Google Colab.</li>
</ul></li>
<li><strong>Read the Original Image:</strong>
<ul>
<li>Use <code>cv2.imread()</code> to read the image from the specified path.</li>
<li>Check if the image was loaded successfully.</li>
</ul></li>
<li><strong>Define the Detections List:</strong>
<ul>
<li>This is the provided list containing detection results, each with a score, label, and bounding box coordinates.</li>
</ul></li>
<li><strong>Iterate Over Detections and Draw Bounding Boxes:</strong>
<ul>
<li>Loop through each detection in the list.</li>
<li>Extract the <code>score</code>, <code>label</code>, and <code>box</code> coordinates.</li>
<li>Use <code>cv2.rectangle()</code> to draw the bounding box on the image.
<ul>
<li>Parameters:
<ul>
<li>Image to draw on.</li>
<li>Starting point <code>(xmin, ymin)</code>.</li>
<li>Ending point <code>(xmax, ymax)</code>.</li>
<li><code>color</code>: Set to green <code>(0, 255, 0)</code>.</li>
<li><code>thickness</code>: Set to <code>2</code>.</li>
</ul></li>
</ul></li>
<li>Prepare the text to display the label and score.</li>
<li>Calculate the size of the text using <code>cv2.getTextSize()</code> to create a background rectangle for better visibility.</li>
<li>Draw a filled rectangle behind the text using <code>cv2.rectangle()</code>.</li>
<li>Put the text on the image using <code>cv2.putText()</code>.</li>
</ul></li>
<li><strong>Display or Save the Image:</strong>
<ul>
<li>Convert the image from BGR to RGB color space for correct color representation in Matplotlib.</li>
<li>Use <code>plt.imshow()</code> to display the image inline.</li>
<li>Alternatively, save the image with detections to a file using <code>cv2.imwrite()</code>.</li>
</ul></li>
</ul>
<p><strong>Notes:</strong></p>
<ul>
<li><strong>Color Spaces:</strong>
<ul>
<li>OpenCV uses BGR color order, while Matplotlib uses RGB. Conversion is necessary for correct color display.</li>
</ul></li>
<li><strong>Text Positioning:</strong>
<ul>
<li>The code attempts to place the label above the bounding box if there’s enough space; otherwise, it places it below.</li>
</ul></li>
<li><strong>Font and Text Styling:</strong>
<ul>
<li>Font: <code>cv2.FONT_HERSHEY_SIMPLEX</code>.</li>
<li>Font scale and thickness are set for readability.</li>
</ul></li>
<li><strong>Saving the Image:</strong>
<ul>
<li>The output image is saved to the specified path.</li>
</ul></li>
</ul>
<p><strong>Dependencies:</strong></p>
<ul>
<li><p>Ensure you have OpenCV installed:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install opencv-python</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>If using Jupyter Notebook or Google Colab, Matplotlib is typically pre-installed.</p></li>
</ul>
<p><strong>Example Output:</strong></p>
<ul>
<li>The resulting image will display the original image with green bounding boxes around detected objects.</li>
<li>Labels and scores are displayed near each bounding box for identification.</li>
</ul>
<p><strong>Adjustments:</strong></p>
<ul>
<li><strong>Image Path:</strong>
<ul>
<li>Ensure that <code>image_path</code> correctly points to your image file.</li>
<li>If the image is not found, you’ll receive an error message.</li>
</ul></li>
<li><strong>Output Path:</strong>
<ul>
<li>The <code>output_path</code> should be adjusted if you want to save the image to a different location.</li>
</ul></li>
<li><strong>Display Method:</strong>
<ul>
<li>If you’re running this code outside of a notebook environment, you might need to use <code>cv2.imshow()</code> instead of Matplotlib to display the image.</li>
</ul></li>
</ul>
<p><strong>Display with OpenCV (Alternative):</strong></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image using OpenCV's imshow (only works in local environments)</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>cv2.imshow(<span class="st">'Image with Detections'</span>, image)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>cv2.waitKey(<span class="dv">0</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>cv2.destroyAllWindows()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Note that <code>cv2.imshow()</code> may not work in notebook environments like Google Colab.</li>
</ul>
</section>
<section id="image-video-to-text" class="level2">
<h2 class="anchored" data-anchor-id="image-video-to-text">image, video to text</h2>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>captioner <span class="op">=</span> pipeline(<span class="st">"image-to-text"</span>, model<span class="op">=</span><span class="st">"Salesforce/blip-image-captioning-base"</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co">#captioner("https://huggingface.co/datasets/Narsil/image_dummy/resolve/main/parrots.png")</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>captioner(<span class="st">"https://raw.githubusercontent.com/cca-cce/osm-cca-cv/0a62207580cf63bc808ec825669af8d6f2eedf16/res/img/image_prefix-000.png"</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co">## [{'generated_text': 'two birds are standing next to each other '}]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="pixtral-cutting-edge.." class="level2">
<h2 class="anchored" data-anchor-id="pixtral-cutting-edge..">pixtral, cutting edge..</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://www.youtube.com/watch?v=7aGTKJJMb5w"><img src="https://img.youtube.com/vi/7aGTKJJMb5w/0.jpg" class="img-fluid figure-img" alt="huggingface account and gpu resources required"></a></p>
<figcaption>huggingface account and gpu resources required</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install dependencies</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co">#!pip install -q --upgrade vllm</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co">#!pip install -q --upgrade mistral_common</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> vllm <span class="im">import</span> LLM</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> vllm.sampling_params <span class="im">import</span> SamplingParams</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"mistralai/Pixtral-12B-2409"</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>sampling_params <span class="op">=</span> SamplingParams(max_tokens<span class="op">=</span><span class="dv">8192</span>)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> LLM(model<span class="op">=</span>model_name, tokenizer_mode<span class="op">=</span><span class="st">"mistral"</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Describe this image in one sentence."</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>image_url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/cca-cce/osm-cca-cv/0a62207580cf63bc808ec825669af8d6f2eedf16/res/img/image_prefix-000.png"</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>messages <span class="op">=</span> [</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">"content"</span>: [{<span class="st">"type"</span>: <span class="st">"text"</span>, <span class="st">"text"</span>: prompt}, {<span class="st">"type"</span>: <span class="st">"image_url"</span>, <span class="st">"image_url"</span>: {<span class="st">"url"</span>: image_url}}]</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> llm.chat(messages, sampling_params<span class="op">=</span>sampling_params)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outputs[<span class="dv">0</span>].outputs[<span class="dv">0</span>].text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="testing..-on-your-own-risk" class="level2">
<h2 class="anchored" data-anchor-id="testing..-on-your-own-risk">testing.. on your own risk :)</h2>
<p>To summarize and describe the visual content of a short video without audio using Python, you can follow these main steps:</p>
<ol type="1">
<li><strong>Extract frames from the video.</strong></li>
<li><strong>Select key frames that represent the content effectively.</strong></li>
<li><strong>Use an image captioning model to generate descriptions of the frames.</strong></li>
<li><strong>Combine the frame descriptions into a coherent summary.</strong></li>
</ol>
<p>Below is a detailed explanation of each step along with the necessary code.</p>
<hr>
<section id="extract-frames-from-the-video" class="level3">
<h3 class="anchored" data-anchor-id="extract-frames-from-the-video">1. Extract Frames from the Video</h3>
<p><em>We use the <code>OpenCV</code> library to extract frames from the video file. Alternatively, <code>moviepy</code> can also be used for frame extraction.</em></p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the path to the video file</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>video_file <span class="op">=</span> <span class="st">'path_to_your_video.mp4'</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a directory to store the extracted frames</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>frames_dir <span class="op">=</span> <span class="st">'extracted_frames'</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>os.makedirs(frames_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the video using OpenCV</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>cap <span class="op">=</span> cv2.VideoCapture(video_file)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>frame_count <span class="op">=</span> <span class="bu">int</span>(cap.get(cv2.CAP_PROP_FRAME_COUNT))</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>fps <span class="op">=</span> cap.get(cv2.CAP_PROP_FPS)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Decide on the interval between frames to extract (e.g., every 1 second)</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>frame_interval <span class="op">=</span> <span class="bu">int</span>(fps)  <span class="co"># Adjust this value as needed</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>frame_number <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>extracted_frames <span class="op">=</span> []</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> cap.isOpened():</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    ret, frame <span class="op">=</span> cap.read()</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> ret:</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> frame_number <span class="op">%</span> frame_interval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>        frame_path <span class="op">=</span> os.path.join(frames_dir, <span class="ss">f'frame_</span><span class="sc">{</span>frame_number<span class="sc">}</span><span class="ss">.jpg'</span>)</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>        cv2.imwrite(frame_path, frame)</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>        extracted_frames.append(frame_path)</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>    frame_number <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>cap.release()</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>cv2.destroyAllWindows()</span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Extracted </span><span class="sc">{</span><span class="bu">len</span>(extracted_frames)<span class="sc">}</span><span class="ss"> frames."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Notes:</strong></p>
<ul>
<li><strong>Frame Extraction Interval:</strong>
<ul>
<li>The <code>frame_interval</code> determines how frequently frames are extracted. For example, extracting one frame per second.</li>
<li>Adjust <code>frame_interval</code> based on the video’s FPS and the level of detail you need.</li>
</ul></li>
<li><strong>Saving Frames:</strong>
<ul>
<li>Frames are saved as JPEG images in the <code>extracted_frames</code> directory.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="use-an-image-captioning-model-to-generate-descriptions" class="level3">
<h3 class="anchored" data-anchor-id="use-an-image-captioning-model-to-generate-descriptions">2. Use an Image Captioning Model to Generate Descriptions</h3>
<p><em>We use the <code>transformers</code> library from Hugging Face along with a pre-trained image captioning model to generate descriptions for each extracted frame.</em></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the pre-trained image captioning model and processor</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"nlpconnect/vit-gpt2-image-captioning"</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> VisionEncoderDecoderModel.from_pretrained(model_name)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>feature_extractor <span class="op">=</span> ViTImageProcessor.from_pretrained(model_name)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the device to GPU if available</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to generate caption for an image</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_caption(image_path):</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load and preprocess the image</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> Image.<span class="bu">open</span>(image_path).convert(<span class="st">"RGB"</span>)</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>    pixel_values <span class="op">=</span> feature_extractor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>).pixel_values</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>    pixel_values <span class="op">=</span> pixel_values.to(device)</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate caption</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>        output_ids <span class="op">=</span> model.generate(pixel_values, max_length<span class="op">=</span><span class="dv">50</span>, num_beams<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>    caption <span class="op">=</span> tokenizer.decode(output_ids[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> caption</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate captions for all extracted frames</span></span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>frame_captions <span class="op">=</span> []</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> frame_path <span class="kw">in</span> extracted_frames:</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>    caption <span class="op">=</span> generate_caption(frame_path)</span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>    frame_captions.append({<span class="st">'frame'</span>: frame_path, <span class="st">'caption'</span>: caption})</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>frame_path<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>caption<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Notes:</strong></p>
<ul>
<li><strong>Model Selection:</strong>
<ul>
<li>We use the <code>nlpconnect/vit-gpt2-image-captioning</code> model, which is a Vision Transformer (ViT) encoder and GPT-2 decoder.</li>
<li>You can explore other models on Hugging Face Hub if needed.</li>
</ul></li>
<li><strong>Device Configuration:</strong>
<ul>
<li>The code checks for GPU availability and uses it if possible for faster processing.</li>
</ul></li>
<li><strong>Caption Generation:</strong>
<ul>
<li>The <code>generate_caption</code> function processes an image and generates a caption.</li>
<li>Adjust parameters like <code>max_length</code> and <code>num_beams</code> for different results.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="combine-captions-into-a-coherent-summary" class="level3">
<h3 class="anchored" data-anchor-id="combine-captions-into-a-coherent-summary">3. Combine Captions into a Coherent Summary</h3>
<p><em>We combine the generated captions to form a coherent summary of the video’s visual content.</em></p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine captions into a summary</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>summary_text <span class="op">=</span> <span class="st">' '</span>.join([item[<span class="st">'caption'</span>] <span class="cf">for</span> item <span class="kw">in</span> frame_captions])</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Video Summary:"</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(summary_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Notes:</strong></p>
<ul>
<li><strong>Simple Concatenation:</strong>
<ul>
<li>We concatenate the captions to create a rough summary.</li>
<li>This method may result in repetitive or disjointed text.</li>
</ul></li>
<li><strong>Advanced Summarization (Optional):</strong>
<ul>
<li>For a more coherent summary, you can use a text summarization model to refine the combined captions.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the summarization pipeline</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>summarizer <span class="op">=</span> pipeline(<span class="st">'summarization'</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Summarize the combined captions</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>final_summary <span class="op">=</span> summarizer(summary_text, max_length<span class="op">=</span><span class="dv">130</span>, min_length<span class="op">=</span><span class="dv">30</span>, do_sample<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>][<span class="st">'summary_text'</span>]</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Final Summary:"</span>)</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(final_summary)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Notes:</strong></p>
<ul>
<li><strong>Summarization Model:</strong>
<ul>
<li>The <code>summarization</code> pipeline uses a pre-trained model to condense the text.</li>
</ul></li>
<li><strong>Adjusting Parameters:</strong>
<ul>
<li>Modify <code>max_length</code> and <code>min_length</code> to control the length of the final summary.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="complete-code-example" class="level3">
<h3 class="anchored" data-anchor-id="complete-code-example">Complete Code Example</h3>
<p>Putting it all together, here’s the complete code:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer, pipeline</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Extract Frames from the Video</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>video_file <span class="op">=</span> <span class="st">'path_to_your_video.mp4'</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>frames_dir <span class="op">=</span> <span class="st">'extracted_frames'</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>os.makedirs(frames_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>cap <span class="op">=</span> cv2.VideoCapture(video_file)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>frame_count <span class="op">=</span> <span class="bu">int</span>(cap.get(cv2.CAP_PROP_FRAME_COUNT))</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>fps <span class="op">=</span> cap.get(cv2.CAP_PROP_FPS)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>frame_interval <span class="op">=</span> <span class="bu">int</span>(fps)  <span class="co"># Extract one frame per second</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>frame_number <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>extracted_frames <span class="op">=</span> []</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> cap.isOpened():</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>    ret, frame <span class="op">=</span> cap.read()</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> ret:</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> frame_number <span class="op">%</span> frame_interval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>        frame_path <span class="op">=</span> os.path.join(frames_dir, <span class="ss">f'frame_</span><span class="sc">{</span>frame_number<span class="sc">}</span><span class="ss">.jpg'</span>)</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>        cv2.imwrite(frame_path, frame)</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>        extracted_frames.append(frame_path)</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>    frame_number <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>cap.release()</span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>cv2.destroyAllWindows()</span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Extracted </span><span class="sc">{</span><span class="bu">len</span>(extracted_frames)<span class="sc">}</span><span class="ss"> frames."</span>)</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Generate Captions for Each Frame</span></span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"nlpconnect/vit-gpt2-image-captioning"</span></span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> VisionEncoderDecoderModel.from_pretrained(model_name)</span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a>feature_extractor <span class="op">=</span> ViTImageProcessor.from_pretrained(model_name)</span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_caption(image_path):</span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> Image.<span class="bu">open</span>(image_path).convert(<span class="st">"RGB"</span>)</span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a>    pixel_values <span class="op">=</span> feature_extractor(images<span class="op">=</span>image, return_tensors<span class="op">=</span><span class="st">"pt"</span>).pixel_values</span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a>    pixel_values <span class="op">=</span> pixel_values.to(device)</span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-50"><a href="#cb31-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb31-51"><a href="#cb31-51" aria-hidden="true" tabindex="-1"></a>        output_ids <span class="op">=</span> model.generate(pixel_values, max_length<span class="op">=</span><span class="dv">50</span>, num_beams<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb31-52"><a href="#cb31-52" aria-hidden="true" tabindex="-1"></a>    caption <span class="op">=</span> tokenizer.decode(output_ids[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-53"><a href="#cb31-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> caption</span>
<span id="cb31-54"><a href="#cb31-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-55"><a href="#cb31-55" aria-hidden="true" tabindex="-1"></a>frame_captions <span class="op">=</span> []</span>
<span id="cb31-56"><a href="#cb31-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-57"><a href="#cb31-57" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> frame_path <span class="kw">in</span> extracted_frames:</span>
<span id="cb31-58"><a href="#cb31-58" aria-hidden="true" tabindex="-1"></a>    caption <span class="op">=</span> generate_caption(frame_path)</span>
<span id="cb31-59"><a href="#cb31-59" aria-hidden="true" tabindex="-1"></a>    frame_captions.append({<span class="st">'frame'</span>: frame_path, <span class="st">'caption'</span>: caption})</span>
<span id="cb31-60"><a href="#cb31-60" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>frame_path<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>caption<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb31-61"><a href="#cb31-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-62"><a href="#cb31-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Combine Captions into a Summary</span></span>
<span id="cb31-63"><a href="#cb31-63" aria-hidden="true" tabindex="-1"></a>summary_text <span class="op">=</span> <span class="st">' '</span>.join([item[<span class="st">'caption'</span>] <span class="cf">for</span> item <span class="kw">in</span> frame_captions])</span>
<span id="cb31-64"><a href="#cb31-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-65"><a href="#cb31-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Optional: Summarize the Combined Captions</span></span>
<span id="cb31-66"><a href="#cb31-66" aria-hidden="true" tabindex="-1"></a>summarizer <span class="op">=</span> pipeline(<span class="st">'summarization'</span>)</span>
<span id="cb31-67"><a href="#cb31-67" aria-hidden="true" tabindex="-1"></a>final_summary <span class="op">=</span> summarizer(summary_text, max_length<span class="op">=</span><span class="dv">130</span>, min_length<span class="op">=</span><span class="dv">30</span>, do_sample<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>][<span class="st">'summary_text'</span>]</span>
<span id="cb31-68"><a href="#cb31-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-69"><a href="#cb31-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Final Summary:"</span>)</span>
<span id="cb31-70"><a href="#cb31-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(final_summary)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="additional-considerations" class="level3">
<h3 class="anchored" data-anchor-id="additional-considerations">Additional Considerations</h3>
<ul>
<li><p><strong>Installing Dependencies:</strong></p>
<p>Ensure that you have all the required libraries installed:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install opencv-python</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install transformers</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch  <span class="co"># For GPU support, install torch appropriate for your CUDA version</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install pillow</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Frame Selection:</strong></p>
<ul>
<li><strong>Dynamic Frame Selection:</strong>
<ul>
<li>Instead of extracting frames at fixed intervals, you can implement shot detection or scene change detection to extract frames when the content changes significantly.</li>
<li>Libraries like <code>scenedetect</code> can help with this.</li>
</ul></li>
</ul></li>
<li><p><strong>Processing Time:</strong></p>
<ul>
<li>Image captioning models can be computationally intensive.</li>
<li>Processing time will increase with the number of frames and the complexity of the model.</li>
<li>Using a GPU significantly speeds up the process.</li>
</ul></li>
<li><p><strong>Model Limitations:</strong></p>
<ul>
<li>The quality of the generated captions depends on the model’s capabilities.</li>
<li>Pre-trained models may not accurately describe all types of content.</li>
<li>For domain-specific videos, consider fine-tuning a model on relevant datasets.</li>
</ul></li>
<li><p><strong>Enhancing Summarization:</strong></p>
<ul>
<li><strong>Text Preprocessing:</strong>
<ul>
<li>Remove duplicate captions or captions with minimal information before summarization.</li>
</ul></li>
<li><strong>Semantic Understanding:</strong>
<ul>
<li>Use natural language understanding techniques to better combine captions.</li>
</ul></li>
</ul></li>
<li><p><strong>Privacy and Compliance:</strong></p>
<ul>
<li>Be cautious when processing videos containing personal or sensitive information.</li>
<li>Ensure compliance with data protection regulations.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>By following these steps, you can extract frames from a video without audio, generate descriptions of the visual content using an image captioning model, and combine those descriptions into a textual summary. This approach leverages powerful pre-trained models available through the Hugging Face Transformers library and allows you to create summaries of visual content programmatically.</p>
<p><strong>References:</strong></p>
<ul>
<li><strong>OpenCV Documentation:</strong> <a href="https://docs.opencv.org/">https://docs.opencv.org/</a></li>
<li><strong>Hugging Face Transformers Documentation:</strong> <a href="https://huggingface.co/transformers/">https://huggingface.co/transformers/</a></li>
<li><strong>Image Captioning Models:</strong> <a href="https://huggingface.co/models?pipeline_tag=image-to-text">https://huggingface.co/models?pipeline_tag=image-to-text</a></li>
<li><strong>Scene Detection with PySceneDetect:</strong> <a href="https://pyscenedetect.readthedocs.io/">https://pyscenedetect.readthedocs.io/</a></li>
</ul>
<hr>
<p><strong>Example Output:</strong></p>
<p>Assuming we have a video and we run the code, the output might look like:</p>
<pre><code>Extracted 10 frames.
extracted_frames/frame_0.jpg: a group of people standing in a room.
extracted_frames/frame_30.jpg: a man holding a microphone.
extracted_frames/frame_60.jpg: a woman sitting at a desk with a laptop.
...
Final Summary:
A group of people are standing in a room. A man is holding a microphone. A woman is sitting at a desk with a laptop. ...</code></pre>
<hr>
<p><strong>Note:</strong></p>
<ul>
<li>Replace <code>'path_to_your_video.mp4'</code> with the actual path to your video file.</li>
<li>Adjust the <code>frame_interval</code> based on the video’s length and desired granularity.</li>
<li>Ensure you have sufficient system resources to run the models, especially if processing longer videos or using higher-resolution frames.</li>
</ul>
<hr>
<p>By automating the process of generating textual descriptions from video frames, you can create summaries of videos without audio, which is particularly useful for silent surveillance footage, visual-only content, or scenarios where audio data is unavailable or unusable.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>