[
  {
    "objectID": "pyws01-0-getting-started.html",
    "href": "pyws01-0-getting-started.html",
    "title": "part 1: getting started",
    "section": "",
    "text": "Google Colab Notebooks offer a unique, cloud-based Python environment, making it particularly convenient for data science and research applications. Since Colab runs entirely in the cloud, participants don’t need to worry about local installations or hardware limitations. With Colab, you can access a fully functioning Python environment directly through your browser, complete with popular libraries like NumPy, Pandas, and Matplotlib pre-installed. This makes it ideal for users who may have limited access to high-performance hardware, as Google’s servers handle the heavy lifting. Additionally, Colab supports collaboration, allowing multiple users to work on the same notebook in real-time, much like Google Docs. This contrasts with traditional local Python environments such as Anaconda or PyCharm, where you must install Python and manage dependencies on your own machine. While local environments offer greater control and customization, they can also be more challenging to configure for beginners, particularly when it comes to managing different Python packages and versions.",
    "crumbs": [
      "workshop",
      "part 1: getting started"
    ]
  },
  {
    "objectID": "pyws01-0-getting-started.html#python-environments",
    "href": "pyws01-0-getting-started.html#python-environments",
    "title": "part 1: getting started",
    "section": "",
    "text": "Google Colab Notebooks offer a unique, cloud-based Python environment, making it particularly convenient for data science and research applications. Since Colab runs entirely in the cloud, participants don’t need to worry about local installations or hardware limitations. With Colab, you can access a fully functioning Python environment directly through your browser, complete with popular libraries like NumPy, Pandas, and Matplotlib pre-installed. This makes it ideal for users who may have limited access to high-performance hardware, as Google’s servers handle the heavy lifting. Additionally, Colab supports collaboration, allowing multiple users to work on the same notebook in real-time, much like Google Docs. This contrasts with traditional local Python environments such as Anaconda or PyCharm, where you must install Python and manage dependencies on your own machine. While local environments offer greater control and customization, they can also be more challenging to configure for beginners, particularly when it comes to managing different Python packages and versions.",
    "crumbs": [
      "workshop",
      "part 1: getting started"
    ]
  },
  {
    "objectID": "pyws01-0-getting-started.html#basic-python-syntax",
    "href": "pyws01-0-getting-started.html#basic-python-syntax",
    "title": "part 1: getting started",
    "section": "basic python syntax",
    "text": "basic python syntax\nWhen comparing Python syntax to other popular data science platforms such as Excel or SPSS, Python provides much more flexibility and scalability. Excel is widely used for smaller datasets and offers an intuitive, visual interface for users without coding experience. However, it can become cumbersome for handling large datasets or performing more complex operations, like advanced statistical analysis or machine learning. In contrast, Python’s syntax is relatively simple but powerful, allowing users to write reusable scripts for tasks like data manipulation, cleaning, and analysis. Control structures in Python, such as loops and conditionals, provide much greater control over data operations compared to the rigid, formula-based system of Excel. Moreover, Python can handle more diverse data types and larger datasets more efficiently than Excel, which tends to slow down or crash with extensive data. Compared to SPSS, a statistical software package, Python offers greater flexibility with open-source libraries like SciPy and StatsModels, though SPSS remains easier for non-programmers due to its point-and-click interface. Ultimately, Python is an excellent choice for users looking to scale their work, automate processes, or engage in more complex data science tasks.",
    "crumbs": [
      "workshop",
      "part 1: getting started"
    ]
  },
  {
    "objectID": "pyws03-1-text-analysis.html",
    "href": "pyws03-1-text-analysis.html",
    "title": "reading text content",
    "section": "",
    "text": "code examples nltk\ncode examples spacy\ndownload jupyter notebook",
    "crumbs": [
      "workshop",
      "part 3: text analysis",
      "reading text content"
    ]
  },
  {
    "objectID": "pyws03-1-text-analysis.html#recap-reading-data-files",
    "href": "pyws03-1-text-analysis.html#recap-reading-data-files",
    "title": "reading text content",
    "section": "recap reading data files",
    "text": "recap reading data files\n#| eval: false\n#| echo: true\n\nimport pandas as pd\n\n# comma separated\ndf = pd.read_csv('users.csv', sep=',', quotechar='\"', header=0)\n#df = pd.read_csv('users.csv', sep=',', quotechar='\"', header=None)\n#df = pd.read_csv('users.csv', sep=',', quotechar=\"'\", header=0)\n#df = pd.read_csv('users.csv', sep=',', quotechar=\"'\", header=None)\n#df = pd.read_csv('users.csv', sep='\\t', quotechar='\"', header=0)\n#df = pd.read_csv('users.csv', sep='\\t', quotechar='\"', header=None)\n#df = pd.read_csv('users.csv', sep='\\t', quotechar=\"'\", header=0)\n#df = pd.read_csv('users.csv', sep='\\t', quotechar=\"'\", header=None)\n\n# tab separated \n#df = pd.read_csv('users.tsv', sep=',', quotechar='\"', header=0)\n#df = pd.read_csv('users.tsv', sep=',', quotechar='\"', header=None)\n#df = pd.read_csv('users.tsv', sep=',', quotechar=\"'\", header=0)\n#df = pd.read_csv('users.tsv', sep=',', quotechar=\"'\", header=None)\ndf = pd.read_csv('users.tsv', sep='\\t', quotechar='\"', header=0)\n#df = pd.read_csv('users.tsv', sep='\\t', quotechar='\"', header=None)\n#df = pd.read_csv('users.tsv', sep='\\t', quotechar=\"'\", header=0)\n#df = pd.read_csv('users.tsv', sep='\\t', quotechar=\"'\", header=None)\n\n# excel\n#df = pd.read_excel('users.xlsx', header=0, sheet_name=1)\n#df = pd.read_excel('users.xlsx', header=None, sheet_name=1)\n\ndf.head()",
    "crumbs": [
      "workshop",
      "part 3: text analysis",
      "reading text content"
    ]
  },
  {
    "objectID": "pyws03-1-text-analysis.html#nltk-and-text-corpora",
    "href": "pyws03-1-text-analysis.html#nltk-and-text-corpora",
    "title": "reading text content",
    "section": "nltk and text corpora",
    "text": "nltk and text corpora\n\nImport Libraries and Download NLTK Data\nIn this step, we import the necessary libraries and download the required NLTK data packages. Specifically, we use NLTK’s download function to ensure the ‘gutenberg’ corpus and the ‘punkt’ tokenizer are available for use. The ‘punkt’ tokenizer is essential for splitting text into sentences and words.\nimport nltk\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfrom IPython.display import display\n\n# Download necessary NLTK data files\nnltk.download('gutenberg')\nnltk.download('punkt')\nnltk.download('punkt_tab')\n\n\nLoad the Gutenberg Corpus\nHere, we import the Gutenberg corpus from NLTK’s corpus module. The Gutenberg corpus is a collection of literary texts that we will analyze. We retrieve the list of file IDs available in the corpus using gutenberg.fileids(), which provides us with the filenames of the texts in the corpus.\nfrom nltk.corpus import gutenberg\n\n# Get list of file IDs from the Gutenberg corpus\nfile_ids = gutenberg.fileids()\n\n\nAnalyze Each Text in the Corpus\nIn this section, we iterate over each text in the Gutenberg corpus to compute various linguistic statistics. We use NLTK’s raw() method to get the raw text, word_tokenize() to split the text into words, and sent_tokenize() to split the text into sentences. These NLTK tokenizers are essential for textual analysis.\n# Initialize a list to store statistics\nstats_list = []\n\n# Analyze each text in the corpus\nfor file_id in file_ids:\n    raw_text = gutenberg.raw(file_id)\n    words = nltk.word_tokenize(raw_text)\n    sentences = nltk.sent_tokenize(raw_text)\n    num_words = len(words)\n    num_sentences = len(sentences)\n    avg_word_length = sum(len(word) for word in words) / num_words\n    vocab_size = len(set(words))\n    lexical_diversity = vocab_size / num_words\n    stats_list.append({\n        'Title': file_id,\n        'Num_Words': num_words,\n        'Num_Sentences': num_sentences,\n        'Avg_Word_Length': avg_word_length,\n        'Vocab_Size': vocab_size,\n        'Lexical_Diversity': lexical_diversity\n    })\n\n\nCreate and Display the DataFrame\nWe create a pandas DataFrame from the collected statistics for easier analysis and display it within the notebook using display(). This allows us to view the computed statistics in a structured tabular format.\n# Create a DataFrame to hold the statistics\nstats_df = pd.DataFrame(stats_list)\n\n# Display the statistics table\ndisplay(stats_df)\n\n\nSet Up the Output Directory\nHere, we define the output path where we’ll save the text files and figures. We use os.makedirs() with exist_ok=True to create the directory if it doesn’t already exist, ensuring that our output files have a designated location.\n# Define the output path for saving text files and figures\noutput_path = \"/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/tmp\"\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_path, exist_ok=True)\n\n\nGenerate and Display Plots\nIn this step, we create various plots to visualize the text statistics using Seaborn and Matplotlib. We display these plots inline in the notebook using plt.show(). The plots include:\n\nA bar plot of the number of words per text.\nA bar plot of the average word length per text.\nA scatter plot of vocabulary size versus the number of words.\n\nWe utilize NLTK’s tokenization outputs to extract the necessary values for plotting.\n# Set up seaborn style\nsns.set(style='whitegrid')\n\n# Bar plot of number of words per text\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Title', y='Num_Words', data=stats_df)\nplt.xticks(rotation=45)\nplt.title('Number of Words per Text')\nplt.tight_layout()\nplt.savefig(os.path.join(output_path, 'num_words_per_text.png'))\nplt.show()\n\n# Bar plot of average word length per text\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Title', y='Avg_Word_Length', data=stats_df)\nplt.xticks(rotation=45)\nplt.title('Average Word Length per Text')\nplt.tight_layout()\nplt.savefig(os.path.join(output_path, 'avg_word_length_per_text.png'))\nplt.show()\n\n# Scatter plot of vocabulary size vs. number of words\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='Num_Words', y='Vocab_Size', data=stats_df, hue='Title')\nplt.title('Vocabulary Size vs. Number of Words')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.savefig(os.path.join(output_path, 'vocab_size_vs_num_words.png'))\nplt.show()\n\n\nSave Texts to Disk\nFinally, we save each text from the Gutenberg corpus as a plain text file to the specified output directory. We use NLTK’s raw() method again to retrieve the full text of each file and write it to disk using standard file I/O operations.\n# Save each text as a plain text file to the output path\nfor file_id in file_ids:\n    raw_text = gutenberg.raw(file_id)\n    output_file_path = os.path.join(output_path, file_id)\n    with open(output_file_path, 'w', encoding='utf-8') as f:\n        f.write(raw_text)\n\n\nLoad Saved Texts into an NLTK Corpus\nIn this final step, we read the saved texts from the output directory back into an NLTK corpus using PlaintextCorpusReader. This allows us to treat the collection of saved texts as a corpus for further analysis. PlaintextCorpusReader is an NLTK class designed to read plain text files from a directory and create a corpus object.\nfrom nltk.corpus import PlaintextCorpusReader\n\n# Define the corpus root directory\ncorpus_root = output_path\n\n# Define the pattern to match the text files (e.g., all files with .txt extension)\nfile_pattern = '.*'  # Matches all files\n# Matches only text files\nfile_pattern = r'.*\\.txt'  # Matches all files ending with .txt\n\n# Create a PlaintextCorpusReader object\nnew_corpus = PlaintextCorpusReader(corpus_root, file_pattern)\n\n# Access the file IDs in the new corpus\nnew_file_ids = new_corpus.fileids()\nprint(\"Files in the new corpus:\", new_file_ids)\n\n# Example: Read words from a specific file\nwords_in_file = new_corpus.words(new_file_ids[0])\nprint(\"First 20 words in\", new_file_ids[0], \":\", words_in_file[:20])\nBy using PlaintextCorpusReader, we can load all the saved text files into a new NLTK corpus. The fileids() method lists all the files in the corpus, and methods like words(), sents(), and paras() allow us to access words, sentences, and paragraphs, respectively. This demonstrates NLTK’s capability to handle custom corpora built from local text files, enabling further text processing and analysis on the newly created corpus.",
    "crumbs": [
      "workshop",
      "part 3: text analysis",
      "reading text content"
    ]
  },
  {
    "objectID": "pyws03-1-text-analysis.html#text-to-pandas-dataframe",
    "href": "pyws03-1-text-analysis.html#text-to-pandas-dataframe",
    "title": "reading text content",
    "section": "text to pandas dataframe",
    "text": "text to pandas dataframe\n\nexample sustainability communication\n\n\nImport Libraries and Define Text Cleaning Function\nIn this step, we import the necessary libraries and define a function to clean text data. We use the os module for file and directory operations, re for regular expressions, pandas for data manipulation, and spacy for natural language processing tasks.\n#| eval: true\n#| echo: true\n#| output: false\n\nimport os\nimport re\nimport pandas as pd\nimport spacy\n\n!python -m spacy download en_core_web_sm\n\n# Function to clean text by removing non-ASCII characters\ndef clean_text(text):\n    # Remove non-ASCII characters (commented out to preserve UTF-8 text)\n    # cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    cleaned_text = text\n    return cleaned_text\nThe clean_text function is intended to remove non-ASCII characters using re.sub. However, since we are dealing with UTF-8 encoded text (e.g., Swedish text data), we retain the original text by commenting out the removal line.\n\n\n\nSet Directory Paths and Initialize Data Structures\nHere, we specify the directory paths where the text files are located and initialize data structures for storing the text data. The directory_path variable holds the path to the directory containing the text files. We also initialize an empty list data to store the text information and a counter unique_id for assigning unique identifiers to each text.\n# Directory containing text files\ndirectory_path = '/content/osm-cca-nlp/res/txt'\ndirectory_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/res/txt'\n\n# Initialize an empty list to store the data\ndata = []\n\n# Initialize a unique ID counter\nunique_id = 1\nThe os module functions will later use directory_path to access the files. The unique_id will increment for each file, ensuring each text has a unique identifier.\n\n\n\nRead and Clean Text Files\nIn this section, we iterate over the text files in the specified directory, read their contents, clean the text using the clean_text function, and store the data in the data list. The os.listdir function lists all files in the directory, and os.path.join constructs the full file path.\n# Iterate over the text files in the directory\nfor filename in os.listdir(directory_path):\n    # Consider only plain text files\n    if filename.endswith(\".txt\") or filename.endswith(\".md\"):\n        file_path = os.path.join(directory_path, filename)\n\n        # Read the file content\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n\n        # Clean the text\n        cleaned_text = clean_text(text)\n\n        # Append the data as a dictionary with a unique ID\n        data.append({\n            'id': unique_id,\n            'filename': filename,\n            'original_text': text,\n            'cleaned_text': cleaned_text\n        })\n\n        # Increment the unique ID\n        unique_id += 1\nWe use open with encoding='utf-8' to read the files, ensuring that UTF-8 characters are handled correctly. The cleaned text and metadata are stored as dictionaries in the data list.\n\n\n\nCreate and Save DataFrame\nWe convert the collected data into a Pandas DataFrame for easier manipulation and analysis. We then save this DataFrame as a TSV (Tab-Separated Values) file using the to_csv method with sep='\\t'. The index=False parameter ensures that the DataFrame index is not included in the output file.\n# Create a Pandas DataFrame\ntext_df = pd.DataFrame(data)\n\n# Save the DataFrame as a TSV file in the 'csv' subdirectory\noutput_file_path = '/content/osm-cca-nlp/csv/text_data.tsv'\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/text_data.tsv'\n\n# Save the DataFrame to a TSV file\ntext_df.to_csv(output_file_path, sep='\\t', index=False)\n\n# Display the DataFrame\nprint(text_df)\nThis step utilizes Pandas’ data handling capabilities to structure our text data effectively and save it for future use.\n\n\n\nLoad spaCy Model\nWe load a spaCy language model to perform natural language processing tasks. The spacy.load function loads the specified model into memory. In this case, we use the small English model en_core_web_sm.\n# Load the spaCy model (small English model is used here)\nnlp = spacy.load(\"en_core_web_sm\")\nThe loaded nlp object provides access to spaCy’s powerful NLP features, including tokenization, part-of-speech tagging, and sentence segmentation.\n\n\n\nCompute Text Statistics\nWe calculate word counts, character counts, and sentence counts for each cleaned text in the DataFrame. Pandas’ apply function applies a lambda function to each row in the cleaned_text column. For sentence counting, we use spaCy’s sentence segmentation by processing the text with nlp and accessing the .sents attribute.\n# Perform word count and character count on each cleaned text in the DataFrame\ntext_df['word_count'] = text_df['cleaned_text'].apply(lambda x: len(x.split()))\ntext_df['character_count'] = text_df['cleaned_text'].apply(lambda x: len(x))\n\n# Perform sentence count using spaCy\ntext_df['sentence_count'] = text_df['cleaned_text'].apply(lambda x: len(list(nlp(x).sents)))\nThe len(x.split()) calculates the number of words by splitting the text on whitespace. The character count is obtained with len(x). For sentence count, we process the text with the spaCy model and convert the sents generator to a list to count the sentences.\n\n\n\nDisplay DataFrame with Selected Columns\nFinally, we display the DataFrame, excluding the ‘original_text’ and ‘cleaned_text’ columns for brevity. The columns.difference function identifies columns to exclude, and we use this to select the remaining columns for display.\n# Select and print all columns except 'original_text' and 'cleaned_text'\ncolumns_to_display = text_df.columns.difference(['original_text', 'cleaned_text'])\nprint(text_df[columns_to_display])\nThis step showcases the metadata and statistical information we’ve gathered, such as the unique ID, filename, word count, character count, and sentence count, without displaying the potentially lengthy text content.\n\nSummary:\n\nos module: Used for interacting with the operating system, listing directory contents, and constructing file paths.\nre module: Provides regular expression matching operations for text cleaning (though in this code, the regex is commented out).\npandas: Used for creating and manipulating the DataFrame to store text data and computed statistics.\nspacy: Provides advanced NLP capabilities; we load a language model to perform sentence segmentation for counting sentences.\napply and lambda functions in pandas: Used to apply functions to DataFrame columns for calculating word counts, character counts, and sentence counts.\n\nThis modular approach allows for easy understanding and maintenance of the code, with each section handling a specific part of the text processing pipeline.",
    "crumbs": [
      "workshop",
      "part 3: text analysis",
      "reading text content"
    ]
  },
  {
    "objectID": "pyws05-0-data-collection.html",
    "href": "pyws05-0-data-collection.html",
    "title": "part 5: data collection",
    "section": "",
    "text": "Web scraping is an essential technique for gathering data from websites, and it can be approached differently depending on whether the target is static or dynamic content. Static web content is straightforward to scrape because the content is directly embedded in the HTML of the webpage, and tools like BeautifulSoup can easily parse this HTML to extract useful information, such as financial data, text, or images. For instance, scraping static content from a financial news site could involve fetching stock prices, headlines, and metadata, all available in the page’s source code. However, dynamic web content presents additional challenges. This type of content is often rendered via JavaScript after the initial page load, making traditional HTML scraping tools ineffective. To handle dynamic content, you need tools like Selenium or Playwright, which can simulate a browser environment, interact with the page, and extract data after it has been dynamically loaded. For example, scraping a site with real-time financial data, such as stock trading platforms or cryptocurrency exchanges, requires navigating dynamic elements like charts and live updates, all of which are powered by JavaScript.",
    "crumbs": [
      "workshop",
      "part 5: data collection"
    ]
  },
  {
    "objectID": "pyws05-0-data-collection.html#web-scraping",
    "href": "pyws05-0-data-collection.html#web-scraping",
    "title": "part 5: data collection",
    "section": "",
    "text": "Web scraping is an essential technique for gathering data from websites, and it can be approached differently depending on whether the target is static or dynamic content. Static web content is straightforward to scrape because the content is directly embedded in the HTML of the webpage, and tools like BeautifulSoup can easily parse this HTML to extract useful information, such as financial data, text, or images. For instance, scraping static content from a financial news site could involve fetching stock prices, headlines, and metadata, all available in the page’s source code. However, dynamic web content presents additional challenges. This type of content is often rendered via JavaScript after the initial page load, making traditional HTML scraping tools ineffective. To handle dynamic content, you need tools like Selenium or Playwright, which can simulate a browser environment, interact with the page, and extract data after it has been dynamically loaded. For example, scraping a site with real-time financial data, such as stock trading platforms or cryptocurrency exchanges, requires navigating dynamic elements like charts and live updates, all of which are powered by JavaScript.",
    "crumbs": [
      "workshop",
      "part 5: data collection"
    ]
  },
  {
    "objectID": "pyws05-0-data-collection.html#web-experiments",
    "href": "pyws05-0-data-collection.html#web-experiments",
    "title": "part 5: data collection",
    "section": "web experiments",
    "text": "web experiments\nBeyond scraping, the rise of platforms like Streamlit and GitHub Codespaces offers powerful possibilities for hosting web experiments that collect user interaction and behavioral data. Streamlit is a Python-based framework that simplifies the creation of interactive web applications, making it easy for researchers to design experiments that capture user input in real-time. For example, researchers in social science could build a survey that adjusts dynamically based on user responses or a task-based experiment where user behavior is logged and analyzed. Streamlit’s simplicity allows for fast deployment of experiments that run directly in the browser, eliminating the need for complex backend infrastructure. On the other hand, GitHub Codespaces provides a full development environment in the cloud, enabling researchers to collaborate on and host interactive experiments. By setting up a Codespace, researchers can deploy real-time applications with persistent storage, making it possible to record user behaviors such as clicks, navigation patterns, and text input during the experiment. The ability to run experiments in the cloud with either platform means data collection can scale easily, and researchers can access a broader pool of participants without requiring them to install software or participate in person. Both platforms offer streamlined ways to collect, store, and analyze behavioral data, which can be particularly useful for conducting social science research in a modern, online setting.",
    "crumbs": [
      "workshop",
      "part 5: data collection"
    ]
  },
  {
    "objectID": "pyws04-0-image-analysis.html",
    "href": "pyws04-0-image-analysis.html",
    "title": "part 4: image analysis",
    "section": "",
    "text": "Handling images in Python involves a variety of intricacies, especially when processing raw visual data to extract meaningful information. Unlike text, which is inherently unstructured, images contain pixel-based data that needs to be interpreted and organized. When reading and processing image content using libraries like OpenCV and Pillow, common challenges include handling different image formats (such as JPEG, PNG, or TIFF), varying image resolutions, and potential distortions like noise or compression artifacts. Additionally, images can come in different color spaces (RGB, grayscale, etc.), and choosing the right one for analysis is crucial depending on the task at hand. For instance, converting an image to grayscale simplifies it by removing color information, making it easier to focus on structural aspects of the image like edges or shapes. Another complexity arises in how to break down the image into meaningful units—whether it’s identifying objects, segmenting regions, or extracting pixel values for quantitative analysis. This process requires a deep understanding of how visual information is encoded and how it can be transformed for different analytical purposes.",
    "crumbs": [
      "workshop",
      "part 4: image analysis"
    ]
  },
  {
    "objectID": "pyws04-0-image-analysis.html#reading-image-content",
    "href": "pyws04-0-image-analysis.html#reading-image-content",
    "title": "part 4: image analysis",
    "section": "",
    "text": "Handling images in Python involves a variety of intricacies, especially when processing raw visual data to extract meaningful information. Unlike text, which is inherently unstructured, images contain pixel-based data that needs to be interpreted and organized. When reading and processing image content using libraries like OpenCV and Pillow, common challenges include handling different image formats (such as JPEG, PNG, or TIFF), varying image resolutions, and potential distortions like noise or compression artifacts. Additionally, images can come in different color spaces (RGB, grayscale, etc.), and choosing the right one for analysis is crucial depending on the task at hand. For instance, converting an image to grayscale simplifies it by removing color information, making it easier to focus on structural aspects of the image like edges or shapes. Another complexity arises in how to break down the image into meaningful units—whether it’s identifying objects, segmenting regions, or extracting pixel values for quantitative analysis. This process requires a deep understanding of how visual information is encoded and how it can be transformed for different analytical purposes.",
    "crumbs": [
      "workshop",
      "part 4: image analysis"
    ]
  },
  {
    "objectID": "pyws04-0-image-analysis.html#object-recognition",
    "href": "pyws04-0-image-analysis.html#object-recognition",
    "title": "part 4: image analysis",
    "section": "object recognition",
    "text": "object recognition\nObject recognition, a more advanced image analysis technique, involves detecting and classifying objects within an image, often using machine learning models. This contrasts with more basic types of image analysis, such as finding contours or corners, which are simpler geometric features. Contour detection in an image focuses on identifying the boundaries or edges of objects, which can be useful for shape analysis, but it doesn’t provide any understanding of the object’s identity or function. Corner detection, on the other hand, finds points in the image where there is a sharp change in direction, such as the corners of a rectangle, which can be helpful in tasks like motion tracking or object detection based on feature points. While both contour and corner detection are essential for breaking down images into simpler shapes or features, they do not attempt to interpret or classify the objects in the scene. Object recognition takes this further by using algorithms or trained models to not only detect an object but also to classify it—whether it’s identifying a car, a person, or another object in the image. This step is critical in fields like behavioral analysis or media studies, where understanding what is in the image, rather than just its shape or structure, is essential for deriving insights from visual data.",
    "crumbs": [
      "workshop",
      "part 4: image analysis"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html",
    "href": "pyws01-2-getting-started.html",
    "title": "basic python syntax",
    "section": "",
    "text": "interactive learnpython\ndownload jupyter notebook",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html#strings-and-numbers",
    "href": "pyws01-2-getting-started.html#strings-and-numbers",
    "title": "basic python syntax",
    "section": "strings and numbers",
    "text": "strings and numbers\nHere are 10 Python code lines with different types of strings and numbers, each followed by an explanation:\n\nCode:\n# This is a comment explaining the next line\nExplanation:\nThis line starts with a #, making it a comment. Python ignores this line during execution. It’s used to explain code or leave notes for other programmers.\nCode:\nprint('Hello, World!')\nExplanation:\nThis prints the string 'Hello, World!' to the console. Single quotes enclose the string. In Python, single and double quotes are interchangeable for defining strings.\nCode:\nprint(\"Python is fun\")\nExplanation:\nHere, double quotes are used to define the string \"Python is fun\". Python treats strings defined with single or double quotes the same way.\nCode:\nprint(\"He said, \\\"Python is cool\\\"\")\nExplanation:\nThis line prints He said, \"Python is cool\". The backslash \\ before the double quotes escapes them, telling Python to treat them as part of the string instead of ending it.\nCode:\nprint(\"Line one\\nLine two\")\nExplanation:\nThe \\n is a newline escape character, so this will output:\nLine one\nLine two\nThe \\n tells Python to move to a new line.\nCode:\nprint(\"Hello\" + \" \" + \"World\")\nExplanation:\nThis line concatenates three strings: \"Hello\", a space (\" \"), and \"World\", resulting in Hello World. The + operator combines strings.\nCode:\nprint(5 + 3)\nExplanation:\nThis performs an arithmetic operation, adding two integers 5 and 3, resulting in the output 8. Python interprets + as an addition operator when used with numbers.\nCode:\nprint(5.0 + 3)\nExplanation:\nThis adds a floating-point number 5.0 and an integer 3. Python automatically converts the integer to a float and outputs 8.0, demonstrating Python’s support for mixed-type arithmetic.\nCode:\nprint(7 / 2)\nExplanation:\nThis division operation between two integers results in 3.5. In Python 3, division with / always results in a float, even when dividing two integers.\nCode: python     print(type(3.14)) Explanation:\nThis uses the type() function, which returns the data type of the value passed to it. Here, 3.14 is a floating-point number, so the output will be &lt;class 'float'&gt;, indicating the value is of type float.\n\nThese examples cover key features of Python strings, numbers, and the type() function, along with how comments and escape characters work.",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html#lists-and-dictionaries",
    "href": "pyws01-2-getting-started.html#lists-and-dictionaries",
    "title": "basic python syntax",
    "section": "lists and dictionaries",
    "text": "lists and dictionaries\nHere are 10 Python code lines illustrating different types of lists and dictionaries, with explanations:\n\nCode:\nmy_list = [1, 2, 3, 4]\nExplanation:\nThis creates a list called my_list containing four integer elements: [1, 2, 3, 4]. Lists are ordered and mutable collections in Python, allowing for element addition, removal, and modification. Each element can be accessed by its index, starting from 0.\nCode:\nmy_dict = {'name': 'Alice', 'age': 30}\nExplanation:\nThis creates a dictionary my_dict with two key-value pairs: 'name': 'Alice' and 'age': 30. Dictionaries are unordered collections that map keys to values, and values can be accessed using the keys.\nCode:\nnested_list = [[1, 2], [3, 4], [5, 6]]\nExplanation:\nThis creates a 2D list nested_list, where each element is another list. Accessing elements can be done using two indices, such as nested_list[0][1] to get the value 2.\nCode:\nnested_dict = {'person1': {'name': 'Alice', 'age': 30}, 'person2': {'name': 'Bob', 'age': 25}}\nExplanation:\nThis is a 2D dictionary, where each key ('person1', 'person2') maps to another dictionary. For example, you can access Alice’s age by using nested_dict['person1']['age'], which returns 30.\nCode:\nmy_list.append(5)\nExplanation:\nThis appends the value 5 to the end of my_list. The append() method is a built-in function for adding elements to a list, modifying it in place.\nCode:\nlast_item = my_list.pop()\nExplanation:\nThis removes and returns the last element from my_list using the pop() method. If my_list = [1, 2, 3, 4, 5], after popping, my_list becomes [1, 2, 3, 4] and last_item is assigned the value 5.\nCode:\nsecond_item = my_list[1]\nExplanation:\nThis accesses the second element of my_list using the index 1 (Python uses 0-based indexing). For example, if my_list = [1, 2, 3, 4], second_item will be 2.\nCode:\nmy_dict['city'] = 'New York'\nExplanation:\nThis adds a new key-value pair 'city': 'New York' to my_dict. Dictionaries allow dynamic insertion of key-value pairs. If my_dict already contains 'city', this will update its value.\nCode:\nremoved_value = my_dict.pop('age')\nExplanation:\nThis removes the key 'age' from my_dict and returns its value (30 in this case). The pop() method removes the specified key-value pair and modifies the dictionary.\nCode: python     print(type(my_list)) Explanation:\nThis uses the type() function to check the data type of my_list. The output will be &lt;class 'list'&gt;, indicating that my_list is a list. Similarly, calling type(my_dict) would return &lt;class 'dict'&gt;, showing that my_dict is a dictionary.\n\nThese examples illustrate key operations with lists and dictionaries, including element access, appending, popping, and the use of the type() function to check data types.",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html#loops-and-conditionals",
    "href": "pyws01-2-getting-started.html#loops-and-conditionals",
    "title": "basic python syntax",
    "section": "loops and conditionals",
    "text": "loops and conditionals\nHere are 10 Python code chunks demonstrating different types of loops and conditionals, with explanations:\n\nCode:\nfor i in range(5):\n    print(i)\nExplanation:\nThis is a basic for loop that iterates over the range 0 to 4 (Python ranges are zero-indexed and exclusive of the stop value). It prints each value of i in the loop: 0, 1, 2, 3, 4.\nCode:\nwhile True:\n    print(\"Looping...\")\n    break\nExplanation:\nThis is a while loop with a True condition, which would normally create an infinite loop. However, the break statement exits the loop after the first iteration. Without break, it would continuously print \"Looping...\".\nCode:\nif 10 &gt; 5:\n    print(\"10 is greater than 5\")\nelse:\n    print(\"5 is greater than or equal to 10\")\nExplanation:\nThis is an if-else statement. The condition 10 &gt; 5 evaluates to True, so the first block is executed, printing \"10 is greater than 5\". The else block would run if the condition were False.\nCode:\nnumber = 7\nif number % 2 == 0:\n    print(\"Even\")\nelse:\n    print(\"Odd\")\nExplanation:\nThis checks if the variable number is even or odd using the modulo operator (%). If number % 2 == 0, it prints \"Even\", otherwise, it prints \"Odd\". In this case, it prints \"Odd\" because 7 is not divisible by 2.\nCode:\nfor x in range(10):\n    if x % 2 == 0:\n        continue\n    print(x)\nExplanation:\nThis for loop prints all odd numbers from 0 to 9. The continue statement skips the rest of the loop when x is even, so only odd values (1, 3, 5, 7, 9) are printed.\nCode:\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor index, fruit in enumerate(fruits):\n    print(f\"Fruit {index}: {fruit}\")\nExplanation:\nThe enumerate() function provides both the index and value of each element in the fruits list. The loop iterates over the list, printing each fruit along with its index:\nFruit 0: apple\nFruit 1: banana\nFruit 2: cherry\nCode:\nis_raining = True\nis_sunny = False\nif is_raining and not is_sunny:\n    print(\"It's raining but not sunny\")\nelif is_sunny and not is_raining:\n    print(\"It's sunny but not raining\")\nelse:\n    print(\"It's either both raining and sunny or neither\")\nExplanation:\nThis demonstrates a compound conditional using Boolean variables. Since is_raining is True and is_sunny is False, the first block is executed, printing \"It's raining but not sunny\".\nCode:\neven_numbers = [x for x in range(10) if x % 2 == 0]\nprint(even_numbers)\nExplanation:\nThis is a list comprehension that creates a list of even numbers from 0 to 9. It iterates over the range of numbers and only includes those where x % 2 == 0. The output is [0, 2, 4, 6, 8].\nCode:\ntry:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero\")\nExplanation:\nThis is an example of a try-except block for exception handling. The try block contains code that could raise an exception (division by zero), and the except block catches the ZeroDivisionError and prints \"Cannot divide by zero\". Without the exception handling, the program would crash.\nCode: python     for i in range(5):        try:            print(10 / i)        except ZeroDivisionError:            print(\"Division by zero is not allowed\") Explanation:\nThis loop attempts to divide 10 by i for values from 0 to 4. When i is 0, a ZeroDivisionError occurs, which is caught by the except block, printing \"Division by zero is not allowed\". For other values of i, the result of the division is printed.\n\nThese examples cover various types of loops, conditionals, list comprehensions, Boolean logic, and exception handling in Python.",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html#user-defined-functions",
    "href": "pyws01-2-getting-started.html#user-defined-functions",
    "title": "basic python syntax",
    "section": "user defined functions",
    "text": "user defined functions\nHere are 10 Python code chunks demonstrating different types of user-defined functions, with explanations:\n\nCode:\ndef greet():\n    \"\"\"This function prints a simple greeting message.\"\"\"\n    print(\"Hello, welcome!\")\ngreet()\nExplanation:\nThis is a simple function greet() that takes no arguments and prints a greeting message. It is called using greet(). The triple quotes \"\"\" define a docstring, which serves as the function’s documentation. When called, it prints \"Hello, welcome!\".\nCode:\ndef greet_person(name):\n    \"\"\"This function greets a person by name.\"\"\"\n    print(f\"Hello, {name}!\")\ngreet_person(\"Alice\")\nExplanation:\nThis function greet_person(name) accepts a single argument, name, and prints a personalized greeting. When you call greet_person(\"Alice\"), it prints \"Hello, Alice!\". The docstring explains what the function does.\nCode:\ndef add_numbers(a, b):\n    \"\"\"Returns the sum of two numbers.\"\"\"\n    return a + b\nresult = add_numbers(5, 3)\nprint(result)\nExplanation:\nadd_numbers(a, b) is a function that takes two arguments, a and b, and returns their sum. In this case, add_numbers(5, 3) returns 8, which is printed. The return keyword is used to send the result back to the calling code.\nCode:\ndef multiply(a, b=2):\n    \"\"\"Multiplies two numbers, with the second number having a default value of 2.\"\"\"\n    return a * b\nprint(multiply(4))  # uses default value for b\nprint(multiply(4, 3))  # overrides default value for b\nExplanation:\nThis function multiply(a, b=2) takes two arguments but assigns a default value of 2 to b. If only one argument is passed, the function uses the default value. Calling multiply(4) returns 8, while multiply(4, 3) returns 12.\nCode:\ndef divide(a, b):\n    \"\"\"Divides a by b and handles division by zero.\"\"\"\n    if b == 0:\n        return \"Cannot divide by zero!\"\n    return a / b\nprint(divide(10, 2))\nprint(divide(10, 0))\nExplanation:\ndivide(a, b) takes two arguments and returns the result of dividing a by b. It includes a conditional to check for division by zero. If b is 0, it returns an error message. Calling divide(10, 2) returns 5.0, while divide(10, 0) returns \"Cannot divide by zero!\".\nCode:\ndef square_elements(numbers):\n    \"\"\"Takes a list of numbers and returns a list of their squares.\"\"\"\n    return [x ** 2 for x in numbers]\nprint(square_elements([1, 2, 3, 4]))\nExplanation:\nThis function square_elements(numbers) takes a list of numbers and returns a new list containing the squares of those numbers. The function uses list comprehension. Calling square_elements([1, 2, 3, 4]) returns [1, 4, 9, 16].\nCode:\ndef factorial(n):\n    \"\"\"Recursively calculates the factorial of n.\"\"\"\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\nprint(factorial(5))\nExplanation:\nThis function factorial(n) uses recursion to calculate the factorial of a number. If n is 0, it returns 1 (base case). Otherwise, it multiplies n by factorial(n - 1). Calling factorial(5) returns 120.\nCode:\ndef is_even(number):\n    \"\"\"Checks if a number is even.\"\"\"\n    return number % 2 == 0\nprint(is_even(4))  # True\nprint(is_even(7))  # False\nExplanation:\nThe function is_even(number) checks if a number is even by using the modulo operator (%). If the remainder is 0, it returns True, otherwise False. Calling is_even(4) returns True, and is_even(7) returns False.\nCode:\ndef describe_person(name, age, *hobbies):\n    \"\"\"Takes a name, age, and any number of hobbies, and prints a description.\"\"\"\n    print(f\"{name} is {age} years old and enjoys {', '.join(hobbies)}.\")\ndescribe_person(\"Alice\", 30, \"reading\", \"hiking\", \"cooking\")\nExplanation:\nThis function describe_person(name, age, *hobbies) accepts a variable number of hobby arguments using the * syntax, which collects extra arguments into a tuple. The join() method creates a string from the hobbies. Calling describe_person(\"Alice\", 30, \"reading\", \"hiking\", \"cooking\") prints \"Alice is 30 years old and enjoys reading, hiking, cooking.\"\nCode: python     def calculate_average(*numbers):         \"\"\"Calculates the average of any number of values.\"\"\"         if len(numbers) == 0:             return 0         return sum(numbers) / len(numbers)     print(calculate_average(5, 10, 15))     print(calculate_average()) Explanation:\nThe calculate_average(*numbers) function calculates the average of any number of arguments. It first checks if any numbers were provided (if the length of numbers is 0, it returns 0), then calculates the average by dividing the sum by the length. Calling calculate_average(5, 10, 15) returns 10.0, and calculate_average() returns 0.\n\nThese examples show different ways to define functions with varying arguments, handling edge cases, using recursion, and incorporating function documentation.",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html#modules-main-statement",
    "href": "pyws01-2-getting-started.html#modules-main-statement",
    "title": "basic python syntax",
    "section": "modules, main statement",
    "text": "modules, main statement\nHere is a sample Python script that defines two simple functions and includes a main statement to call them:\n# Function to add two numbers\ndef add_numbers(a, b):\n    return a + b\n\n# Function to subtract two numbers\ndef subtract_numbers(a, b):\n    return a - b\n\n# Main statement\nif __name__ == \"__main__\":\n    num1 = 10\n    num2 = 5\n\n    # Calling the functions\n    sum_result = add_numbers(num1, num2)\n    diff_result = subtract_numbers(num1, num2)\n\n    # Printing the results\n    print(f\"The sum of {num1} and {num2} is: {sum_result}\")\n    print(f\"The difference between {num1} and {num2} is: {diff_result}\")\nExplanation:\n\nadd_numbers(a, b): A simple function that takes two arguments and returns their sum.\nsubtract_numbers(a, b): A simple function that takes two arguments and returns their difference.\nMain statement (if __name__ == \"__main__\":): This block ensures that the code inside it runs only when the script is executed directly, not when it’s imported as a module.\nCalling the functions: Inside the main block, it calls the add_numbers and subtract_numbers functions with num1 and num2 as arguments.\nPrint results: Displays the results of the addition and subtraction operations.",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The following info is fetched from timeedit:",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "about.html#workshop-schedule",
    "href": "about.html#workshop-schedule",
    "title": "About",
    "section": "",
    "text": "The following info is fetched from timeedit:",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "about.html#workshop-map",
    "href": "about.html#workshop-map",
    "title": "About",
    "section": "Workshop map",
    "text": "Workshop map\n\n\n\nworkshop locations",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "pyws02-2-data-analysis.html",
    "href": "pyws02-2-data-analysis.html",
    "title": "result visualizations",
    "section": "",
    "text": "example gallery matplotlib\nexample gallery seaborn\nexample gallery plotnine\nexample gallery plotly\nexample gallery altair\ndownload jupyter notebook",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "result visualizations"
    ]
  },
  {
    "objectID": "pyws02-2-data-analysis.html#seaborn-descriptive",
    "href": "pyws02-2-data-analysis.html#seaborn-descriptive",
    "title": "result visualizations",
    "section": "seaborn, descriptive",
    "text": "seaborn, descriptive\n# Import necessary libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nIn this code chunk, we import the essential Python libraries needed for data manipulation and visualization:\n\npandas is used for handling data structures like DataFrames and provides functions for data manipulation.\nseaborn is a statistical data visualization library built on top of matplotlib, offering a high-level interface for drawing attractive graphs.\nmatplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB, used here as plt for plotting.\n\n\n# Load the penguins dataset\npenguins = sns.load_dataset('penguins')\nHere, we load the ‘penguins’ dataset using seaborn’s load_dataset() function and store it in a pandas DataFrame named penguins. This dataset contains measurements for different penguin species, including features like bill length, flipper length, body mass, and more.\n\n# Set the output file path\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/fig/'\nThis line sets the directory path where the generated plots will be saved. The variable output_file_path holds the string representing the file path to ensure all saved figures are organized in the specified location.\n\n# First plot: Graph two categorical variables\n# Plot the count of penguins by species and island\nplt.figure(figsize=(8, 6))\nsns.countplot(x='species', hue='island', data=penguins)\nplt.title('Count of Penguin Species by Island')\nplt.xlabel('Species')\nplt.ylabel('Count')\nplt.legend(title='Island')\nplt.tight_layout()\nplt.savefig(f'{output_file_path}penguins_species_island.png')\nplt.savefig(f'{output_file_path}penguins_species_island.pdf')\nplt.close()\nIn this code chunk, we create a bar plot to visualize the relationship between two categorical variables, species and island:\n\nplt.figure(figsize=(8, 6)) initializes a new figure with a specified size.\nsns.countplot() generates a count plot showing the number of penguins for each species, differentiated by island using the hue parameter.\nplt.title(), plt.xlabel(), and plt.ylabel() set the plot’s title and axis labels.\nplt.legend(title='Island') adds a legend with the title ‘Island’ to differentiate categories.\nplt.tight_layout() adjusts the layout to prevent clipping of labels.\nplt.savefig() saves the figure in both PNG and PDF formats to the specified output path.\nplt.close() closes the current figure to free up memory.\n\n\n# Second plot: Graph one continuous variable by one categorical using simple barplot without error bars\n# Plot the average body mass by species\nplt.figure(figsize=(8, 6))\nsns.barplot(x='species', y='body_mass_g', data=penguins, ci=None)\nplt.title('Average Body Mass by Species')\nplt.xlabel('Species')\nplt.ylabel('Body Mass (g)')\nplt.tight_layout()\nplt.savefig(f'{output_file_path}penguins_body_mass_species.png')\nplt.savefig(f'{output_file_path}penguins_body_mass_species.pdf')\nplt.close()\nThis chunk generates a bar plot to visualize a continuous variable (body_mass_g) against a categorical variable (species):\n\nsns.barplot() creates a bar plot showing the average body mass for each species. Setting ci=None removes the error bars (confidence intervals) to display simple bars.\nThe plt functions add a title and axis labels to the plot.\nThe plot is saved as both PNG and PDF files in the specified directory using plt.savefig().\nplt.close() closes the figure to free memory.\n\n\n# Third plot: Graph two continuous variables\n# Plot flipper length vs. body mass\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='flipper_length_mm', y='body_mass_g', data=penguins)\nplt.title('Flipper Length vs. Body Mass')\nplt.xlabel('Flipper Length (mm)')\nplt.ylabel('Body Mass (g)')\nplt.tight_layout()\nplt.savefig(f'{output_file_path}penguins_flipper_length_body_mass.png')\nplt.savefig(f'{output_file_path}penguins_flipper_length_body_mass.pdf')\nplt.close()\nIn the final code chunk, we create a scatter plot to examine the relationship between two continuous variables, flipper_length_mm and body_mass_g:\n\nsns.scatterplot() plots data points representing each penguin’s flipper length versus body mass.\nTitles and axis labels are added using plt.title(), plt.xlabel(), and plt.ylabel().\nThe plot is saved in both PNG and PDF formats using plt.savefig().\nplt.close() closes the figure to ensure that subsequent plots are not affected by the current figure.\n\n\n\nseaborn, subplots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load the penguins dataset into a DataFrame\ndf = sns.load_dataset('penguins')\n\n# Create a 1 row by 3 columns plot\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Left subplot: Graph two categorical variables (species and island) using a countplot\nsns.countplot(data=df, x='species', hue='island', ax=axes[0])\naxes[0].set_title('Count of Species by Island')\n\n# Middle subplot: Graph one continuous variable by one categorical using a barplot (without error bars)\nsns.barplot(data=df, x='species', y='body_mass_g', errorbar=None, ax=axes[1])\naxes[1].set_title('Average Body Mass by Species')\n\n# Right subplot: Graph two continuous variables (flipper_length_mm vs. body_mass_g) using a scatterplot\nsns.scatterplot(data=df, x='flipper_length_mm', y='body_mass_g', ax=axes[2])\naxes[2].set_title('Flipper Length vs. Body Mass')\n\n# Adjust layout\nplt.tight_layout()\n\n# Output directory\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/fig/'\n\n# Save the figure as PNG and PDF\nfig.savefig(f\"{output_file_path}seaborn_descriptive.png\")\nfig.savefig(f\"{output_file_path}seaborn_descriptive.pdf\")\n\n# Display the plot\nplt.show()\nHere is the analysis of the Python code, broken down into separate chunks, with explanations for each step:\n\n\n1. Import Libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nExplanation:\nThis step imports the necessary libraries. seaborn is used for data visualization, matplotlib.pyplot for creating plots, and pandas for data manipulation. These libraries provide powerful tools for analyzing and visualizing data in a variety of formats.\n\n\n\n2. Load the Dataset\ndf = sns.load_dataset('penguins')\nExplanation:\nThis line loads the “penguins” dataset into a Pandas DataFrame (df) using Seaborn’s load_dataset function. The dataset contains information about penguin species, measurements, and other attributes. By loading it into a DataFrame, you make it easier to perform data analysis and visualizations.\n\n\n\n3. Create the Plot Figure and Axes\n#| eval: false\n#| echo: true\n#| output: false\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nExplanation:\nThis step creates a figure (fig) with three subplots (axes) arranged in a single row using plt.subplots(). The figsize=(18, 5) argument specifies the size of the entire plot in inches. This layout allows for multiple visualizations to be displayed side-by-side in one figure.\n\n\n\n4. Create the Left Subplot\nsns.countplot(data=df, x='species', hue='island', ax=axes[0])\naxes[0].set_title('Count of Species by Island')\nExplanation:\nThis block creates a count plot using Seaborn’s countplot() function, displaying the number of penguins by species with a hue for the island column. The plot is assigned to the first subplot (axes[0]). The set_title() method adds a title to this subplot, making it easier to interpret.\n\n\n\n5. Create the Middle Subplot\nsns.barplot(data=df, x='species', y='body_mass_g', errorbar=None, ax=axes[1])\naxes[1].set_title('Average Body Mass by Species')\nExplanation:\nThis block generates a bar plot that shows the average body mass (body_mass_g) for each penguin species using sns.barplot(). The errorbar=None argument ensures that no error bars are displayed. The plot is assigned to the second subplot (axes[1]), and a title is set for clarity.\n\n\n\n6. Create the Right Subplot\nsns.scatterplot(data=df, x='flipper_length_mm', y='body_mass_g', ax=axes[2])\naxes[2].set_title('Flipper Length vs. Body Mass')\nExplanation:\nHere, a scatter plot is created using sns.scatterplot() to show the relationship between flipper_length_mm (x-axis) and body_mass_g (y-axis). This plot is added to the third subplot (axes[2]). The title provides context for what the plot represents.\n\n\n\n7. Adjust the Layout\nplt.tight_layout()\nExplanation:\nThis line adjusts the layout of the subplots to ensure they do not overlap. plt.tight_layout() automatically adjusts the spacing between subplots to make the figure look cleaner and more readable.\n\n\n\n8. Define the Output Directory and Save the Figure\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/fig/'\n\nfig.savefig(f\"{output_file_path}seaborn_descriptive.png\")\nfig.savefig(f\"{output_file_path}seaborn_descriptive.pdf\")\nExplanation:\nThe output_file_path variable defines the directory where the plots will be saved. The fig.savefig() method saves the entire figure in both PNG and PDF formats to the specified directory. This allows you to keep a copy of the visualizations for future reference or inclusion in reports.\n\n\n\n9. Display the Plot\nplt.show()\nExplanation:\nThis line displays the figure with all three subplots in an interactive window (or inline if using a Jupyter notebook). This command is necessary to visualize the plots when running the script in an environment that supports graphical output.",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "result visualizations"
    ]
  },
  {
    "objectID": "pyws02-2-data-analysis.html#seaborn-inferential",
    "href": "pyws02-2-data-analysis.html#seaborn-inferential",
    "title": "result visualizations",
    "section": "seaborn, inferential",
    "text": "seaborn, inferential\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Read the penguins dataset into a pandas DataFrame\ndf = sns.load_dataset('penguins')\n\n# Create a color palette for species\nspecies_list = df['species'].dropna().unique()\npalette = sns.color_palette('Set1', n_colors=len(species_list))\nspecies_palette = dict(zip(species_list, palette))\n\n# Create a 1 row by 3 columns plot\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Left subplot: Barplot of two categorical variables with error bars (species by island)\nsns.barplot(data=df, x='species', y='body_mass_g', hue='island', errorbar='sd', palette=palette, ax=axes[0])\naxes[0].set_title('Body Mass by Species and Island')\n\n# Middle subplot: Boxplot of one continuous variable by one categorical (body_mass_g by species)\nsns.boxplot(data=df, x='species', y='body_mass_g', hue='species', palette=palette, ax=axes[1], legend=False)\naxes[1].set_title('Body Mass Distribution by Species')\n\n# Right subplot: Scatterplot with linear regression lines and confidence intervals differentiated by species\nsns.scatterplot(data=df, x='flipper_length_mm', y='body_mass_g', hue='species', palette=species_palette, ax=axes[2])\n\n# Add linear regression lines for each species\nfor species, color in species_palette.items():\n    sns.regplot(\n        data=df[df['species'] == species],\n        x='flipper_length_mm',\n        y='body_mass_g',\n        ax=axes[2],\n        scatter=False,\n        label=species,\n        ci=95,\n        line_kws={'color': color},\n    )\n\naxes[2].legend()\naxes[2].set_title('Flipper Length vs. Body Mass by Species (Linear Regression)')\n\n# Adjust layout\nplt.tight_layout()\n\n# Save the figure as PNG and PDF\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/fig/'\nfig.savefig(f\"{output_file_path}seaborn_inferential.png\")\nfig.savefig(f\"{output_file_path}seaborn_inferential.pdf\")\n\n# Display the plot\nplt.show()\nHere is an analysis of the Python code, broken down into separate chunks, followed by explanations for each step:\n\n1. Import Libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nExplanation:\nThis step imports the necessary libraries: seaborn for statistical data visualization, matplotlib.pyplot for plotting, and pandas for data manipulation. Importing these libraries allows access to their functions, which are used for loading datasets, creating plots, and saving figures.\n\n\n\n2. Load the Penguins Dataset\ndf = sns.load_dataset('penguins')\nExplanation:\nThis line loads the “penguins” dataset into a Pandas DataFrame (df) using the Seaborn library’s load_dataset function. The dataset contains information on penguin species, measurements (e.g., body mass, flipper length), and habitat. Loading the data into a DataFrame enables easier manipulation and visualization of the dataset.\n\n\n\n3. Create a Color Palette for Species\nspecies_list = df['species'].dropna().unique()\npalette = sns.color_palette('Set1', n_colors=len(species_list))\nspecies_palette = dict(zip(species_list, palette))\nExplanation:\nThis chunk creates a color palette to differentiate penguin species in the plots: 1. species_list extracts unique species names from the species column, excluding any missing values. 2. palette generates a color palette using Seaborn’s Set1 palette with a number of colors equal to the number of species. 3. species_palette creates a dictionary mapping each species to a specific color, enabling consistent coloring across multiple plots.\n\n\n\n4. Create a 1x3 Plot Grid\n#| eval: false\n#| echo: true\n#| output: false\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nExplanation:\nThis line creates a figure with a grid of subplots arranged in one row and three columns using matplotlib. The figsize parameter sets the overall dimensions of the figure to 18 inches in width and 5 inches in height. The returned axes array will be used to plot the individual subplots.\n\n\n\n5. Create the Bar Plot (Left Subplot)\nsns.barplot(data=df, x='species', y='body_mass_g', hue='island', errorbar='sd', palette=palette, ax=axes[0])\naxes[0].set_title('Body Mass by Species and Island')\nExplanation:\nThis block creates a bar plot in the leftmost subplot (axes[0]): - Uses sns.barplot() to plot body_mass_g (y-axis) for each species (x-axis), separated by island (hue). - The errorbar='sd' parameter adds standard deviation error bars to each bar. - palette specifies the color scheme, while ax=axes[0] assigns the plot to the first subplot. - The set_title() method sets the title of the subplot.\n\n\n\n6. Create the Box Plot (Middle Subplot)\nsns.boxplot(data=df, x='species', y='body_mass_g', hue='species', palette=palette, ax=axes[1], legend=False)\naxes[1].set_title('Body Mass Distribution by Species')\nExplanation:\nThis block creates a box plot in the middle subplot (axes[1]): - sns.boxplot() displays the distribution of body_mass_g for each species. - The hue='species' parameter assigns different colors to each species, using the same palette. - legend=False suppresses the legend for this plot to avoid redundancy. - The plot is assigned to the second subplot (ax=axes[1]), and set_title() adds a title to the plot.\n\n\n\n7. Create the Scatter Plot with Regression Lines (Right Subplot)\nsns.scatterplot(data=df, x='flipper_length_mm', y='body_mass_g', hue='species', palette=species_palette, ax=axes[2])\nExplanation:\nThis line creates a scatter plot in the rightmost subplot (axes[2]): - sns.scatterplot() plots flipper_length_mm (x-axis) against body_mass_g (y-axis) for each species. - The hue='species' parameter color-codes the scatter points according to species using the previously defined species_palette. - ax=axes[2] places this plot in the third subplot.\n\n\n\n8. Add Linear Regression Lines for Each Species\nfor species, color in species_palette.items():\n    sns.regplot(\n        data=df[df['species'] == species],\n        x='flipper_length_mm',\n        y='body_mass_g',\n        ax=axes[2],\n        scatter=False,\n        label=species,\n        ci=95,\n        line_kws={'color': color},\n    )\nExplanation:\nThis block adds linear regression lines to the scatter plot for each species: - Iterates through the species_palette dictionary to plot a separate regression line for each species. - sns.regplot() fits a linear regression line with confidence intervals (ci=95), using scatter=False to suppress additional scatter points. - line_kws={'color': color} sets the color of the line to match the scatter plot dots. - Each line is labeled with the species name for legend purposes.\n\n\n\n9. Add Legend and Title for the Right Subplot\naxes[2].legend()\naxes[2].set_title('Flipper Length vs. Body Mass by Species (Linear Regression)')\nExplanation:\nThis block adds a legend to the rightmost subplot (axes[2]) to indicate which color corresponds to each species. It also sets the title of the subplot for better context.\n\n\n\n10. Adjust Layout\nplt.tight_layout()\nExplanation:\nThis line adjusts the spacing between subplots to prevent overlap and ensure a clean, organized layout. plt.tight_layout() automatically manages the subplot parameters for a visually appealing output.\n\n\n\n11. Save the Figure\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/fig/'\nfig.savefig(f\"{output_file_path}penguins_inferential_statistics_linear.png\")\nfig.savefig(f\"{output_file_path}penguins_inferential_statistics_linear.pdf\")\nExplanation:\nThis block saves the figure in both PNG and PDF formats to the specified directory (output_file_path). The fig.savefig() method captures the entire figure, including all subplots, and writes it to the desired file format.\n\n\n\n12. Display the Plot\nplt.show()\nExplanation:\nThis line displays the generated figure with all three subplots. It is especially useful when running the script in an interactive environment (e.g., Jupyter notebooks) to visualize the plot output.",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "result visualizations"
    ]
  },
  {
    "objectID": "pyws02-2-data-analysis.html#plotnine-inferential",
    "href": "pyws02-2-data-analysis.html#plotnine-inferential",
    "title": "result visualizations",
    "section": "plotnine, inferential",
    "text": "plotnine, inferential\n# Import necessary libraries\nimport polars as pl\nfrom plotnine import *\nimport seaborn as sns\nIn this code chunk, we import the required Python libraries:\n\npolars as pl: Polars is a high-performance DataFrame library for data manipulation, similar to pandas but optimized for speed.\nplotnine: A grammar of graphics plotting library for Python, inspired by ggplot2 in R. We import all functions and classes from plotnine for plotting.\nseaborn as sns: We use seaborn to load the built-in ‘penguins’ dataset into a pandas DataFrame.\n\n\n# Load the penguins dataset into a polars DataFrame\npenguins_pd = sns.load_dataset('penguins')\npenguins = pl.from_pandas(penguins_pd)\nHere, we load the ‘penguins’ dataset using seaborn’s load_dataset() function, which returns a pandas DataFrame (penguins_pd). We then convert this pandas DataFrame into a polars DataFrame (penguins) using pl.from_pandas(). This allows us to leverage polars’ efficient data manipulation capabilities.\n\n# Set the output file path\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/fig/'\nThis line sets the directory path where the generated plots will be saved. The variable output_file_path holds the string representing the file path to ensure all saved figures are organized in the specified location.\n\n# Convert polars DataFrame back to pandas for plotting with plotnine\npenguins_df = penguins.to_pandas()\nSince plotnine works with pandas DataFrames, we convert the polars DataFrame back to a pandas DataFrame using the to_pandas() method, storing it in penguins_df. This allows us to use plotnine for creating the plots.\n\n# First plot: Graph two categorical variables as barplots with error bars\n# Calculate counts and proportions\ncounts = penguins_df.groupby(['species', 'island']).size().reset_index(name='count')\ntotal_counts = counts.groupby('species')['count'].transform('sum')\ncounts['proportion'] = counts['count'] / total_counts\ncounts['se'] = (counts['proportion'] * (1 - counts['proportion']) / total_counts) ** 0.5\nIn this code chunk, we prepare the data for the first plot:\n\nWe group the data by ‘species’ and ‘island’ and count the number of observations in each group using groupby() and size().\nWe reset the index to turn the grouped data into a DataFrame with reset_index(), naming the count column as ‘count’.\nWe calculate the total counts for each species using groupby() and transform('sum').\nWe compute the proportion of each island within each species group.\nWe calculate the standard error (se) for the proportions using the formula for the standard error of a proportion.\n\n\n# Create the bar plot with error bars\nplot1 = (\n    ggplot(counts, aes(x='species', y='proportion', fill='island')) +\n    geom_bar(stat='identity', position='dodge') +\n    geom_errorbar(aes(ymin='proportion - se', ymax='proportion + se'),\n                  position=position_dodge(0.9), width=0.25) +\n    labs(title='Proportion of Penguins by Species and Island',\n         x='Species', y='Proportion') +\n    theme_minimal()\n)\nHere, we create the first plot using plotnine:\n\nWe initialize a ggplot object with counts DataFrame and specify the aesthetics (aes), mapping ‘species’ to the x-axis, ‘proportion’ to the y-axis, and ‘island’ to the fill color.\ngeom_bar(stat='identity', position='dodge') creates a bar plot using the actual values of ‘proportion’ and positions the bars side by side for each species.\ngeom_errorbar() adds error bars to the bars, representing the confidence intervals calculated earlier.\nlabs() sets the title and axis labels of the plot.\ntheme_minimal() applies a minimal theme to the plot for a clean look.\n\n\n# Save the first plot\nplot1.save(filename=f'{output_file_path}penguins_species_island_barplot.png', dpi=300)\nplot1.save(filename=f'{output_file_path}penguins_species_island_barplot.pdf', dpi=300)\nThis code saves the first plot in both PNG and PDF formats to the specified output directory, with a resolution of 300 dots per inch (dpi).\n\n# Second plot: Graph one continuous variable by one categorical using boxplot\nplot2 = (\n    ggplot(penguins_df, aes(x='species', y='body_mass_g')) +\n    geom_boxplot() +\n    labs(title='Body Mass by Species',\n         x='Species', y='Body Mass (g)') +\n    theme_minimal()\n)\nIn the second plot, we create a boxplot to visualize the distribution of the continuous variable ‘body_mass_g’ across different categories of ‘species’:\n\nWe initialize the ggplot object with penguins_df and specify the aesthetics.\ngeom_boxplot() creates a boxplot for each species, showing the median, quartiles, and potential outliers.\nlabs() sets the plot title and axis labels.\ntheme_minimal() applies a minimal theme for aesthetics.\n\n\n# Save the second plot\nplot2.save(filename=f'{output_file_path}penguins_body_mass_species_boxplot.png', dpi=300)\nplot2.save(filename=f'{output_file_path}penguins_body_mass_species_boxplot.pdf', dpi=300)\nThis code saves the second plot in both PNG and PDF formats to the specified directory, ensuring high-quality images for reports or presentations.\n\n# Third plot: Graph two continuous variables with linear regression line and confidence intervals\nplot3 = (\n    ggplot(penguins_df, aes(x='flipper_length_mm', y='body_mass_g')) +\n    geom_point() +\n    geom_smooth(method='lm') +\n    labs(title='Flipper Length vs. Body Mass with Regression Line',\n         x='Flipper Length (mm)', y='Body Mass (g)') +\n    theme_minimal()\n)\nFor the third plot, we create a scatter plot of two continuous variables with a linear regression line:\n\ngeom_point() plots individual data points of ‘flipper_length_mm’ vs. ‘body_mass_g’.\ngeom_smooth(method='lm') adds a linear regression line with confidence intervals (shaded area), indicating the relationship between the two variables.\nlabs() and theme_minimal() are used to set titles and apply a clean theme.\n\n\n# Save the third plot\nplot3.save(filename=f'{output_file_path}penguins_flipper_length_body_mass_regression.png', dpi=300)\nplot3.save(filename=f'{output_file_path}penguins_flipper_length_body_mass_regression.pdf', dpi=300)\nFinally, we save the third plot in both PNG and PDF formats to the output directory, completing the data visualization tasks.\n\nIn each code chunk, we’ve carefully explained the purpose and functionality of the Python commands used, ensuring clarity and understanding of the data manipulation and plotting processes.",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "result visualizations"
    ]
  },
  {
    "objectID": "pyws02-2-data-analysis.html#plotnine-testing",
    "href": "pyws02-2-data-analysis.html#plotnine-testing",
    "title": "result visualizations",
    "section": "plotnine, testing",
    "text": "plotnine, testing\n#| eval: true\n#| echo: true\n#| output: true\n\n# prompt: acquire dataset penguins from seaborn package, read it into a polars dataframe, generate a data analysis pipeline where bill_length is aggregated by species, pipe the aggregated data out to a plotnine bar chart with error bars\n\n#!pip install polars plotnine\n\nimport seaborn as sns\nimport polars as pl\nfrom plotnine import *\n\n# Load the penguins dataset\npenguins = sns.load_dataset('penguins')\n\n# Convert to a Polars DataFrame\ndf = pl.from_pandas(penguins)\n\n# Define the data analysis pipeline\n(\n    df\n    .group_by('species')\n    .agg([\n        pl.col('bill_length_mm').mean().alias('mean_bill_length'),\n        pl.col('bill_length_mm').std().alias('std_bill_length')\n    ])\n    .pipe(lambda df: (\n        ggplot(df, aes(x='species', y='mean_bill_length', fill='species'))\n        + geom_bar(stat='identity', position='dodge')\n        + geom_errorbar(aes(ymin='mean_bill_length - std_bill_length', ymax='mean_bill_length + std_bill_length'), width=0.2)\n        + theme_bw()\n        + labs(x='Species', y='Mean Bill Length (mm)', title='Mean Bill Length by Species')\n    ))\n    .draw()\n)",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "result visualizations"
    ]
  },
  {
    "objectID": "pyws03-0-text-analysis.html",
    "href": "pyws03-0-text-analysis.html",
    "title": "part 3: text analysis",
    "section": "",
    "text": "Reading and analyzing unstructured text presents a variety of challenges, particularly when dealing with different types of content such as articles, social media posts, or even longer documents like reports. Unstructured text, unlike tabular data, lacks a predefined format, which means it may contain irregularities such as spelling errors, slang, or varied sentence structures. For example, reading short texts like social media posts requires handling informal language, abbreviations, and emojis, whereas longer texts such as research papers or survey responses involve the challenge of organizing and summarizing large amounts of information. Rendered texts, such as those from PDFs or HTML files, often come with additional complexities like layout formatting, metadata, or embedded images, which may need to be stripped out or interpreted correctly before the actual text can be processed. Handling such varied lengths and formats requires careful preprocessing, including cleaning and standardizing the text data to make it suitable for analysis.",
    "crumbs": [
      "workshop",
      "part 3: text analysis"
    ]
  },
  {
    "objectID": "pyws03-0-text-analysis.html#reading-text-content",
    "href": "pyws03-0-text-analysis.html#reading-text-content",
    "title": "part 3: text analysis",
    "section": "",
    "text": "Reading and analyzing unstructured text presents a variety of challenges, particularly when dealing with different types of content such as articles, social media posts, or even longer documents like reports. Unstructured text, unlike tabular data, lacks a predefined format, which means it may contain irregularities such as spelling errors, slang, or varied sentence structures. For example, reading short texts like social media posts requires handling informal language, abbreviations, and emojis, whereas longer texts such as research papers or survey responses involve the challenge of organizing and summarizing large amounts of information. Rendered texts, such as those from PDFs or HTML files, often come with additional complexities like layout formatting, metadata, or embedded images, which may need to be stripped out or interpreted correctly before the actual text can be processed. Handling such varied lengths and formats requires careful preprocessing, including cleaning and standardizing the text data to make it suitable for analysis.",
    "crumbs": [
      "workshop",
      "part 3: text analysis"
    ]
  },
  {
    "objectID": "pyws03-0-text-analysis.html#text-tokenization",
    "href": "pyws03-0-text-analysis.html#text-tokenization",
    "title": "part 3: text analysis",
    "section": "text tokenization",
    "text": "text tokenization\nText tokenization is the foundational step in processing unstructured text, where the text is broken down into smaller units like words or phrases. This allows for basic analysis, such as word frequency counts or simple keyword extraction. However, more advanced techniques go beyond tokenization to capture deeper insights from text. Topic modeling, for instance, identifies latent themes within large text corpora by analyzing patterns in word co-occurrences. It helps uncover hidden structures in the data, which can be particularly useful when working with large datasets of documents, like survey responses or news articles. Sentiment analysis is another advanced technique that goes beyond simple tokenization by determining the emotional tone behind the text, whether it is positive, negative, or neutral. This is especially useful for analyzing customer feedback or social media sentiment. Finally, word vectorization techniques such as Word2Vec or GloVe transform words into numerical vectors based on their context, enabling more sophisticated tasks like measuring semantic similarity between words or phrases, clustering similar documents, or feeding text data into machine learning models. While tokenization provides a starting point, these more advanced techniques enable richer and more meaningful interpretations of textual data.",
    "crumbs": [
      "workshop",
      "part 3: text analysis"
    ]
  },
  {
    "objectID": "pyws03-2-text-analysis.html",
    "href": "pyws03-2-text-analysis.html",
    "title": "text tokenization",
    "section": "",
    "text": "code examples nltk\ncode examples spacy\ndownload jupyter notebook",
    "crumbs": [
      "workshop",
      "part 3: text analysis",
      "text tokenization"
    ]
  },
  {
    "objectID": "pyws03-2-text-analysis.html#natural-language-processing",
    "href": "pyws03-2-text-analysis.html#natural-language-processing",
    "title": "text tokenization",
    "section": "natural language processing",
    "text": "natural language processing\nimport spacy\nfrom spacy import displacy\n\n# Load the spaCy English model\nnlp = spacy.load('en_core_web_sm')\n\n# Example sentence\nsentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n\n# Process the sentence with spaCy\ndoc = nlp(sentence)\n\n# Print parts of speech tags\nprint(\"Parts of Speech:\")\nfor token in doc:\n    print(f\"{token.text}: {token.pos_} ({token.tag_})\")\n\n# Print named entities\nprint(\"\\nNamed Entities:\")\nfor ent in doc.ents:\n    print(f\"{ent.text}: {ent.label_}\")\n\n# Render the dependency parse tree in Jupyter Notebook\ndisplacy.render(doc, style='dep', jupyter=True)\n\n# Render the named entity recognition visualization in Jupyter Notebook\ndisplacy.render(doc, style='ent', jupyter=True)\nInstructions:\n\nImports:\n\nspacy is imported for natural language processing tasks.\ndisplacy from spacy is imported for visualizations.\n\nLoad Model:\n\nThe English language model en_core_web_sm is loaded using spacy.load().\n\nProcess Sentence:\n\nThe example sentence is processed to create a Doc object.\n\nParts of Speech (POS) Tags:\n\nIterating over doc, each token’s text and POS tags are printed.\n\nNamed Entities:\n\nIterating over doc.ents, each entity’s text and label are printed.\n\nVisualizations:\n\nThe dependency parse tree and named entity recognition (NER) are rendered using displacy.render() with jupyter=True to display within a Jupyter notebook.",
    "crumbs": [
      "workshop",
      "part 3: text analysis",
      "text tokenization"
    ]
  },
  {
    "objectID": "pyws03-2-text-analysis.html#spacy-sentence-processing",
    "href": "pyws03-2-text-analysis.html#spacy-sentence-processing",
    "title": "text tokenization",
    "section": "spacy sentence processing",
    "text": "spacy sentence processing\n\nImport Libraries\nIn this step, we import the necessary libraries: pandas for data manipulation, spaCy for natural language processing tasks, and os for interacting with the operating system.\n#| eval: true\n#| echo: true\n#| output: false\n\nimport pandas as pd\nimport spacy\nimport os\n\n!python -m spacy download en_core_web_sm\n\n\n\nLoad Data from TSV File\nWe load the DataFrame text_df from a TSV (Tab-Separated Values) file using pandas’ read_csv function with the separator set to tab (\\t). The input_file_path variable specifies the path to the input file. Note that two file paths are provided; the second one overwrites the first, so only the last path is used.\n# Load text_df from the TSV file\ninput_file_path = '/content/osm-cca-nlp/csv/text_data.tsv'\ninput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/text_data.tsv'\ntext_df = pd.read_csv(input_file_path, sep='\\t')\nHere, pandas reads the TSV file into a DataFrame, which allows for efficient data manipulation and analysis.\n\n\n\nLoad the spaCy Model\nWe load the spaCy English language model using spacy.load. The model en_core_web_sm is a small English model that includes vocabulary, syntax, entities, and word vectors.\n# Load the spaCy model (small English model is used here)\nnlp = spacy.load(\"en_core_web_sm\")\nThis model provides the necessary tools for tokenization, part-of-speech tagging, named entity recognition, and sentence segmentation, which are essential for processing and analyzing text data.\n\n\n\nInitialize List to Store Sentence Data\nWe initialize an empty list sentence_data to store information about each sentence extracted from the texts. This list will be populated with dictionaries containing sentence-level data.\n# Initialize an empty list to store sentence data\nsentence_data = []\nBy using a list, we can dynamically append sentence data as we process each text, which will later be converted into a pandas DataFrame for further analysis.\n\n\n\nExtract Sentences Using spaCy\nIn this step, we iterate over each row in the text_df DataFrame using iterrows(). For each text, we process the ‘cleaned_text’ column with the spaCy NLP pipeline to obtain a Doc object. We then iterate over the sentences in the Doc using doc.sents and collect the original text ID, sentence number, and the sentence text. Each sentence’s data is appended to the sentence_data list as a dictionary.\n# Iterate over the cleaned text in the DataFrame\nfor index, row in text_df.iterrows():\n    doc = nlp(row['cleaned_text'])  # Process the cleaned text with spaCy\n\n    # Iterate over the sentences in the document\n    for i, sentence in enumerate(doc.sents):\n        sentence_data.append({\n            'id': row['id'],           # Original text ID\n            'sentence_number': i + 1,  # Sentence number (starting from 1)\n            'sentence_text': sentence.text.strip()  # Sentence text\n        })\nThis process leverages spaCy’s sentence segmentation capabilities, which use linguistic rules and machine learning models to accurately split text into sentences.\n\n\n\nCreate DataFrame with Sentence Data\nWe create a new pandas DataFrame sentence_df from the sentence_data list. This DataFrame contains all the sentences extracted from the texts along with their corresponding IDs and sentence numbers.\n# Create a new DataFrame with the sentence data\nsentence_df = pd.DataFrame(sentence_data)\nUsing pandas allows us to efficiently organize and manipulate the sentence-level data for analysis or storage.\n\n\n\nSave Sentence Data to TSV File\nWe save the sentence_df DataFrame to a TSV file using the to_csv method, specifying the tab separator (\\t) and setting index=False to exclude the DataFrame’s index from the output file. Again, two output file paths are provided, with the second one overwriting the first.\n# Save the sentence_df DataFrame as a TSV file\noutput_file_path = '/content/osm-cca-nlp/csv/sentence_data.tsv'\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/sentence_data.tsv'\nsentence_df.to_csv(output_file_path, sep='\\t', index=False)\nThis step ensures that the extracted sentence data is saved in a structured format, which can be easily shared or used in subsequent analyses.\n\n\n\nDisplay the Sentence DataFrame\nFinally, we display the sentence_df DataFrame by printing it to the console. This allows us to verify the contents and ensure that the sentence extraction was successful.\n# Display the sentence DataFrame\nprint(sentence_df)\nViewing the DataFrame helps in quick validation of the data processing steps and provides an immediate look at the results of our sentence extraction.\n\nSummary:\n\npandas (pd): Used for data manipulation and storage in DataFrames, providing efficient methods to read from and write to various file formats.\nspaCy (spacy): Utilized for advanced natural language processing tasks, particularly for sentence segmentation in this code.\nos module: Although imported, it isn’t actively used in this code snippet. Typically, it would be used for file path manipulations or checking file existence.\nData Processing Steps:\n\nLoading text data from a TSV file.\nProcessing text with spaCy to extract sentences.\nStoring the sentences along with metadata in a DataFrame.\nSaving the processed data back to a TSV file for future use.\n\n\nThis modular approach breaks down the task into manageable steps, making the code easier to understand and maintain.",
    "crumbs": [
      "workshop",
      "part 3: text analysis",
      "text tokenization"
    ]
  },
  {
    "objectID": "pyws03-2-text-analysis.html#spacy-token-processing",
    "href": "pyws03-2-text-analysis.html#spacy-token-processing",
    "title": "text tokenization",
    "section": "spacy token processing",
    "text": "spacy token processing\n\nImport Libraries\nIn this step, we import the necessary libraries: pandas for data manipulation, spaCy for natural language processing tasks, and os for interacting with the operating system.\nimport pandas as pd\nimport spacy\nimport os\n\n\n\nLoad Sentence Data from TSV File\nWe load the sentence_df DataFrame from a TSV (Tab-Separated Values) file using pandas’ read_csv function with the separator set to tab (\\t). The input_file_path variable specifies the path to the input file. Note that the second assignment of input_file_path overwrites the first, so only the last path is used.\n# Load sentence_df from the TSV file\ninput_file_path = '/content/osm-cca-nlp/csv/sentence_data.tsv'\ninput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/sentence_data.tsv'\nsentence_df = pd.read_csv(input_file_path, sep='\\t')\nBy using pandas, we efficiently read the TSV file into a DataFrame for further processing.\n\n\n\nLoad the spaCy Model\nWe load the spaCy English language model en_core_web_sm using spacy.load(). This model provides tools for tokenization, part-of-speech tagging, named entity recognition, and more.\n# Load the spaCy model (small English model is used here)\nnlp = spacy.load(\"en_core_web_sm\")\nThe nlp object now contains the language model, which we’ll use to process text data.\n\n\n\nInitialize List to Store Token Data\nWe initialize an empty list token_data to store information about each token extracted from the sentences. This list will accumulate dictionaries containing token-level data.\n# Initialize an empty list to store token data\ntoken_data = []\nThis prepares us to collect detailed linguistic information from each sentence.\n\n\n\nExtract Tokens from Sentences Using spaCy\nWe iterate over each sentence in sentence_df using iterrows(). For each sentence, we process it with spaCy to obtain a Doc object, which contains the tokens and their linguistic annotations. We then iterate over each token in the Doc, extracting attributes such as the token’s text, lemma, part-of-speech tag, and named entity type. This information is stored in the token_data list as dictionaries.\n# Iterate over the sentences in the sentence_df DataFrame\nfor index, row in sentence_df.iterrows():\n    doc = nlp(row['sentence_text'])  # Process the sentence text with spaCy\n\n    # Iterate over the tokens in the sentence\n    for j, token in enumerate(doc):\n        token_data.append({\n            'id': row['id'],                            # Original text ID\n            'sentence_number': row['sentence_number'],  # Sentence number\n            'token_number': j + 1,                      # Token number (starting from 1)\n            'token_text': token.text,                   # Token text\n            'token_lemma': token.lemma_,                # Token lemma\n            'token_pos': token.pos_,                    # Token part of speech\n            'token_entity': token.ent_type_             # Token entity type (if any)\n        })\nThis step leverages spaCy’s powerful NLP features to extract and annotate tokens, which is essential for detailed text analysis.\n\n\n\nCreate a DataFrame with Token Data\nWe convert the token_data list into a pandas DataFrame called token_df. This DataFrame organizes the token-level information in a tabular format, making it easier to analyze and manipulate.\n# Create a new DataFrame with the token data\ntoken_df = pd.DataFrame(token_data)\nUsing pandas allows us to handle large amounts of data efficiently and provides tools for data analysis.\n\n\n\nSave Token Data to TSV File\nWe save the token_df DataFrame to a TSV file using pandas’ to_csv() method. We set the separator to tab (\\t) and index=False to exclude the DataFrame’s index from the output file. Similar to before, the second assignment of output_file_path overwrites the first.\n# Save the token_df DataFrame as a TSV file\noutput_file_path = '/content/osm-cca-nlp/csv/token_data.tsv'\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/token_data.tsv'\ntoken_df.to_csv(output_file_path, sep='\\t', index=False)\nThis saves our token-level data in a structured format that can be shared or used in future analyses.\n\n\n\nDisplay the Token DataFrame\nFinally, we print the token_df DataFrame to display the collected token data. This allows us to verify that the token extraction and annotation processes were successful.\n# Display the token DataFrame\nprint(token_df)\nViewing the DataFrame provides immediate feedback on the results of our processing pipeline.\n\nSummary:\n\npandas (pd): Used for reading and writing TSV files and handling DataFrames.\nspaCy (spacy): Utilized for processing text to extract tokens, lemmas, parts of speech, and named entities.\nData Processing Steps:\n\nLoading sentence-level data from a TSV file.\nProcessing sentences with spaCy to extract token-level information.\nStoring and organizing token data in a pandas DataFrame.\nSaving the token data to a TSV file for future use or analysis.\n\n\nThis modular approach ensures that each part of the code is focused on a specific task, making it easier to understand and maintain.\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\n# Load token_df from the TSV file\ninput_file_path = '/content/osm-cca-nlp/csv/token_data.tsv'\ninput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/token_data.tsv'\ntoken_df = pd.read_csv(input_file_path, sep='\\t')\n\n# Filter the DataFrame to keep only rows where the part of speech is 'NOUN'\nnoun_df = token_df[token_df['token_pos'] == 'NOUN']\n\n# Group by the lemma and count the occurrences of each lemma\nlemma_counts = noun_df['token_lemma'].value_counts().reset_index()\n\n# Rename the columns for clarity\nlemma_counts.columns = ['lemma', 'count']\n\n# Get the 20 most frequent lemmas\ntop_lemmas = lemma_counts.head(20)\n\n# Plot the 20 most frequent nouns using Seaborn\nplt.figure(figsize=(10, 8))\nsns.barplot(x='count', y='lemma', data=top_lemmas, palette='viridis')\nplt.title('Top 20 Most Frequent Nouns')\nplt.xlabel('Count')\nplt.ylabel('Lemma')\n\n# Save the figure to a PNG file\noutput_file_path = '/content/osm-cca-nlp/fig/token_noun.png'\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/fig/token_noun.png'\nplt.savefig(output_file_path)\n\n# Display the plot\nplt.show()",
    "crumbs": [
      "workshop",
      "part 3: text analysis",
      "text tokenization"
    ]
  },
  {
    "objectID": "pyws03-2-text-analysis.html#text-vectorization",
    "href": "pyws03-2-text-analysis.html#text-vectorization",
    "title": "text tokenization",
    "section": "text vectorization",
    "text": "text vectorization\nimport spacy\n\n# Load spaCy model.. or en_core_web_sm if you don’t need word embeddings\n#nlp = spacy.load(\"en_core_web_md\")\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef preprocess_text(text):\n    doc = nlp(text)\n    tokens = []\n    for token in doc:\n        # Remove stopwords, punctuation, and non-alphabetic tokens\n        if not token.is_stop and not token.is_punct and token.is_alpha:\n            tokens.append(token.lemma_.lower())  # Append lemmatized form and lowercase\n    return tokens\n\n# Generate news headlines with some words occurring twice in the same headline\nheadlines = [\n    \"Apple launches new iPhone iPhone in September\",\n    \"Google announces AI advancements with AI in health sector\",\n    \"Tesla's electric cars revolutionize the electric car industry\",\n    \"Amazon announces new grocery delivery for grocery stores\",\n    \"Netflix announces new series based on new AI-based technology\",\n    \"Microsoft launches cloud services and cloud infrastructure\",\n    \"Facebook unveils privacy controls with enhanced privacy features\",\n    \"Pfizer launches vaccine trials for new vaccine prevention\",\n    \"Nike launches new eco-friendly shoe and shoe design\",\n    \"BMW announces electric car breakthrough in the electric vehicle market\"\n]\n\n# Preprocess the list of headlines\npreprocessed_headlines = [\" \".join(preprocess_text(headline)) for headline in headlines]\nprint(preprocessed_headlines)  # Output preprocessed headlines\nfrom gensim import corpora\nfrom gensim.models import TfidfModel\n\n# Tokenize preprocessed headlines\ntokenized_headlines = [headline.split() for headline in preprocessed_headlines]\n\n# Create a dictionary of words\ndictionary = corpora.Dictionary(tokenized_headlines)\n\n# Create a Bag of Words corpus\nbow_corpus = [dictionary.doc2bow(text) for text in tokenized_headlines]\n\n# Or, alternatively, create a TF-IDF corpus\ntfidf = TfidfModel(bow_corpus)\ntfidf_corpus = tfidf[bow_corpus]\nimport pandas as pd\nfrom gensim import corpora\nfrom gensim.models import TfidfModel\n\n# Assuming bow_corpus is already defined and dictionary is available\n# dictionary = corpora.Dictionary(tokenized_headlines)\n\n# Create a list of terms (vocabulary) from the dictionary\nterms = [dictionary[i] for i in range(len(dictionary))]\n\n# Create a document-term matrix for the Bag of Words (BoW) corpus\nbow_doc_term_matrix = pd.DataFrame([[dict(doc).get(i, 0) for doc in bow_corpus] for i in range(len(dictionary))], index=terms)\n\n# Display the BoW document-term matrix\n#tools.display_dataframe_to_user(name=\"BoW Document-Term Matrix\", dataframe=bow_doc_term_matrix)\nprint(bow_doc_term_matrix)",
    "crumbs": [
      "workshop",
      "part 3: text analysis",
      "text tokenization"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Welcome! This intro to Python workshop is oriented around the principles of Open Science. Python, with its versatile ecosystem and powerful libraries, is a cornerstone for computational data analysis in social science research, enabling key Open Science principles like replicability, transparency, collaboration, and the use of open-source software. Python scripts allow researchers to automate and document their analysis, ensuring replicability—other researchers can run the same code on the same data to verify findings. Transparency is achieved by openly sharing the code, data, and methodology, so that every step of the research process is accessible and understandable. Python also fosters collaboration by allowing multiple researchers to contribute to the same project, often through platforms like GitHub, where code is shared, versioned, and improved. Finally, Python itself is open-source software, meaning anyone can use, modify, and distribute it freely, promoting equitable access to powerful research tools and fostering an inclusive scientific community.\nWorkshop link and Schedule\n\nPart 1: Getting Started\nIn this part, participants will be introduced to Python environments, with a focus on using Google Colab Notebooks, an accessible and powerful tool for coding in Python. We will cover the installation and setup process, ensuring everyone is ready to run Python code in their browsers. The session will also explore basic Python syntax, including variables, data types, and control structures like loops and conditionals, laying the foundation for more advanced applications in social science research.\n\n\nPart 2: Data Analysis\nThis section focuses on performing data analysis with Python, using libraries like Pandas for dataframe manipulation. Participants will learn how to load, clean, and transform data, enabling them to analyze datasets commonly used in social science. We’ll also dive into results visualization using Matplotlib and Seaborn, teaching participants how to create informative charts and graphs to present their findings effectively.\n\n\nPart 3: Text Analysis\nIn this part, participants will explore the basics of text analysis in Python, starting with reading text data from various sources such as documents or online content. We will cover text tokenization, the process of breaking text into individual words or phrases, using libraries like spaCy and NLTK. This will allow participants to process, analyze, and extract meaningful insights from large volumes of textual data, such as social media posts or survey responses.\n\n\nPart 4: Image Analysis\nThe image analysis section will introduce participants to working with visual data in Python. We’ll cover how to read and process image content using libraries like OpenCV and Pillow. Participants will also explore basic object recognition techniques, learning how to detect and classify objects within images, which is particularly useful for social science fields that rely on visual data, such as media studies or behavioral analysis.\n\n\nPart 5: Data Collection\nThe final part of the workshop focuses on data collection techniques using Python. Participants will learn web scraping methods to gather data from websites using tools like BeautifulSoup and Scrapy. Additionally, we will explore how to design and run web experiments, allowing researchers to collect behavioral data from users in real-time. These skills will empower participants to gather the data they need for their social science research projects.\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "start"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The following info is fetched from timeedit:"
  },
  {
    "objectID": "pyws02-1-data-analysis.html",
    "href": "pyws02-1-data-analysis.html",
    "title": "dataframe manipulation",
    "section": "",
    "text": "dataframe examples pandas\ndataframe examples polars\ndownload jupyter notebook",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "dataframe manipulation"
    ]
  },
  {
    "objectID": "pyws02-1-data-analysis.html#read-and-explore-data",
    "href": "pyws02-1-data-analysis.html#read-and-explore-data",
    "title": "dataframe manipulation",
    "section": "read and explore data",
    "text": "read and explore data\nHere is an analysis of your code, separated into logical steps with each step in its own code chunk, followed by explanations:\n\n1. Import necessary libraries\nimport seaborn as sns\nimport pandas as pd\nExplanation:\nThis step imports the required libraries for the script. seaborn is a data visualization library that provides built-in datasets, and pandas is used for data manipulation and analysis. Importing these libraries allows the script to use their functions, such as loading a dataset and manipulating DataFrames.\n\n\n\n2. Load the dataset and select specific columns\ndf = sns.load_dataset(\"penguins\")\ndf_selected = df[['body_mass_g', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'island']]\nExplanation:\nHere, the penguins dataset from Seaborn is loaded into a Pandas DataFrame (df). The next step selects specific columns of interest (body_mass_g, bill_length_mm, bill_depth_mm, flipper_length_mm, and island) and stores them in df_selected. These columns represent both the dependent and independent variables used for further analysis.\n\n\n\n3. Rename selected columns for easier reference\ndf_renamed = df_selected.rename(columns={\n    'body_mass_g': 'dep_var', \n    'bill_length_mm': 'indep_var_1', \n    'bill_depth_mm': 'indep_var_2', \n    'flipper_length_mm': 'indep_var_3', \n    'island': 'indep_var_4'\n})\nExplanation:\nThis step renames the columns of df_selected to more generic names for easier reference. The dependent variable (body_mass_g) is renamed to dep_var, and the independent variables are renamed to indep_var_1, indep_var_2, indep_var_3, and indep_var_4 (for island). This renamed DataFrame (df_renamed) is used for subsequent analysis.\n\n\n\n4. Save the renamed DataFrame as a TSV file\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/csv/data.tsv'\ndf_renamed.to_csv(output_file_path, sep='\\t', index=False)\nExplanation:\nThe renamed DataFrame is saved to a TSV (tab-separated values) file at the specified path. The to_csv() method is used with the sep='\\t' argument to ensure that the file is saved in TSV format. The index=False option prevents the DataFrame index from being written to the file.\n\n\n\n5. Generate summary statistics of the DataFrame\nsummary_stats = df_renamed.describe()\nExplanation:\nThis step generates summary statistics for all numeric columns in the DataFrame using the describe() function. The resulting DataFrame (summary_stats) contains descriptive statistics such as count, mean, standard deviation, minimum, and maximum values, as well as the quartile ranges for the selected variables.\n\n\n\n6. Retrieve the first five records of the DataFrame\nfirst_five_records = df_renamed.head()\nExplanation:\nThe head() function retrieves the first five rows of the DataFrame. This is useful for a quick inspection of the dataset to verify that the data was loaded and renamed correctly. The first_five_records DataFrame contains the first five records of the renamed DataFrame.\n\n\n\n7. Convert a column to a categorical data type\ndf_renamed['indep_var_4'] = df_renamed['indep_var_4'].astype('category')\nExplanation:\nThis step converts the indep_var_4 column (formerly island) to a categorical data type using astype('category'). Categorical data types are more memory efficient and appropriate when dealing with a limited number of distinct values, such as categorical variables in a dataset.\n\n\n\n8. Fill missing values in numeric columns with their mean\ndf_filled = df_renamed.fillna(df_renamed.mean(numeric_only=True))\nExplanation:\nIn this step, any missing values in the numeric columns of df_renamed are filled with the mean value of each column using fillna(). The mean(numeric_only=True) calculates the mean for only numeric columns, and fillna() replaces the missing values with these means. The modified DataFrame is stored as df_filled.\n\n\n\n9. Remove rows with any remaining missing values\ndf_no_missing = df_filled.dropna()\nExplanation:\nHere, the dropna() function is used to remove any rows that still contain missing values in the DataFrame after filling the numeric columns. Rows with missing values in non-numeric columns will be dropped. The cleaned DataFrame is stored as df_no_missing.\n\n\n\n10. Remove duplicate records\ndf_no_duplicates = df_no_missing.drop_duplicates()\nExplanation:\nThis step removes any duplicate rows from the DataFrame using the drop_duplicates() method. Duplicate rows are those where all column values are identical. The resulting DataFrame (df_no_duplicates) contains only unique rows, ensuring there are no duplicate records in the dataset.\n\n\n\n11. Output summary statistics and first five records\nprint(summary_stats)\nprint(first_five_records)\nExplanation:\nThe final step prints the summary statistics generated earlier (summary_stats) and the first five records (first_five_records) of the renamed DataFrame. This provides an overview of the dataset and allows verification that the data processing steps were applied correctly.",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "dataframe manipulation"
    ]
  },
  {
    "objectID": "pyws02-1-data-analysis.html#select-and-group-data",
    "href": "pyws02-1-data-analysis.html#select-and-group-data",
    "title": "dataframe manipulation",
    "section": "select and group data",
    "text": "select and group data\nHere is an analysis of your Python code, separated into logical steps with each step in its own code chunk, followed by explanations:\n\n1. Import the necessary library\nimport pandas as pd\nExplanation:\nThis line imports the Pandas library, a powerful Python tool for data manipulation and analysis. By importing Pandas, the script gains access to functions like reading data, filtering, grouping, and saving results.\n\n\n\n2. Load DataFrame from TSV File\ninput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/csv/data.tsv'\ndf = pd.read_csv(input_file_path, sep='\\t')\nExplanation:\nThis block reads a TSV (tab-separated values) file from the specified file path (input_file_path) into a Pandas DataFrame (df). The sep='\\t' argument specifies that the file uses tabs as delimiters. The result is a DataFrame containing the data from the TSV file, with columns and rows ready for further manipulation.\n\n\n\n3. Filter the Data\nfiltered_df = df[df['dep_var'] &gt; 3500]\nExplanation:\nHere, the DataFrame is filtered to include only rows where the value in the dep_var column is greater than 3500. The resulting filtered DataFrame (filtered_df) contains a subset of the original data that meets this condition.\n\n\n\n4. Select Specific Columns\nselected_columns = filtered_df[['dep_var', 'indep_var_1', 'indep_var_4']]\nExplanation:\nIn this step, a new DataFrame (selected_columns) is created by selecting only the specified columns (dep_var, indep_var_1, and indep_var_4) from the previously filtered DataFrame. This reduces the dataset to just the relevant columns needed for further analysis.\n\n\n\n5. Group Data and Calculate the Mean\ngrouped_data = df.groupby('indep_var_4')['dep_var'].mean().reset_index()\nExplanation:\nThis block groups the data by the indep_var_4 column and calculates the mean of the dep_var column for each group. The result is stored in grouped_data, a DataFrame containing the unique values of indep_var_4 and their corresponding mean dep_var values. The reset_index() function ensures the grouped values are converted back into a DataFrame format.\n\n\n\n6. Merge DataFrames\nmerged_df = pd.merge(df, grouped_data, on='indep_var_4', suffixes=('', '_mean'))\nExplanation:\nThis step merges the original DataFrame (df) with the grouped_data DataFrame on the indep_var_4 column. The result (merged_df) contains all original columns from df along with the mean dep_var for each group. The suffixes=('', '_mean') ensures that the new dep_var_mean column has a distinct name.\n\n\n\n7. Calculate a New Column\nmerged_df['dep_var_diff'] = merged_df['dep_var'] - merged_df['dep_var_mean']\nExplanation:\nA new column (dep_var_diff) is added to the merged_df DataFrame. This column represents the difference between the original dep_var values and the mean dep_var values for each group (from the merged dep_var_mean column). The result provides insight into how each dep_var deviates from the group mean.\n\n\n\n8. Create a Pivot Table\npivot_table = merged_df.pivot_table(values='dep_var', index='indep_var_4', aggfunc='mean')\nExplanation:\nThis block creates a pivot table from merged_df, where indep_var_4 becomes the index, and the mean of dep_var is calculated for each value of indep_var_4. The result is stored in the pivot_table DataFrame, which aggregates the data by indep_var_4 and provides a summary of the mean dep_var.\n\n\n\n9. Save the Pivot Table as a TSV File\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/csv/data_pivot.tsv'\npivot_table.to_csv(output_file_path, sep='\\t')\nExplanation:\nIn this step, the pivot_table DataFrame is saved as a TSV file to the specified path (output_file_path). The sep='\\t' argument ensures that the data is saved in tab-separated format. This allows the pivot table to be stored and used for further analysis or reporting.\n\n\n\n10. Output Results to Verify\nprint(filtered_df.head())\nprint(selected_columns.head())\nprint(grouped_data.head())\nprint(merged_df.head())\nprint(pivot_table)\nExplanation:\nThis final block prints the first few rows (head()) of various DataFrames, including the filtered data, selected columns, grouped data, merged data, and the pivot table. These print statements allow you to verify the results of each step in the analysis.",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "dataframe manipulation"
    ]
  },
  {
    "objectID": "pyws02-1-data-analysis.html#inferential-data-analysis",
    "href": "pyws02-1-data-analysis.html#inferential-data-analysis",
    "title": "dataframe manipulation",
    "section": "inferential data analysis",
    "text": "inferential data analysis\n# Import necessary libraries\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Load the penguins dataset\npenguins = sns.load_dataset('penguins')\nIn this step, we import the required libraries for data manipulation and visualization. We use seaborn to load the built-in ‘penguins’ dataset into a pandas DataFrame called penguins.\n\n#| colab: {base_uri: 'https://localhost:8080/'}\n# Display the first few rows to inspect the dataset\nprint(penguins.head())\n\n# Check for missing values\nprint(\"\\nMissing values in each column:\")\nprint(penguins.isnull().sum())\nWe use penguins.head() to preview the first few rows of the DataFrame, allowing us to understand the structure and contents of the data. The penguins.isnull().sum() operation checks each column for missing values by summing up the number of NaN entries, providing insight into data completeness.\n\n# Drop rows with missing values\npenguins_clean = penguins.dropna()\nTo handle missing data, we use dropna() to remove any rows that contain NaN values. This operation results in a new DataFrame penguins_clean that contains only complete cases, ensuring the integrity of subsequent analyses.\n\n#| colab: {base_uri: 'https://localhost:8080/'}\n# Descriptive statistics for numerical variables\nprint(\"\\nDescriptive statistics:\")\nprint(penguins_clean.describe())\nWe generate descriptive statistics using penguins_clean.describe(), which calculates summary metrics like mean, standard deviation, and quartiles for each numerical column. This helps in understanding the distribution and central tendencies of the data.\n\n#| colab: {base_uri: 'https://localhost:8080/', height: 630}\n# Histograms for numerical variables\nnumerical_vars = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\npenguins_clean[numerical_vars].hist(bins=15, figsize=(10, 6))\nplt.suptitle('Histograms of Numerical Variables')\nplt.tight_layout()\nplt.show()\nWe create histograms for numerical variables using DataFrame.hist(), which plots the frequency distribution of each variable in numerical_vars. The histograms help visualize the distribution shape, skewness, and potential outliers in the data.\n\n#| colab: {base_uri: 'https://localhost:8080/', height: 827}\n# Boxplots to detect outliers\nplt.figure(figsize=(12, 8))\nfor i, var in enumerate(numerical_vars, 1):\n    plt.subplot(2, 2, i)\n    sns.boxplot(y=penguins_clean[var])\n    plt.title(f'Boxplot of {var}')\nplt.tight_layout()\nplt.show()\nTo detect outliers, we use sns.boxplot() for each numerical variable, plotting them in a grid layout. Boxplots display the median, quartiles, and potential outliers as individual points, making it easier to identify anomalies in the data.\n\n# Identify outliers using the IQR method\nQ1 = penguins_clean[numerical_vars].quantile(0.25)\nQ3 = penguins_clean[numerical_vars].quantile(0.75)\nIQR = Q3 - Q1\noutlier_condition = ((penguins_clean[numerical_vars] &lt; (Q1 - 1.5 * IQR)) | (penguins_clean[numerical_vars] &gt; (Q3 + 1.5 * IQR))).any(axis=1)\n\n# Remove outliers\npenguins_no_outliers = penguins_clean[~outlier_condition]\nWe calculate the Interquartile Range (IQR) for each numerical variable to identify outliers using the 1.5 * IQR rule. The condition outlier_condition flags rows with outliers, and we create a new DataFrame penguins_no_outliers by excluding these rows, thus cleaning the dataset.\n\n#| colab: {base_uri: 'https://localhost:8080/'}\n# Compare dataset sizes before and after removing outliers\nprint(\"\\nDataset size before removing outliers:\", penguins_clean.shape)\nprint(\"Dataset size after removing outliers:\", penguins_no_outliers.shape)\nWe use DataFrame.shape to compare the number of rows and columns before and after outlier removal. This helps assess the impact of outlier elimination on the dataset size.\n\n#| colab: {base_uri: 'https://localhost:8080/'}\n# Correlation matrix\nnumerical_cols = penguins_no_outliers.select_dtypes(include=[np.number]).columns\ncorr_matrix = penguins_no_outliers[numerical_cols].corr()\nprint(\"\\nCorrelation matrix:\")\nprint(corr_matrix)\nWe compute the correlation matrix using DataFrame.corr(), which calculates pairwise correlation coefficients between numerical variables. This identifies the strength and direction of linear relationships between variables.\n\n#| colab: {base_uri: 'https://localhost:8080/', height: 565}\n# Visualize the correlation matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()\nWe visualize the correlation matrix using sns.heatmap(), which creates a color-coded matrix where each cell represents the correlation coefficient between variables. The annot=True parameter displays the numerical values within the cells for precise interpretation.\n\n#| colab: {base_uri: 'https://localhost:8080/'}\n# Select variables with the highest correlation to body_mass_g\nprint(\"\\nCorrelation with body_mass_g:\")\nprint(corr_matrix['body_mass_g'].sort_values(ascending=False))\nWe extract the correlation coefficients of all variables with body_mass_g by accessing the corresponding column in the correlation matrix. Sorting these values helps identify which variables have the strongest linear relationship with body mass.\n\n# Prepare data for regression\n# Encode categorical variables\npenguins_encoded = pd.get_dummies(penguins_no_outliers, drop_first=True)\nWe use pd.get_dummies() to encode categorical variables into numerical format suitable for regression analysis. The drop_first=True parameter avoids multicollinearity by removing the first category in each encoded variable.\n\n# Prepare data for regression\n# Encode categorical variables\npenguins_encoded = pd.get_dummies(penguins_no_outliers, drop_first=True)\n\n# Define dependent and independent variables\nX = penguins_encoded.drop(['body_mass_g'], axis=1)\ny = penguins_encoded['body_mass_g']\n\n# Add a constant to the independent variables\nX = sm.add_constant(X)\nWe define the independent variables X by dropping the target variable body_mass_g and the year column from the encoded DataFrame. The dependent variable y is set as body_mass_g. We add a constant term to X using sm.add_constant() to include the intercept in the regression model.\n\n#| colab: {base_uri: 'https://localhost:8080/'}\n# Prepare data for regression\n# Encode categorical variables\npenguins_encoded = pd.get_dummies(penguins_no_outliers, drop_first=True)\n\n# Define dependent and independent variables\nX = penguins_encoded.drop(['body_mass_g'], axis=1)\ny = penguins_encoded['body_mass_g']\n\n# Convert boolean columns to int64\nbool_cols = X.select_dtypes(include=['bool']).columns\nX[bool_cols] = X[bool_cols].astype(int)\n\n# Now check data types\nprint(\"Data types of X after converting bools to int:\")\nprint(X.dtypes)\n\n# Add a constant to the independent variables\nX = sm.add_constant(X)\n\n# Fit the multiple linear regression model\nmodel = sm.OLS(y, X).fit()\n\n# Print the model summary\nprint(\"\\nRegression Model Summary:\")\nprint(model.summary())\nWe fit a multiple linear regression model using sm.OLS() from the statsmodels library and call .fit() to train the model. The model.summary() function provides a comprehensive summary of the regression results, including coefficients, p-values, and goodness-of-fit metrics.\n\n#| colab: {base_uri: 'https://localhost:8080/', height: 584}\n# Plot actual vs. predicted body mass\nplt.figure(figsize=(8, 6))\nplt.scatter(y, model.predict(X), alpha=0.7)\nplt.xlabel('Actual Body Mass (g)')\nplt.ylabel('Predicted Body Mass (g)')\nplt.title('Actual vs. Predicted Body Mass')\nplt.show()\nWe create a scatter plot to compare the actual and predicted body mass values using plt.scatter(). This visualization helps assess how well the model predictions align with the actual data, indicating the model’s predictive performance.\n\n#| colab: {base_uri: 'https://localhost:8080/', height: 584}\n# Residual plot to check for homoscedasticity\nplt.figure(figsize=(8, 6))\nsns.residplot(x=model.predict(X), y=model.resid, lowess=True, line_kws={'color': 'red'})\nplt.xlabel('Predicted Body Mass (g)')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()\nWe generate a residual plot using sns.residplot() to examine the homoscedasticity (constant variance) of residuals. The plot displays residuals against predicted values, and a red lowess line indicates any systematic patterns, which helps in diagnosing model assumptions.",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "dataframe manipulation"
    ]
  },
  {
    "objectID": "pyws01-1-getting-started.html",
    "href": "pyws01-1-getting-started.html",
    "title": "python environments",
    "section": "",
    "text": "sign in to google account to connect colab cloud app\n\n\n\nhttps://colab.research.google.com/",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "python environments"
    ]
  },
  {
    "objectID": "pyws01-1-getting-started.html#google-colab",
    "href": "pyws01-1-getting-started.html#google-colab",
    "title": "python environments",
    "section": "",
    "text": "sign in to google account to connect colab cloud app\n\n\n\nhttps://colab.research.google.com/",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "python environments"
    ]
  },
  {
    "objectID": "pyws01-1-getting-started.html#visual-studio-code",
    "href": "pyws01-1-getting-started.html#visual-studio-code",
    "title": "python environments",
    "section": "visual studio code",
    "text": "visual studio code\n\n\n\ndownload and install vscode app on local computer\n\n\n\nhttps://code.visualstudio.com/download\nhttps://www.python.org/downloads/",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "python environments"
    ]
  },
  {
    "objectID": "pyws01-1-getting-started.html#anaconda",
    "href": "pyws01-1-getting-started.html#anaconda",
    "title": "python environments",
    "section": "anaconda",
    "text": "anaconda\n\n\n\ndownload and install anaconda on local computer\n\n\n\nhttps://www.anaconda.com/download",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "python environments"
    ]
  },
  {
    "objectID": "pyws02-0-data-analysis.html",
    "href": "pyws02-0-data-analysis.html",
    "title": "part 2: data analysis",
    "section": "",
    "text": "Dataframe operations in Python, particularly with the Pandas library, allow for highly flexible and efficient data manipulation, making it a cornerstone of data analysis in fields like social science. One of the most common operations involves loading data from various sources such as CSV files, databases, or APIs, transforming it into a structured dataframe for analysis. From there, you can perform cleaning operations, such as handling missing values, filtering rows or columns, and converting data types. These cleaning steps are essential in real-world data analysis, where datasets are rarely perfect. Once the data is clean, more advanced dataframe operations come into play, such as grouping and aggregating data by specific columns, which allows users to extract insights like means, counts, or sums based on key categories. Pandas also supports joining and merging data from multiple sources, enabling researchers to combine different datasets for more comprehensive analyses. These operations contrast with the more manual approaches used in tools like Excel, where data manipulation might involve repetitive tasks like copy-pasting or formula adjustments, which can be error-prone and less scalable.",
    "crumbs": [
      "workshop",
      "part 2: data analysis"
    ]
  },
  {
    "objectID": "pyws02-0-data-analysis.html#dataframe-manipulation",
    "href": "pyws02-0-data-analysis.html#dataframe-manipulation",
    "title": "part 2: data analysis",
    "section": "",
    "text": "Dataframe operations in Python, particularly with the Pandas library, allow for highly flexible and efficient data manipulation, making it a cornerstone of data analysis in fields like social science. One of the most common operations involves loading data from various sources such as CSV files, databases, or APIs, transforming it into a structured dataframe for analysis. From there, you can perform cleaning operations, such as handling missing values, filtering rows or columns, and converting data types. These cleaning steps are essential in real-world data analysis, where datasets are rarely perfect. Once the data is clean, more advanced dataframe operations come into play, such as grouping and aggregating data by specific columns, which allows users to extract insights like means, counts, or sums based on key categories. Pandas also supports joining and merging data from multiple sources, enabling researchers to combine different datasets for more comprehensive analyses. These operations contrast with the more manual approaches used in tools like Excel, where data manipulation might involve repetitive tasks like copy-pasting or formula adjustments, which can be error-prone and less scalable.",
    "crumbs": [
      "workshop",
      "part 2: data analysis"
    ]
  },
  {
    "objectID": "pyws02-0-data-analysis.html#result-visualizations",
    "href": "pyws02-0-data-analysis.html#result-visualizations",
    "title": "part 2: data analysis",
    "section": "result visualizations",
    "text": "result visualizations\nWhen it comes to data visualization, it’s important to distinguish between descriptive and inferential visualizations, both of which play a key role in data analysis. Descriptive visualizations, often created using libraries like Matplotlib and Seaborn, are used to summarize data in a visually intuitive way. Examples include bar charts, histograms, and scatter plots, which help the analyst understand the distribution, trends, or relationships within a dataset. These types of visualizations are valuable for exploratory data analysis, where the goal is to make sense of the data’s underlying patterns and features. On the other hand, inferential visualizations take things a step further by incorporating statistical models to make predictions or generalizations about a population based on sample data. Examples include confidence intervals, regression lines, or p-value plots, which are often layered on top of standard visualizations to highlight the uncertainty or significance of findings. While descriptive visualizations provide an overview of the data at hand, inferential visualizations are essential for drawing conclusions that extend beyond the immediate dataset, particularly in social science research, where researchers often need to infer trends across larger populations.",
    "crumbs": [
      "workshop",
      "part 2: data analysis"
    ]
  },
  {
    "objectID": "pyws04-2-image-analysis.html",
    "href": "pyws04-2-image-analysis.html",
    "title": "object recognition",
    "section": "",
    "text": "code examples huggingface\ndownload jupyter notebook",
    "crumbs": [
      "workshop",
      "part 4: image analysis",
      "object recognition"
    ]
  },
  {
    "objectID": "pyws04-2-image-analysis.html#images-human-content",
    "href": "pyws04-2-image-analysis.html#images-human-content",
    "title": "object recognition",
    "section": "images, human content",
    "text": "images, human content\n\nImport OpenCV Library\nWe start by importing the OpenCV library (cv2), which provides functions for image processing and computer vision tasks.\nimport cv2\n\n\n\nLoad the Haar Cascade Classifier for Face Detection\nIn this step, we load the pre-trained Haar cascade classifier for frontal face detection. OpenCV comes with these classifiers stored in the haarcascades directory. We use the cv2.CascadeClassifier function to load the classifier file haarcascade_frontalface_default.xml.\n# Load the Haar cascade classifier for face detection\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n\n\n\nLoad the Image\nWe load the image from the specified file path using cv2.imread(). This function reads the image into a NumPy array in BGR color format.\n# Load the image\nimage_path = '/content/osm-cca-cv/res/img/image_prefix-003.png'\nimage = cv2.imread(image_path)\n\n\n\nConvert the Image to Grayscale\nFace detection algorithms often perform better on grayscale images. We convert the loaded color image to grayscale using cv2.cvtColor() with the cv2.COLOR_BGR2GRAY conversion code.\n# Convert the image to grayscale\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n\n\nDetect Faces in the Image\nWe use the detectMultiScale() method of the Haar cascade classifier to detect faces in the grayscale image. This method returns a list of rectangles, where each rectangle contains the coordinates (x, y) and the width and height (w, h) of a detected face. The parameters scaleFactor, minNeighbors, and minSize control the accuracy and speed of the detection.\n# Detect faces in the image\nfaces = face_cascade.detectMultiScale(\n    gray,\n    scaleFactor=1.1,\n    minNeighbors=5,\n    minSize=(30, 30)\n)\n\n\n\nDraw Rectangles Around Detected Faces\nWe iterate over the list of detected faces and draw rectangles around each face on the original image using cv2.rectangle(). The rectangle is drawn using the top-left corner (x, y) and the bottom-right corner (x + w, y + h). We set the color to blue (255, 0, 0) and the thickness to 2 pixels.\n# Draw rectangles around the detected faces\nfor (x, y, w, h) in faces:\n    cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n\n\n\nDisplay the Image with Detected Faces\nFinally, we display the image with the detected faces highlighted. Since we are in a Google Colab environment, we use cv2_imshow() from google.colab.patches to display the image within the notebook.\n# Display the image with detected faces\nfrom google.colab.patches import cv2_imshow\n\ncv2_imshow(image)\n\nSummary:\n\nImport OpenCV: Provides functions for image processing.\nLoad Haar Cascade Classifier: Loads a pre-trained model for face detection.\nLoad Image: Reads the image file into a NumPy array.\nConvert to Grayscale: Prepares the image for detection by simplifying color channels.\nDetect Faces: Uses the classifier to find faces in the image.\nDraw Rectangles: Highlights detected faces on the original image.\nDisplay Image: Shows the processed image within the notebook.\n\n\nBy following these steps, you can use OpenCV’s Haar cascades to detect and localize faces in an image. This process involves loading the necessary libraries and classifiers, preprocessing the image, performing face detection, and visualizing the results.",
    "crumbs": [
      "workshop",
      "part 4: image analysis",
      "object recognition"
    ]
  },
  {
    "objectID": "pyws04-2-image-analysis.html#image-classification",
    "href": "pyws04-2-image-analysis.html#image-classification",
    "title": "object recognition",
    "section": "image classification",
    "text": "image classification\n\nInstall the DeepFace Package\nWe begin by installing the deepface package using pip. This package provides easy-to-use face recognition and facial attribute analysis functionalities.\n!pip install -q deepface\n\n\n\nImport Required Libraries\nWe import the necessary libraries: DeepFace from the deepface package for emotion analysis and cv2 from OpenCV for image processing tasks.\nfrom deepface import DeepFace\nimport cv2\n\n\n\nLoad the Image\nWe load the image from the specified path using OpenCV’s imread function. The image is read into a NumPy array for processing.\n# Load the image\nimage_path = '/content/osm-cca-cv/res/img/image_prefix-003.png'\nimage = cv2.imread(image_path)\n\n\n\nConvert the Image to Grayscale\nWe convert the loaded image to grayscale using OpenCV’s cvtColor function. Grayscale images simplify the computation required for face detection.\n# Convert the image to grayscale\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n\n\nLoad the Haar Cascade Classifier for Face Detection\nWe load the pre-trained Haar Cascade classifier for frontal face detection provided by OpenCV. This classifier will help us detect faces in the image.\n# Load the Haar cascade classifier for face detection\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n\n\n\nDetect Faces in the Image\nWe use the Haar Cascade classifier’s detectMultiScale method to detect faces within the grayscale image. The method returns a list of bounding boxes around detected faces.\n# Detect faces in the image\nfaces = face_cascade.detectMultiScale(\n    gray, \n    scaleFactor=1.1, \n    minNeighbors=5, \n    minSize=(30, 30)\n)\n\n\n\nAnalyze Emotions for Each Detected Face\nWe iterate over each detected face, extract the region of interest (ROI), and use DeepFace’s analyze function to predict the dominant emotion. We handle exceptions to catch any errors during the analysis.\n# Analyze emotions for each detected face\nfor (x, y, w, h) in faces:\n    face_roi = image[y:y+h, x:x+w]  # Extract the face region of interest\n    try:\n        # Analyze emotions using DeepFace\n        result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n        dominant_emotion = result['dominant_emotion']\n        print(f\"Detected emotion: {dominant_emotion}\")\n\n        # Optionally, draw the dominant emotion on the image\n        cv2.putText(\n            image, \n            dominant_emotion, \n            (x, y - 10), \n            cv2.FONT_HERSHEY_SIMPLEX, \n            0.9, \n            (0, 255, 0), \n            2\n        )\n    except Exception as e:\n        print(f\"Error analyzing emotions: {e}\")\n\n\n\nDisplay the Image with Detected Faces and Emotions\nFinally, we display the image with the detected faces and their corresponding emotions using cv2_imshow, which is available in Google Colab for displaying images.\n# Display the image with detected faces and emotions\nfrom google.colab.patches import cv2_imshow\n\ncv2_imshow(image)\n\nSummary:\n\nInstallation and Imports: We installed the deepface package and imported necessary libraries for image processing and emotion analysis.\nImage Loading and Preprocessing: The image was loaded and converted to grayscale to prepare for face detection.\nFace Detection: Used OpenCV’s Haar Cascade classifier to detect faces in the image.\nEmotion Analysis: For each detected face, we used DeepFace to analyze and predict the dominant emotion.\nVisualization: Displayed the image with bounding boxes around faces and the predicted emotions annotated.\n\nThis code effectively combines OpenCV for face detection and DeepFace for emotion analysis, allowing you to identify and display emotions detected in faces within an image.\n\n\nother natural objects\nfrom transformers import pipeline\nclf = pipeline(\"image-classification\")\nclf(\"/content/osm-cca-cv/res/img/image_prefix-000.png\")\n\n#[{'label': 'tabby cat', 'score': 0.731},\n#...\n#]",
    "crumbs": [
      "workshop",
      "part 4: image analysis",
      "object recognition"
    ]
  },
  {
    "objectID": "pyws04-2-image-analysis.html#object-recognition",
    "href": "pyws04-2-image-analysis.html#object-recognition",
    "title": "object recognition",
    "section": "object recognition",
    "text": "object recognition\nfrom transformers import pipeline\n\nmodel = pipeline(\"object-detection\")\nmodel(\"/content/osm-cca-cv/res/img/image_prefix-000.png\")\n# [{'label': 'blanket',\n#  'mask': mask_string,\n#  'score': 0.917},\n#...]\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Original image path\nimage_path = \"/content/osm-cca-cv/res/img/image_prefix-000.png\"\n\n# Read the image\nimage = cv2.imread(image_path)\n\n# Check if the image was successfully loaded\nif image is None:\n    print(\"Error: Could not read image.\")\nelse:\n    # List of detections\n    detections = [\n        {\n            'score': 0.8914425373077393,\n            'label': 'dog',\n            'box': {'xmin': 265, 'ymin': 187, 'xmax': 282, 'ymax': 204}\n        },\n        {\n            'score': 0.999186098575592,\n            'label': 'person',\n            'box': {'xmin': 237, 'ymin': 136, 'xmax': 303, 'ymax': 218}\n        },\n        {\n            'score': 0.9989272952079773,\n            'label': 'person',\n            'box': {'xmin': 352, 'ymin': 126, 'xmax': 385, 'ymax': 184}\n        },\n        {\n            'score': 0.999570906162262,\n            'label': 'person',\n            'box': {'xmin': 481, 'ymin': 150, 'xmax': 589, 'ymax': 344}\n        }\n    ]\n\n    # Iterate over detections and draw bounding boxes\n    for detection in detections:\n        score = detection['score']\n        label = detection['label']\n        box = detection['box']\n        xmin = box['xmin']\n        ymin = box['ymin']\n        xmax = box['xmax']\n        ymax = box['ymax']\n\n        # Draw the bounding box\n        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color=(0, 255, 0), thickness=2)\n\n        # Prepare label with score\n        text = f\"{label}: {score:.2f}\"\n\n        # Calculate text size for background rectangle\n        (text_width, text_height), baseline = cv2.getTextSize(\n            text, cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, thickness=1\n        )\n\n        # Position the text above the bounding box if possible\n        text_x = xmin\n        text_y = ymin - 10 if ymin - 10 &gt; text_height else ymin + text_height + 10\n\n        # Draw background rectangle for text\n        cv2.rectangle(\n            image,\n            (text_x, text_y - text_height - baseline),\n            (text_x + text_width, text_y + baseline),\n            color=(0, 255, 0),\n            thickness=-1\n        )\n\n        # Put the text on the image\n        cv2.putText(\n            image,\n            text,\n            (text_x, text_y),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            fontScale=0.5,\n            color=(0, 0, 0),\n            thickness=1\n        )\n\n    # Convert BGR to RGB for displaying with matplotlib\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Display the image using matplotlib\n    plt.figure(figsize=(12, 8))\n    plt.imshow(image_rgb)\n    plt.axis('off')\n    plt.show()\n\n    # Alternatively, save the image to a file\n    output_path = \"/content/osm-cca-cv/res/img/image_with_detections.png\"\n    cv2.imwrite(output_path, image)\n    print(f\"Output image saved to {output_path}\")\nExplanation:\n\nImport Libraries:\n\ncv2 for image processing.\nmatplotlib.pyplot for displaying the image inline if using a Jupyter notebook or Google Colab.\n\nRead the Original Image:\n\nUse cv2.imread() to read the image from the specified path.\nCheck if the image was loaded successfully.\n\nDefine the Detections List:\n\nThis is the provided list containing detection results, each with a score, label, and bounding box coordinates.\n\nIterate Over Detections and Draw Bounding Boxes:\n\nLoop through each detection in the list.\nExtract the score, label, and box coordinates.\nUse cv2.rectangle() to draw the bounding box on the image.\n\nParameters:\n\nImage to draw on.\nStarting point (xmin, ymin).\nEnding point (xmax, ymax).\ncolor: Set to green (0, 255, 0).\nthickness: Set to 2.\n\n\nPrepare the text to display the label and score.\nCalculate the size of the text using cv2.getTextSize() to create a background rectangle for better visibility.\nDraw a filled rectangle behind the text using cv2.rectangle().\nPut the text on the image using cv2.putText().\n\nDisplay or Save the Image:\n\nConvert the image from BGR to RGB color space for correct color representation in Matplotlib.\nUse plt.imshow() to display the image inline.\nAlternatively, save the image with detections to a file using cv2.imwrite().\n\n\nNotes:\n\nColor Spaces:\n\nOpenCV uses BGR color order, while Matplotlib uses RGB. Conversion is necessary for correct color display.\n\nText Positioning:\n\nThe code attempts to place the label above the bounding box if there’s enough space; otherwise, it places it below.\n\nFont and Text Styling:\n\nFont: cv2.FONT_HERSHEY_SIMPLEX.\nFont scale and thickness are set for readability.\n\nSaving the Image:\n\nThe output image is saved to the specified path.\n\n\nDependencies:\n\nEnsure you have OpenCV installed:\npip install opencv-python\nIf using Jupyter Notebook or Google Colab, Matplotlib is typically pre-installed.\n\nExample Output:\n\nThe resulting image will display the original image with green bounding boxes around detected objects.\nLabels and scores are displayed near each bounding box for identification.\n\nAdjustments:\n\nImage Path:\n\nEnsure that image_path correctly points to your image file.\nIf the image is not found, you’ll receive an error message.\n\nOutput Path:\n\nThe output_path should be adjusted if you want to save the image to a different location.\n\nDisplay Method:\n\nIf you’re running this code outside of a notebook environment, you might need to use cv2.imshow() instead of Matplotlib to display the image.\n\n\nDisplay with OpenCV (Alternative):\n# Display the image using OpenCV's imshow (only works in local environments)\ncv2.imshow('Image with Detections', image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n\nNote that cv2.imshow() may not work in notebook environments like Google Colab.",
    "crumbs": [
      "workshop",
      "part 4: image analysis",
      "object recognition"
    ]
  },
  {
    "objectID": "pyws04-2-image-analysis.html#image-video-to-text",
    "href": "pyws04-2-image-analysis.html#image-video-to-text",
    "title": "object recognition",
    "section": "image, video to text",
    "text": "image, video to text\nfrom transformers import pipeline\n\ncaptioner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n#captioner(\"https://huggingface.co/datasets/Narsil/image_dummy/resolve/main/parrots.png\")\ncaptioner(\"https://raw.githubusercontent.com/cca-cce/osm-cca-cv/0a62207580cf63bc808ec825669af8d6f2eedf16/res/img/image_prefix-000.png\")\n## [{'generated_text': 'two birds are standing next to each other '}]",
    "crumbs": [
      "workshop",
      "part 4: image analysis",
      "object recognition"
    ]
  },
  {
    "objectID": "pyws04-2-image-analysis.html#pixtral-cutting-edge..",
    "href": "pyws04-2-image-analysis.html#pixtral-cutting-edge..",
    "title": "object recognition",
    "section": "pixtral, cutting edge..",
    "text": "pixtral, cutting edge..\n\n\n\nhuggingface account and gpu resources required\n\n\n# install dependencies\n#!pip install -q --upgrade vllm\n#!pip install -q --upgrade mistral_common\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\n\nmodel_name = \"mistralai/Pixtral-12B-2409\"\n\nsampling_params = SamplingParams(max_tokens=8192)\n\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\")\n\nprompt = \"Describe this image in one sentence.\"\nimage_url = \"https://raw.githubusercontent.com/cca-cce/osm-cca-cv/0a62207580cf63bc808ec825669af8d6f2eedf16/res/img/image_prefix-000.png\"\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [{\"type\": \"text\", \"text\": prompt}, {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}]\n    },\n]\n\noutputs = llm.chat(messages, sampling_params=sampling_params)\n\nprint(outputs[0].outputs[0].text)",
    "crumbs": [
      "workshop",
      "part 4: image analysis",
      "object recognition"
    ]
  },
  {
    "objectID": "pyws04-2-image-analysis.html#testing..-on-your-own-risk",
    "href": "pyws04-2-image-analysis.html#testing..-on-your-own-risk",
    "title": "object recognition",
    "section": "testing.. on your own risk :)",
    "text": "testing.. on your own risk :)\nTo summarize and describe the visual content of a short video without audio using Python, you can follow these main steps:\n\nExtract frames from the video.\nSelect key frames that represent the content effectively.\nUse an image captioning model to generate descriptions of the frames.\nCombine the frame descriptions into a coherent summary.\n\nBelow is a detailed explanation of each step along with the necessary code.\n\n\n1. Extract Frames from the Video\nWe use the OpenCV library to extract frames from the video file. Alternatively, moviepy can also be used for frame extraction.\nimport cv2\nimport os\n\n# Define the path to the video file\nvideo_file = 'path_to_your_video.mp4'\n\n# Create a directory to store the extracted frames\nframes_dir = 'extracted_frames'\nos.makedirs(frames_dir, exist_ok=True)\n\n# Load the video using OpenCV\ncap = cv2.VideoCapture(video_file)\n\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Decide on the interval between frames to extract (e.g., every 1 second)\nframe_interval = int(fps)  # Adjust this value as needed\n\nframe_number = 0\nextracted_frames = []\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    if frame_number % frame_interval == 0:\n        frame_path = os.path.join(frames_dir, f'frame_{frame_number}.jpg')\n        cv2.imwrite(frame_path, frame)\n        extracted_frames.append(frame_path)\n    \n    frame_number += 1\n\ncap.release()\ncv2.destroyAllWindows()\n\nprint(f\"Extracted {len(extracted_frames)} frames.\")\nNotes:\n\nFrame Extraction Interval:\n\nThe frame_interval determines how frequently frames are extracted. For example, extracting one frame per second.\nAdjust frame_interval based on the video’s FPS and the level of detail you need.\n\nSaving Frames:\n\nFrames are saved as JPEG images in the extracted_frames directory.\n\n\n\n\n\n2. Use an Image Captioning Model to Generate Descriptions\nWe use the transformers library from Hugging Face along with a pre-trained image captioning model to generate descriptions for each extracted frame.\nfrom transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nimport torch\nfrom PIL import Image\n\n# Load the pre-trained image captioning model and processor\nmodel_name = \"nlpconnect/vit-gpt2-image-captioning\"\nmodel = VisionEncoderDecoderModel.from_pretrained(model_name)\nfeature_extractor = ViTImageProcessor.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Set the device to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Function to generate caption for an image\ndef generate_caption(image_path):\n    # Load and preprocess the image\n    image = Image.open(image_path).convert(\"RGB\")\n    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n    pixel_values = pixel_values.to(device)\n\n    # Generate caption\n    with torch.no_grad():\n        output_ids = model.generate(pixel_values, max_length=50, num_beams=5)\n    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return caption\n\n# Generate captions for all extracted frames\nframe_captions = []\n\nfor frame_path in extracted_frames:\n    caption = generate_caption(frame_path)\n    frame_captions.append({'frame': frame_path, 'caption': caption})\n    print(f\"{frame_path}: {caption}\")\nNotes:\n\nModel Selection:\n\nWe use the nlpconnect/vit-gpt2-image-captioning model, which is a Vision Transformer (ViT) encoder and GPT-2 decoder.\nYou can explore other models on Hugging Face Hub if needed.\n\nDevice Configuration:\n\nThe code checks for GPU availability and uses it if possible for faster processing.\n\nCaption Generation:\n\nThe generate_caption function processes an image and generates a caption.\nAdjust parameters like max_length and num_beams for different results.\n\n\n\n\n\n3. Combine Captions into a Coherent Summary\nWe combine the generated captions to form a coherent summary of the video’s visual content.\n# Combine captions into a summary\nsummary_text = ' '.join([item['caption'] for item in frame_captions])\n\nprint(\"\\nVideo Summary:\")\nprint(summary_text)\nNotes:\n\nSimple Concatenation:\n\nWe concatenate the captions to create a rough summary.\nThis method may result in repetitive or disjointed text.\n\nAdvanced Summarization (Optional):\n\nFor a more coherent summary, you can use a text summarization model to refine the combined captions.\n\n\nfrom transformers import pipeline\n\n# Initialize the summarization pipeline\nsummarizer = pipeline('summarization')\n\n# Summarize the combined captions\nfinal_summary = summarizer(summary_text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n\nprint(\"\\nFinal Summary:\")\nprint(final_summary)\nNotes:\n\nSummarization Model:\n\nThe summarization pipeline uses a pre-trained model to condense the text.\n\nAdjusting Parameters:\n\nModify max_length and min_length to control the length of the final summary.\n\n\n\n\n\nComplete Code Example\nPutting it all together, here’s the complete code:\nimport cv2\nimport os\nimport torch\nfrom PIL import Image\nfrom transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer, pipeline\n\n# Step 1: Extract Frames from the Video\nvideo_file = 'path_to_your_video.mp4'\nframes_dir = 'extracted_frames'\nos.makedirs(frames_dir, exist_ok=True)\n\ncap = cv2.VideoCapture(video_file)\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nframe_interval = int(fps)  # Extract one frame per second\nframe_number = 0\nextracted_frames = []\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    if frame_number % frame_interval == 0:\n        frame_path = os.path.join(frames_dir, f'frame_{frame_number}.jpg')\n        cv2.imwrite(frame_path, frame)\n        extracted_frames.append(frame_path)\n    \n    frame_number += 1\n\ncap.release()\ncv2.destroyAllWindows()\n\nprint(f\"Extracted {len(extracted_frames)} frames.\")\n\n# Step 2: Generate Captions for Each Frame\nmodel_name = \"nlpconnect/vit-gpt2-image-captioning\"\nmodel = VisionEncoderDecoderModel.from_pretrained(model_name)\nfeature_extractor = ViTImageProcessor.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ndef generate_caption(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n    pixel_values = pixel_values.to(device)\n\n    with torch.no_grad():\n        output_ids = model.generate(pixel_values, max_length=50, num_beams=5)\n    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return caption\n\nframe_captions = []\n\nfor frame_path in extracted_frames:\n    caption = generate_caption(frame_path)\n    frame_captions.append({'frame': frame_path, 'caption': caption})\n    print(f\"{frame_path}: {caption}\")\n\n# Step 3: Combine Captions into a Summary\nsummary_text = ' '.join([item['caption'] for item in frame_captions])\n\n# Optional: Summarize the Combined Captions\nsummarizer = pipeline('summarization')\nfinal_summary = summarizer(summary_text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n\nprint(\"\\nFinal Summary:\")\nprint(final_summary)\n\n\n\nAdditional Considerations\n\nInstalling Dependencies:\nEnsure that you have all the required libraries installed:\npip install opencv-python\npip install transformers\npip install torch  # For GPU support, install torch appropriate for your CUDA version\npip install pillow\nFrame Selection:\n\nDynamic Frame Selection:\n\nInstead of extracting frames at fixed intervals, you can implement shot detection or scene change detection to extract frames when the content changes significantly.\nLibraries like scenedetect can help with this.\n\n\nProcessing Time:\n\nImage captioning models can be computationally intensive.\nProcessing time will increase with the number of frames and the complexity of the model.\nUsing a GPU significantly speeds up the process.\n\nModel Limitations:\n\nThe quality of the generated captions depends on the model’s capabilities.\nPre-trained models may not accurately describe all types of content.\nFor domain-specific videos, consider fine-tuning a model on relevant datasets.\n\nEnhancing Summarization:\n\nText Preprocessing:\n\nRemove duplicate captions or captions with minimal information before summarization.\n\nSemantic Understanding:\n\nUse natural language understanding techniques to better combine captions.\n\n\nPrivacy and Compliance:\n\nBe cautious when processing videos containing personal or sensitive information.\nEnsure compliance with data protection regulations.\n\n\n\n\n\nConclusion\nBy following these steps, you can extract frames from a video without audio, generate descriptions of the visual content using an image captioning model, and combine those descriptions into a textual summary. This approach leverages powerful pre-trained models available through the Hugging Face Transformers library and allows you to create summaries of visual content programmatically.\nReferences:\n\nOpenCV Documentation: https://docs.opencv.org/\nHugging Face Transformers Documentation: https://huggingface.co/transformers/\nImage Captioning Models: https://huggingface.co/models?pipeline_tag=image-to-text\nScene Detection with PySceneDetect: https://pyscenedetect.readthedocs.io/\n\n\nExample Output:\nAssuming we have a video and we run the code, the output might look like:\nExtracted 10 frames.\nextracted_frames/frame_0.jpg: a group of people standing in a room.\nextracted_frames/frame_30.jpg: a man holding a microphone.\nextracted_frames/frame_60.jpg: a woman sitting at a desk with a laptop.\n...\nFinal Summary:\nA group of people are standing in a room. A man is holding a microphone. A woman is sitting at a desk with a laptop. ...\n\nNote:\n\nReplace 'path_to_your_video.mp4' with the actual path to your video file.\nAdjust the frame_interval based on the video’s length and desired granularity.\nEnsure you have sufficient system resources to run the models, especially if processing longer videos or using higher-resolution frames.\n\n\nBy automating the process of generating textual descriptions from video frames, you can create summaries of videos without audio, which is particularly useful for silent surveillance footage, visual-only content, or scenarios where audio data is unavailable or unusable.",
    "crumbs": [
      "workshop",
      "part 4: image analysis",
      "object recognition"
    ]
  },
  {
    "objectID": "pyws04-1-image-analysis.html",
    "href": "pyws04-1-image-analysis.html",
    "title": "reading image content",
    "section": "",
    "text": "code examples pil, pillow\ncode examples opencv\ndownload jupyter notebook",
    "crumbs": [
      "workshop",
      "part 4: image analysis",
      "reading image content"
    ]
  },
  {
    "objectID": "pyws04-1-image-analysis.html#create-images",
    "href": "pyws04-1-image-analysis.html#create-images",
    "title": "reading image content",
    "section": "create images",
    "text": "create images\n\n\n\nsome alt\n\n\n\nImport Libraries\nWe start by importing the necessary libraries for numerical computations, image processing, and plotting. numpy is used for handling numerical arrays, cv2 from OpenCV is used for image processing, and seaborn and matplotlib.pyplot are used for data visualization and plotting images.\nimport numpy as np\nimport cv2\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\nCreate an Empty (Black) Image\nWe create an empty image of size 256x256 pixels with 3 color channels (BGR). The image is initialized with zeros, resulting in a black image. The dtype=np.uint8 ensures that the pixel values are in the 0-255 range, suitable for image data.\n# Create an empty (black) 256x256 image\nempty_image = np.zeros((256, 256, 3), dtype=np.uint8)\n\n\n\nDisplay the Empty Image\nUsing seaborn for styling and matplotlib for plotting, we display the empty (black) image. Since OpenCV uses BGR color format by default and matplotlib expects RGB, we convert the image using cv2.cvtColor. The axes are turned off for a cleaner display.\n# Display the image using seaborn\nsns.set()  # For a nicer plot style\nplt.imshow(cv2.cvtColor(empty_image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for correct color representation\nplt.axis('off')  # Hide axis\nplt.show()\n# Display the image using seaborn\nsns.set()  # For a nicer plot style\nplt.imshow(cv2.cvtColor(empty_image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for correct color representation\n#plt.axis('off')  # Hide axis\nplt.show()\n\n\n\nDefine a Function to Create Images with Specific RGB Values\nWe define a function create_rgb_image that generates a 256x256 image filled with specified red, green, and blue values. The function creates an empty image and assigns the provided color values to the respective channels.\n# Function to create an image with specified RGB values\ndef create_rgb_image(red, green, blue):\n    image = np.zeros((256, 256, 3), dtype=np.uint8)\n    image[:, :, 0] = red    # Red channel\n    image[:, :, 1] = green  # Green channel\n    image[:, :, 2] = blue   # Blue channel\n    return image\n\n\n\nCreate Images for Various Color Combinations\nWe define a list of RGB combinations representing common colors such as black, red, green, blue, yellow, magenta, cyan, and white. Using a list comprehension, we create images for each of these color combinations by calling the create_rgb_image function.\n# Creating images for each combination\ncombinations = [\n    (0, 0, 0),     # Black\n    (255, 0, 0),   # Red\n    (0, 255, 0),   # Green\n    (0, 0, 255),   # Blue\n    (255, 255, 0), # Yellow\n    (255, 0, 255), # Magenta\n    (0, 255, 255), # Cyan\n    (255, 255, 255)# White\n]\n\nimages = [create_rgb_image(*combo) for combo in combinations]\n\n\n\nDisplay the Color Images in a Grid\nWe set the seaborn style and create a figure for plotting. Using matplotlib’s subplot functionality, we display each color image in a grid layout. The RGB values of each image are displayed as the title. The axes are hidden for clarity, and plt.tight_layout() adjusts the spacing between subplots.\n# Plotting\nsns.set()  # For a nicer plot style\nplt.figure(figsize=(12, 6))\n\nfor i, img in enumerate(images):\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title(f'R:{combinations[i][0]} G:{combinations[i][1]} B:{combinations[i][2]}')\n\nplt.tight_layout()\nplt.show()\n\nSummary:\n\nFirst, we imported necessary libraries for numerical operations, image processing, and plotting.\nSecond, we created and displayed an empty (black) image.\nThird, we defined a function to create images with specific RGB values.\nFourth, we generated images for different color combinations.\nFinally, we displayed all the generated images in a grid format with their RGB values.\n\nThis code demonstrates how to manipulate image data at the pixel level using NumPy arrays and how to visualize the results using Matplotlib and Seaborn.",
    "crumbs": [
      "workshop",
      "part 4: image analysis",
      "reading image content"
    ]
  },
  {
    "objectID": "pyws04-1-image-analysis.html#display-images",
    "href": "pyws04-1-image-analysis.html#display-images",
    "title": "reading image content",
    "section": "display images",
    "text": "display images\n\nexample sustainability communication\n\n\nImport Libraries and Load Image Using OpenCV\nIn this step, we import the necessary libraries for image processing and display. We use OpenCV (cv2) for image operations and cv2_imshow from google.colab.patches to display images within Google Colab notebooks. We then attempt to load an image from a specified file path using cv2.imread().\nimport cv2\nfrom google.colab.patches import cv2_imshow\n\n# Load an image from file\nimage_path = '/content/osm-cca-cv/res/img/image_prefix-000.png'\nimage = cv2.imread(image_path)\n\n\n\nCheck if the Image Was Successfully Loaded and Display It\nHere, we check whether the image was successfully loaded by verifying that image is not None. If the image is loaded, we display it using cv2_imshow(), which is a function provided by Google Colab to display images in the notebook environment. If the image is not loaded, an error message is printed to inform the user.\n# Check if the image was successfully loaded\nif image is not None:\n    # Display the image\n    cv2_imshow(image)\nelse:\n    print(\"Error: Unable to load image.\")\n\n\n\nImport Additional Libraries for Plotting\nWe import additional libraries required for plotting and enhancing the image visualization. OpenCV (cv2) is re-imported for consistency, Matplotlib’s pyplot module (plt) is used for plotting images, and Seaborn (sns) is used to improve the aesthetics of the plots.\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nRead the Image and Convert Color Scheme from BGR to RGB\nWe read the image again using OpenCV’s cv2.imread() function. OpenCV reads images in BGR (Blue, Green, Red) color format by default, which can cause colors to appear incorrectly when displayed using Matplotlib (which expects RGB format). To correct this, we convert the image from BGR to RGB using cv2.cvtColor().\n# Read the image using OpenCV\nimage_path = '/content/osm-cca-cv/res/img/image_prefix-000.png'  # Replace with your image path\nimage = cv2.imread(image_path)\n\n# Convert the color scheme from BGR to RGB\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n\n\nApply Seaborn Styles and Display the Image Using Matplotlib\nWe apply Seaborn’s default styling using sns.set() to enhance the plot’s aesthetics. Then, we display the image using Matplotlib’s plt.imshow(), which renders the image data in the notebook. We hide the axes using plt.axis('off') for a cleaner look and finally call plt.show() to display the image.\n# Use Seaborn to improve plot aesthetics\nsns.set()  # Apply Seaborn styles\n\n# Display the image\nplt.imshow(image_rgb)\nplt.axis('off')  # Hide the axis\nplt.show()\n\nSummary:\n\nFirst, we imported the necessary libraries for image processing and display, and loaded an image using OpenCV.\nSecond, we checked if the image was successfully loaded and displayed it using cv2_imshow() in Google Colab.\nThird, we imported additional libraries for plotting and visualization.\nFourth, we read the image again and converted its color space from BGR to RGB to ensure correct color representation in Matplotlib.\nFinally, we applied Seaborn styles for better aesthetics and displayed the image using Matplotlib, hiding the axes for a cleaner presentation.\n\nThis code demonstrates how to read and display images using OpenCV and Matplotlib in a Jupyter or Google Colab notebook, addressing color space differences between libraries and enhancing the visualization with Seaborn styles.",
    "crumbs": [
      "workshop",
      "part 4: image analysis",
      "reading image content"
    ]
  },
  {
    "objectID": "pyws04-1-image-analysis.html#describe-images",
    "href": "pyws04-1-image-analysis.html#describe-images",
    "title": "reading image content",
    "section": "describe images",
    "text": "describe images\n#| colab: {base_uri: 'https://localhost:8080/', height: 206}\nimport os\nimport cv2\nimport pandas as pd\n\n# Path to the directory containing images\ndirectory = '/content/osm-cca-cv/res/img'  # Replace with your directory path\n\n# Initialize a list to store image data\nimage_data = []\n\n# Iterate through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # Add other image formats if needed\n        # Read the image\n        img_path = os.path.join(directory, filename)\n        img = cv2.imread(img_path)\n\n        # Extract image dimensions and channels\n        height, width, channels = img.shape\n\n        # Append the data to the list\n        image_data.append({'Filename': filename, 'Width': width, 'Height': height, 'Channels': channels})\n\n# Create a DataFrame from the image data\nimage_df = pd.DataFrame(image_data)\n\n# Display the DataFrame\nimage_df.head()",
    "crumbs": [
      "workshop",
      "part 4: image analysis",
      "reading image content"
    ]
  },
  {
    "objectID": "pyws04-1-image-analysis.html#manipulate-images",
    "href": "pyws04-1-image-analysis.html#manipulate-images",
    "title": "reading image content",
    "section": "manipulate images",
    "text": "manipulate images\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport requests\nfrom io import BytesIO\n\n# Step 1: Load a sample RGB image\n# You can use any image URL or local image file. Here, we'll use an image from the web.\nimage_url = 'https://raw.githubusercontent.com/cca-cce/osm-cca-cv/0a62207580cf63bc808ec825669af8d6f2eedf16/res/img/image_prefix-000.png'\nresponse = requests.get(image_url)\nimage = Image.open(BytesIO(response.content))\n\n# Ensure the image is in RGB mode\nimage = image.convert('RGB')\n\n# Optionally resize the image for easier processing\nimage = image.resize((200, 200))\n\n# Step 2: Convert the image to a NumPy array\nimage_np = np.array(image)\n\n# Step 3: Manipulate the bottom right quarter to increase green colors\nheight, width, _ = image_np.shape\nstart_x = width // 2\nstart_y = height // 2\n\n# Increase the green channel by 100 (ensure values stay within 0-255)\nimage_np[start_y:, start_x:, 1] = np.clip(image_np[start_y:, start_x:, 1] + 100, 0, 255)\n\n# Step 4: Convert the NumPy array back to a PIL Image\nmodified_image = Image.fromarray(image_np.astype('uint8'), 'RGB')\n\n# Step 5: Display the image in a Google Colab notebook\nplt.imshow(modified_image)\nplt.axis('off')\nplt.show()\nExplanation:\n\nLoading the Image: We load a sample image from a URL using the requests library and Image.open. The image is converted to RGB mode to ensure it has three color channels.\nConverting to NumPy Array: The image is converted to a NumPy array to allow pixel-level manipulation.\nManipulating the Image:\n\nWe calculate the starting indices for the bottom right quarter.\nWe increase the green color channel (image_np[..., 1]) by 100 in the bottom right quarter.\nnp.clip ensures that the pixel values stay within the valid range of 0 to 255.\n\nDisplaying the Image: The modified image is displayed using matplotlib.pyplot in the Google Colab notebook.\n\nNote: This code should be run in a Google Colab notebook to display the image inline. If you’re not using Colab, ensure that your environment supports image display in notebooks.\nDependencies:\n\nMake sure to install the required libraries if they are not already installed:\n!pip install Pillow numpy matplotlib requests\n\nSample Output:\nThe code will display the original image with the bottom right quarter appearing greener due to the increased green color channel.",
    "crumbs": [
      "workshop",
      "part 4: image analysis",
      "reading image content"
    ]
  },
  {
    "objectID": "pyws04-1-image-analysis.html#image-features",
    "href": "pyws04-1-image-analysis.html#image-features",
    "title": "reading image content",
    "section": "image features",
    "text": "image features\n\nImport Libraries and Read the Image\nIn this step, we import the required libraries for image processing and visualization: OpenCV (cv2) for image operations, NumPy (np) for numerical computations, Matplotlib’s pyplot module (plt) for plotting, and Seaborn (sns) for enhancing plot aesthetics. We then read the image from the specified path using cv2.imread(). The image is loaded into a NumPy array in BGR (Blue, Green, Red) format.\n\"\"\"# image features\"\"\"\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Read the image\nimage = cv2.imread('/content/osm-cca-cv/res/img/image_prefix-000.png')\n\n\n\nCalculate and Normalize Color Histograms\nHere, we calculate the color histograms for each of the three color channels (Blue, Green, Red) using OpenCV’s calcHist() function. This function computes the frequency of each pixel intensity value (ranging from 0 to 255) for the specified channel. We then normalize each histogram by dividing it by its total sum to obtain a probability distribution, which allows for comparison between histograms regardless of the image size.\n# Calculate color histograms for each channel\nhist_r = cv2.calcHist([image], [0], None, [256], [0, 256])\nhist_g = cv2.calcHist([image], [1], None, [256], [0, 256])\nhist_b = cv2.calcHist([image], [2], None, [256], [0, 256])\n\n# Normalize histograms\nhist_r /= hist_r.sum()\nhist_g /= hist_g.sum()\nhist_b /= hist_b.sum()\n\n\n\nPlot the Original Image and Color Histograms\nWe set up a figure with two subplots using Matplotlib’s subplots() function. The first subplot displays the original image, converting it from BGR to RGB color space using cv2.cvtColor() for correct color representation in Matplotlib. The second subplot displays the normalized color histograms for each channel using Seaborn’s lineplot(). We customize the plot with titles, labels, and a legend, and finally display the plots using plt.show().\n# Create subplots for the original image and histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Plot the original image\naxes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\naxes[0].set_title('Original Image')\naxes[0].axis('off')\n\n# Plot the color histograms using Seaborn\nsns.lineplot(x=np.arange(256), y=hist_r.squeeze(), color='red', ax=axes[1], label='Red')\nsns.lineplot(x=np.arange(256), y=hist_g.squeeze(), color='green', ax=axes[1], label='Green')\nsns.lineplot(x=np.arange(256), y=hist_b.squeeze(), color='blue', ax=axes[1], label='Blue')\naxes[1].set_title('Color Histograms')\naxes[1].set_xlabel('Pixel Value')\naxes[1].set_ylabel('Frequency')\naxes[1].legend()\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n\n\n\nImport Libraries and Read the Image in Grayscale\nWe import the necessary libraries for edge detection: OpenCV (cv2) for image processing, NumPy (np) for numerical computations, and cv2_imshow from google.colab.patches to display images in Google Colab notebooks. We then read the image in grayscale mode using cv2.imread() with the cv2.IMREAD_GRAYSCALE flag, which simplifies the image data for edge detection algorithms.\nimport cv2\nimport numpy as np\nfrom google.colab.patches import cv2_imshow\n\n# Read the image\nimage = cv2.imread('/content/osm-cca-cv/res/img/image_prefix-000.png', cv2.IMREAD_GRAYSCALE)\n\n\n\nDefine and Apply Edge Detection Methods\nWe define a list called edge_detection_settings that contains tuples of edge detection method names and their corresponding results. We apply different edge detection algorithms provided by OpenCV: Canny, Sobel X, and Sobel Y. The Canny edge detector is applied with threshold values of 50 and 150. The Sobel operators are applied to detect edges in the X and Y directions, respectively.\n# Define edge detection settings\nedge_detection_settings = [\n    ('Canny', cv2.Canny(image, 50, 150)),\n    # ('Laplacian', cv2.Laplacian(image, cv2.CV_64F)),\n    ('Sobel_X', cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5)),\n    ('Sobel_Y', cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5))\n]\n\n\n\nProcess and Display Edge Detection Results\nWe loop through each edge detection result in edge_detection_settings. For each method, we perform the following steps:\n\nConvert the edge image to a binary image where edge pixels are set to 255 (white) and non-edge pixels to 0 (black) using NumPy’s where() function.\nCount the number of white edge pixels using OpenCV’s countNonZero() function.\nConcatenate the original grayscale image and the binary edge image horizontally using NumPy’s hstack() function for side-by-side comparison.\nDisplay the concatenated image using cv2_imshow() in Google Colab.\nPrint the name of the edge detection algorithm and the number of white edge pixels.\nSave the concatenated image to a file with a filename that includes the edge detection method’s name.\n\n# Iterate through edge detection settings\nfor setting_name, edges in edge_detection_settings:\n    # Convert the edges to binary (keeping them in 0-255 range)\n    edges_binary = np.where(edges &gt; 0, 255, 0).astype(np.uint8)\n\n    # Count the number of white edge pixels\n    white_pixel_count = cv2.countNonZero(edges_binary)\n\n    # Concatenate the original image and binary edge image horizontally\n    concatenated_image = np.hstack((image, edges_binary))\n\n    # Display the concatenated image using cv2_imshow\n    print(f'Edge Detection Algorithm: {setting_name}')\n    cv2_imshow(concatenated_image)\n    print(f'Number of white edge pixels: {white_pixel_count}\\n')\n\n    # Save the concatenated image with the specified filename prefix\n    filename = f'edges_{setting_name}.jpg'\n    cv2.imwrite(filename, concatenated_image)\n\n    print(f'Saved as {filename}\\n')\n\nSummary:\n\nFirst, we imported the necessary libraries and read the image for color histogram analysis.\nSecond, we calculated and normalized the color histograms for the red, green, and blue channels.\nThird, we plotted the original image alongside its color histograms using Matplotlib and Seaborn.\nFourth, we imported additional libraries for edge detection and read the image in grayscale mode.\nFifth, we defined various edge detection methods and applied them to the image.\nFinally, we processed each edge detection result by converting edges to binary images, counting edge pixels, displaying the results, and saving the concatenated images.\n\nThis code demonstrates how to perform basic image analysis tasks, such as calculating color histograms and performing edge detection, using OpenCV. It also shows how to visualize the results effectively using Matplotlib and Seaborn, ensuring that all code is kept intact and executable without breaking any loops or structures.",
    "crumbs": [
      "workshop",
      "part 4: image analysis",
      "reading image content"
    ]
  }
]