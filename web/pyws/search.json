[
  {
    "objectID": "pyws01-0-getting-started.html",
    "href": "pyws01-0-getting-started.html",
    "title": "part 1: getting started",
    "section": "",
    "text": "Google Colab Notebooks offer a unique, cloud-based Python environment, making it particularly convenient for data science and research applications. Since Colab runs entirely in the cloud, participants don’t need to worry about local installations or hardware limitations. With Colab, you can access a fully functioning Python environment directly through your browser, complete with popular libraries like NumPy, Pandas, and Matplotlib pre-installed. This makes it ideal for users who may have limited access to high-performance hardware, as Google’s servers handle the heavy lifting. Additionally, Colab supports collaboration, allowing multiple users to work on the same notebook in real-time, much like Google Docs. This contrasts with traditional local Python environments such as Anaconda or PyCharm, where you must install Python and manage dependencies on your own machine. While local environments offer greater control and customization, they can also be more challenging to configure for beginners, particularly when it comes to managing different Python packages and versions.",
    "crumbs": [
      "workshop",
      "part 1: getting started"
    ]
  },
  {
    "objectID": "pyws01-0-getting-started.html#python-environments",
    "href": "pyws01-0-getting-started.html#python-environments",
    "title": "part 1: getting started",
    "section": "",
    "text": "Google Colab Notebooks offer a unique, cloud-based Python environment, making it particularly convenient for data science and research applications. Since Colab runs entirely in the cloud, participants don’t need to worry about local installations or hardware limitations. With Colab, you can access a fully functioning Python environment directly through your browser, complete with popular libraries like NumPy, Pandas, and Matplotlib pre-installed. This makes it ideal for users who may have limited access to high-performance hardware, as Google’s servers handle the heavy lifting. Additionally, Colab supports collaboration, allowing multiple users to work on the same notebook in real-time, much like Google Docs. This contrasts with traditional local Python environments such as Anaconda or PyCharm, where you must install Python and manage dependencies on your own machine. While local environments offer greater control and customization, they can also be more challenging to configure for beginners, particularly when it comes to managing different Python packages and versions.",
    "crumbs": [
      "workshop",
      "part 1: getting started"
    ]
  },
  {
    "objectID": "pyws01-0-getting-started.html#basic-python-syntax",
    "href": "pyws01-0-getting-started.html#basic-python-syntax",
    "title": "part 1: getting started",
    "section": "basic python syntax",
    "text": "basic python syntax\nWhen comparing Python syntax to other popular data science platforms such as Excel or SPSS, Python provides much more flexibility and scalability. Excel is widely used for smaller datasets and offers an intuitive, visual interface for users without coding experience. However, it can become cumbersome for handling large datasets or performing more complex operations, like advanced statistical analysis or machine learning. In contrast, Python’s syntax is relatively simple but powerful, allowing users to write reusable scripts for tasks like data manipulation, cleaning, and analysis. Control structures in Python, such as loops and conditionals, provide much greater control over data operations compared to the rigid, formula-based system of Excel. Moreover, Python can handle more diverse data types and larger datasets more efficiently than Excel, which tends to slow down or crash with extensive data. Compared to SPSS, a statistical software package, Python offers greater flexibility with open-source libraries like SciPy and StatsModels, though SPSS remains easier for non-programmers due to its point-and-click interface. Ultimately, Python is an excellent choice for users looking to scale their work, automate processes, or engage in more complex data science tasks.",
    "crumbs": [
      "workshop",
      "part 1: getting started"
    ]
  },
  {
    "objectID": "pyws05-0-data-collection.html",
    "href": "pyws05-0-data-collection.html",
    "title": "part 5: data collection",
    "section": "",
    "text": "Web scraping is an essential technique for gathering data from websites, and it can be approached differently depending on whether the target is static or dynamic content. Static web content is straightforward to scrape because the content is directly embedded in the HTML of the webpage, and tools like BeautifulSoup can easily parse this HTML to extract useful information, such as financial data, text, or images. For instance, scraping static content from a financial news site could involve fetching stock prices, headlines, and metadata, all available in the page’s source code. However, dynamic web content presents additional challenges. This type of content is often rendered via JavaScript after the initial page load, making traditional HTML scraping tools ineffective. To handle dynamic content, you need tools like Selenium or Playwright, which can simulate a browser environment, interact with the page, and extract data after it has been dynamically loaded. For example, scraping a site with real-time financial data, such as stock trading platforms or cryptocurrency exchanges, requires navigating dynamic elements like charts and live updates, all of which are powered by JavaScript.",
    "crumbs": [
      "workshop",
      "part 5: data collection"
    ]
  },
  {
    "objectID": "pyws05-0-data-collection.html#web-scraping",
    "href": "pyws05-0-data-collection.html#web-scraping",
    "title": "part 5: data collection",
    "section": "",
    "text": "Web scraping is an essential technique for gathering data from websites, and it can be approached differently depending on whether the target is static or dynamic content. Static web content is straightforward to scrape because the content is directly embedded in the HTML of the webpage, and tools like BeautifulSoup can easily parse this HTML to extract useful information, such as financial data, text, or images. For instance, scraping static content from a financial news site could involve fetching stock prices, headlines, and metadata, all available in the page’s source code. However, dynamic web content presents additional challenges. This type of content is often rendered via JavaScript after the initial page load, making traditional HTML scraping tools ineffective. To handle dynamic content, you need tools like Selenium or Playwright, which can simulate a browser environment, interact with the page, and extract data after it has been dynamically loaded. For example, scraping a site with real-time financial data, such as stock trading platforms or cryptocurrency exchanges, requires navigating dynamic elements like charts and live updates, all of which are powered by JavaScript.",
    "crumbs": [
      "workshop",
      "part 5: data collection"
    ]
  },
  {
    "objectID": "pyws05-0-data-collection.html#web-experiments",
    "href": "pyws05-0-data-collection.html#web-experiments",
    "title": "part 5: data collection",
    "section": "web experiments",
    "text": "web experiments\nBeyond scraping, the rise of platforms like Streamlit and GitHub Codespaces offers powerful possibilities for hosting web experiments that collect user interaction and behavioral data. Streamlit is a Python-based framework that simplifies the creation of interactive web applications, making it easy for researchers to design experiments that capture user input in real-time. For example, researchers in social science could build a survey that adjusts dynamically based on user responses or a task-based experiment where user behavior is logged and analyzed. Streamlit’s simplicity allows for fast deployment of experiments that run directly in the browser, eliminating the need for complex backend infrastructure. On the other hand, GitHub Codespaces provides a full development environment in the cloud, enabling researchers to collaborate on and host interactive experiments. By setting up a Codespace, researchers can deploy real-time applications with persistent storage, making it possible to record user behaviors such as clicks, navigation patterns, and text input during the experiment. The ability to run experiments in the cloud with either platform means data collection can scale easily, and researchers can access a broader pool of participants without requiring them to install software or participate in person. Both platforms offer streamlined ways to collect, store, and analyze behavioral data, which can be particularly useful for conducting social science research in a modern, online setting.",
    "crumbs": [
      "workshop",
      "part 5: data collection"
    ]
  },
  {
    "objectID": "pyws04-0-image-analysis.html",
    "href": "pyws04-0-image-analysis.html",
    "title": "part 4: image analysis",
    "section": "",
    "text": "Handling images in Python involves a variety of intricacies, especially when processing raw visual data to extract meaningful information. Unlike text, which is inherently unstructured, images contain pixel-based data that needs to be interpreted and organized. When reading and processing image content using libraries like OpenCV and Pillow, common challenges include handling different image formats (such as JPEG, PNG, or TIFF), varying image resolutions, and potential distortions like noise or compression artifacts. Additionally, images can come in different color spaces (RGB, grayscale, etc.), and choosing the right one for analysis is crucial depending on the task at hand. For instance, converting an image to grayscale simplifies it by removing color information, making it easier to focus on structural aspects of the image like edges or shapes. Another complexity arises in how to break down the image into meaningful units—whether it’s identifying objects, segmenting regions, or extracting pixel values for quantitative analysis. This process requires a deep understanding of how visual information is encoded and how it can be transformed for different analytical purposes.",
    "crumbs": [
      "workshop",
      "part 4: image analysis"
    ]
  },
  {
    "objectID": "pyws04-0-image-analysis.html#reading-image-content",
    "href": "pyws04-0-image-analysis.html#reading-image-content",
    "title": "part 4: image analysis",
    "section": "",
    "text": "Handling images in Python involves a variety of intricacies, especially when processing raw visual data to extract meaningful information. Unlike text, which is inherently unstructured, images contain pixel-based data that needs to be interpreted and organized. When reading and processing image content using libraries like OpenCV and Pillow, common challenges include handling different image formats (such as JPEG, PNG, or TIFF), varying image resolutions, and potential distortions like noise or compression artifacts. Additionally, images can come in different color spaces (RGB, grayscale, etc.), and choosing the right one for analysis is crucial depending on the task at hand. For instance, converting an image to grayscale simplifies it by removing color information, making it easier to focus on structural aspects of the image like edges or shapes. Another complexity arises in how to break down the image into meaningful units—whether it’s identifying objects, segmenting regions, or extracting pixel values for quantitative analysis. This process requires a deep understanding of how visual information is encoded and how it can be transformed for different analytical purposes.",
    "crumbs": [
      "workshop",
      "part 4: image analysis"
    ]
  },
  {
    "objectID": "pyws04-0-image-analysis.html#object-recognition",
    "href": "pyws04-0-image-analysis.html#object-recognition",
    "title": "part 4: image analysis",
    "section": "object recognition",
    "text": "object recognition\nObject recognition, a more advanced image analysis technique, involves detecting and classifying objects within an image, often using machine learning models. This contrasts with more basic types of image analysis, such as finding contours or corners, which are simpler geometric features. Contour detection in an image focuses on identifying the boundaries or edges of objects, which can be useful for shape analysis, but it doesn’t provide any understanding of the object’s identity or function. Corner detection, on the other hand, finds points in the image where there is a sharp change in direction, such as the corners of a rectangle, which can be helpful in tasks like motion tracking or object detection based on feature points. While both contour and corner detection are essential for breaking down images into simpler shapes or features, they do not attempt to interpret or classify the objects in the scene. Object recognition takes this further by using algorithms or trained models to not only detect an object but also to classify it—whether it’s identifying a car, a person, or another object in the image. This step is critical in fields like behavioral analysis or media studies, where understanding what is in the image, rather than just its shape or structure, is essential for deriving insights from visual data.",
    "crumbs": [
      "workshop",
      "part 4: image analysis"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html",
    "href": "pyws01-2-getting-started.html",
    "title": "basic python syntax",
    "section": "",
    "text": "interactive learnpython\ndownload jupyter notebook",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html#strings-and-numbers",
    "href": "pyws01-2-getting-started.html#strings-and-numbers",
    "title": "basic python syntax",
    "section": "strings and numbers",
    "text": "strings and numbers\nHere are 10 Python code lines with different types of strings and numbers, each followed by an explanation:\n\nCode:\n\n# This is a comment explaining the next line\n\nExplanation:\nThis line starts with a #, making it a comment. Python ignores this line during execution. It’s used to explain code or leave notes for other programmers.\nCode:\n\nprint('Hello, World!')\n\nExplanation:\nThis prints the string 'Hello, World!' to the console. Single quotes enclose the string. In Python, single and double quotes are interchangeable for defining strings.\nCode:\n\nprint(\"Python is fun\")\n\nExplanation:\nHere, double quotes are used to define the string \"Python is fun\". Python treats strings defined with single or double quotes the same way.\nCode:\n\nprint(\"He said, \\\"Python is cool\\\"\")\n\nExplanation:\nThis line prints He said, \"Python is cool\". The backslash \\ before the double quotes escapes them, telling Python to treat them as part of the string instead of ending it.\nCode:\n\nprint(\"Line one\\nLine two\")\n\nExplanation:\nThe \\n is a newline escape character, so this will output:\nLine one\nLine two\nThe \\n tells Python to move to a new line.\nCode:\n\nprint(\"Hello\" + \" \" + \"World\")\n\nExplanation:\nThis line concatenates three strings: \"Hello\", a space (\" \"), and \"World\", resulting in Hello World. The + operator combines strings.\nCode:\n\nprint(5 + 3)\n\nExplanation:\nThis performs an arithmetic operation, adding two integers 5 and 3, resulting in the output 8. Python interprets + as an addition operator when used with numbers.\nCode:\n\nprint(5.0 + 3)\n\nExplanation:\nThis adds a floating-point number 5.0 and an integer 3. Python automatically converts the integer to a float and outputs 8.0, demonstrating Python’s support for mixed-type arithmetic.\nCode:\n\nprint(7 / 2)\n\nExplanation:\nThis division operation between two integers results in 3.5. In Python 3, division with / always results in a float, even when dividing two integers.\nCode:\n\nprint(type(3.14))\n\nExplanation:\nThis uses the type() function, which returns the data type of the value passed to it. Here, 3.14 is a floating-point number, so the output will be &lt;class 'float'&gt;, indicating the value is of type float.\n\nThese examples cover key features of Python strings, numbers, and the type() function, along with how comments and escape characters work.",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html#lists-and-dictionaries",
    "href": "pyws01-2-getting-started.html#lists-and-dictionaries",
    "title": "basic python syntax",
    "section": "lists and dictionaries",
    "text": "lists and dictionaries\nHere are 10 Python code lines illustrating different types of lists and dictionaries, with explanations:\n\nCode:\n\nmy_list = [1, 2, 3, 4]\n\nExplanation:\nThis creates a list called my_list containing four integer elements: [1, 2, 3, 4]. Lists are ordered and mutable collections in Python, allowing for element addition, removal, and modification. Each element can be accessed by its index, starting from 0.\nCode:\n\nmy_dict = {'name': 'Alice', 'age': 30}\n\nExplanation:\nThis creates a dictionary my_dict with two key-value pairs: 'name': 'Alice' and 'age': 30. Dictionaries are unordered collections that map keys to values, and values can be accessed using the keys.\nCode:\n\nnested_list = [[1, 2], [3, 4], [5, 6]]\n\nExplanation:\nThis creates a 2D list nested_list, where each element is another list. Accessing elements can be done using two indices, such as nested_list[0][1] to get the value 2.\nCode:\n\nnested_dict = {'person1': {'name': 'Alice', 'age': 30}, 'person2': {'name': 'Bob', 'age': 25}}\n\nExplanation:\nThis is a 2D dictionary, where each key ('person1', 'person2') maps to another dictionary. For example, you can access Alice’s age by using nested_dict['person1']['age'], which returns 30.\nCode:\n\nmy_list.append(5)\n\nExplanation:\nThis appends the value 5 to the end of my_list. The append() method is a built-in function for adding elements to a list, modifying it in place.\nCode:\n\nlast_item = my_list.pop()\n\nExplanation:\nThis removes and returns the last element from my_list using the pop() method. If my_list = [1, 2, 3, 4, 5], after popping, my_list becomes [1, 2, 3, 4] and last_item is assigned the value 5.\nCode:\n\nsecond_item = my_list[1]\n\nExplanation:\nThis accesses the second element of my_list using the index 1 (Python uses 0-based indexing). For example, if my_list = [1, 2, 3, 4], second_item will be 2.\nCode:\n\nmy_dict['city'] = 'New York'\n\nExplanation:\nThis adds a new key-value pair 'city': 'New York' to my_dict. Dictionaries allow dynamic insertion of key-value pairs. If my_dict already contains 'city', this will update its value.\nCode:\n\nremoved_value = my_dict.pop('age')\n\nExplanation:\nThis removes the key 'age' from my_dict and returns its value (30 in this case). The pop() method removes the specified key-value pair and modifies the dictionary.\nCode:\n\nprint(type(my_list))\n\nExplanation:\nThis uses the type() function to check the data type of my_list. The output will be &lt;class 'list'&gt;, indicating that my_list is a list. Similarly, calling type(my_dict) would return &lt;class 'dict'&gt;, showing that my_dict is a dictionary.\n\nThese examples illustrate key operations with lists and dictionaries, including element access, appending, popping, and the use of the type() function to check data types.",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html#loops-and-conditionals",
    "href": "pyws01-2-getting-started.html#loops-and-conditionals",
    "title": "basic python syntax",
    "section": "loops and conditionals",
    "text": "loops and conditionals\nHere are 10 Python code chunks demonstrating different types of loops and conditionals, with explanations:\n\nCode:\n\nfor i in range(5):\n    print(i)\n\nExplanation:\nThis is a basic for loop that iterates over the range 0 to 4 (Python ranges are zero-indexed and exclusive of the stop value). It prints each value of i in the loop: 0, 1, 2, 3, 4.\nCode:\n\nwhile True:\n    print(\"Looping...\")\n    break\n\nExplanation:\nThis is a while loop with a True condition, which would normally create an infinite loop. However, the break statement exits the loop after the first iteration. Without break, it would continuously print \"Looping...\".\nCode:\n\nif 10 &gt; 5:\n    print(\"10 is greater than 5\")\nelse:\n    print(\"5 is greater than or equal to 10\")\n\nExplanation:\nThis is an if-else statement. The condition 10 &gt; 5 evaluates to True, so the first block is executed, printing \"10 is greater than 5\". The else block would run if the condition were False.\nCode:\n\nnumber = 7\nif number % 2 == 0:\n    print(\"Even\")\nelse:\n    print(\"Odd\")\n\nExplanation:\nThis checks if the variable number is even or odd using the modulo operator (%). If number % 2 == 0, it prints \"Even\", otherwise, it prints \"Odd\". In this case, it prints \"Odd\" because 7 is not divisible by 2.\nCode:\n\nfor x in range(10):\n    if x % 2 == 0:\n        continue\n    print(x)\n\nExplanation:\nThis for loop prints all odd numbers from 0 to 9. The continue statement skips the rest of the loop when x is even, so only odd values (1, 3, 5, 7, 9) are printed.\nCode:\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor index, fruit in enumerate(fruits):\n    print(f\"Fruit {index}: {fruit}\")\n\nExplanation:\nThe enumerate() function provides both the index and value of each element in the fruits list. The loop iterates over the list, printing each fruit along with its index:\nFruit 0: apple\nFruit 1: banana\nFruit 2: cherry\nCode:\n\nis_raining = True\nis_sunny = False\nif is_raining and not is_sunny:\n    print(\"It's raining but not sunny\")\nelif is_sunny and not is_raining:\n    print(\"It's sunny but not raining\")\nelse:\n    print(\"It's either both raining and sunny or neither\")\n\nExplanation:\nThis demonstrates a compound conditional using Boolean variables. Since is_raining is True and is_sunny is False, the first block is executed, printing \"It's raining but not sunny\".\nCode:\n\neven_numbers = [x for x in range(10) if x % 2 == 0]\nprint(even_numbers)\n\nExplanation:\nThis is a list comprehension that creates a list of even numbers from 0 to 9. It iterates over the range of numbers and only includes those where x % 2 == 0. The output is [0, 2, 4, 6, 8].\nCode:\n\ntry:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero\")\n\nExplanation:\nThis is an example of a try-except block for exception handling. The try block contains code that could raise an exception (division by zero), and the except block catches the ZeroDivisionError and prints \"Cannot divide by zero\". Without the exception handling, the program would crash.\nCode:\n\nfor i in range(5):\n   try:\n       print(10 / i)\n   except ZeroDivisionError:\n       print(\"Division by zero is not allowed\")\n\nExplanation:\nThis loop attempts to divide 10 by i for values from 0 to 4. When i is 0, a ZeroDivisionError occurs, which is caught by the except block, printing \"Division by zero is not allowed\". For other values of i, the result of the division is printed.\n\nThese examples cover various types of loops, conditionals, list comprehensions, Boolean logic, and exception handling in Python.",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html#user-defined-functions",
    "href": "pyws01-2-getting-started.html#user-defined-functions",
    "title": "basic python syntax",
    "section": "user defined functions",
    "text": "user defined functions\nHere are 10 Python code chunks demonstrating different types of user-defined functions, with explanations:\n\nCode:\n\ndef greet():\n    \"\"\"This function prints a simple greeting message.\"\"\"\n    print(\"Hello, welcome!\")\ngreet()\n\nExplanation:\nThis is a simple function greet() that takes no arguments and prints a greeting message. It is called using greet(). The triple quotes \"\"\" define a docstring, which serves as the function’s documentation. When called, it prints \"Hello, welcome!\".\nCode:\n\ndef greet_person(name):\n    \"\"\"This function greets a person by name.\"\"\"\n    print(f\"Hello, {name}!\")\ngreet_person(\"Alice\")\n\nExplanation:\nThis function greet_person(name) accepts a single argument, name, and prints a personalized greeting. When you call greet_person(\"Alice\"), it prints \"Hello, Alice!\". The docstring explains what the function does.\nCode:\n\ndef add_numbers(a, b):\n    \"\"\"Returns the sum of two numbers.\"\"\"\n    return a + b\nresult = add_numbers(5, 3)\nprint(result)\n\nExplanation:\nadd_numbers(a, b) is a function that takes two arguments, a and b, and returns their sum. In this case, add_numbers(5, 3) returns 8, which is printed. The return keyword is used to send the result back to the calling code.\nCode:\n\ndef multiply(a, b=2):\n    \"\"\"Multiplies two numbers, with the second number having a default value of 2.\"\"\"\n    return a * b\nprint(multiply(4))  # uses default value for b\nprint(multiply(4, 3))  # overrides default value for b\n\nExplanation:\nThis function multiply(a, b=2) takes two arguments but assigns a default value of 2 to b. If only one argument is passed, the function uses the default value. Calling multiply(4) returns 8, while multiply(4, 3) returns 12.\nCode:\n\ndef divide(a, b):\n    \"\"\"Divides a by b and handles division by zero.\"\"\"\n    if b == 0:\n        return \"Cannot divide by zero!\"\n    return a / b\nprint(divide(10, 2))\nprint(divide(10, 0))\n\nExplanation:\ndivide(a, b) takes two arguments and returns the result of dividing a by b. It includes a conditional to check for division by zero. If b is 0, it returns an error message. Calling divide(10, 2) returns 5.0, while divide(10, 0) returns \"Cannot divide by zero!\".\nCode:\n\ndef square_elements(numbers):\n    \"\"\"Takes a list of numbers and returns a list of their squares.\"\"\"\n    return [x ** 2 for x in numbers]\nprint(square_elements([1, 2, 3, 4]))\n\nExplanation:\nThis function square_elements(numbers) takes a list of numbers and returns a new list containing the squares of those numbers. The function uses list comprehension. Calling square_elements([1, 2, 3, 4]) returns [1, 4, 9, 16].\nCode:\n\ndef factorial(n):\n    \"\"\"Recursively calculates the factorial of n.\"\"\"\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\nprint(factorial(5))\n\nExplanation:\nThis function factorial(n) uses recursion to calculate the factorial of a number. If n is 0, it returns 1 (base case). Otherwise, it multiplies n by factorial(n - 1). Calling factorial(5) returns 120.\nCode:\n\ndef is_even(number):\n    \"\"\"Checks if a number is even.\"\"\"\n    return number % 2 == 0\nprint(is_even(4))  # True\nprint(is_even(7))  # False\n\nExplanation:\nThe function is_even(number) checks if a number is even by using the modulo operator (%). If the remainder is 0, it returns True, otherwise False. Calling is_even(4) returns True, and is_even(7) returns False.\nCode:\n\ndef describe_person(name, age, *hobbies):\n    \"\"\"Takes a name, age, and any number of hobbies, and prints a description.\"\"\"\n    print(f\"{name} is {age} years old and enjoys {', '.join(hobbies)}.\")\ndescribe_person(\"Alice\", 30, \"reading\", \"hiking\", \"cooking\")\n\nExplanation:\nThis function describe_person(name, age, *hobbies) accepts a variable number of hobby arguments using the * syntax, which collects extra arguments into a tuple. The join() method creates a string from the hobbies. Calling describe_person(\"Alice\", 30, \"reading\", \"hiking\", \"cooking\") prints \"Alice is 30 years old and enjoys reading, hiking, cooking.\"\nCode:\n\ndef calculate_average(*numbers):\n    \"\"\"Calculates the average of any number of values.\"\"\"\n    if len(numbers) == 0:\n        return 0\n    return sum(numbers) / len(numbers)\nprint(calculate_average(5, 10, 15))\nprint(calculate_average())\n\nExplanation:\nThe calculate_average(*numbers) function calculates the average of any number of arguments. It first checks if any numbers were provided (if the length of numbers is 0, it returns 0), then calculates the average by dividing the sum by the length. Calling calculate_average(5, 10, 15) returns 10.0, and calculate_average() returns 0.\n\nThese examples show different ways to define functions with varying arguments, handling edge cases, using recursion, and incorporating function documentation.",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html#main-statement",
    "href": "pyws01-2-getting-started.html#main-statement",
    "title": "basic python syntax",
    "section": "main statement",
    "text": "main statement\nHere is a sample Python script that defines two simple functions and includes a main statement to call them:\n\n# Function to add two numbers\ndef add_numbers(a, b):\n    return a + b\n\n# Function to subtract two numbers\ndef subtract_numbers(a, b):\n    return a - b\n\n# Main statement\nif __name__ == \"__main__\":\n    num1 = 10\n    num2 = 5\n\n    # Calling the functions\n    sum_result = add_numbers(num1, num2)\n    diff_result = subtract_numbers(num1, num2)\n\n    # Printing the results\n    print(f\"The sum of {num1} and {num2} is: {sum_result}\")\n    print(f\"The difference between {num1} and {num2} is: {diff_result}\")\n\nExplanation:\n\nadd_numbers(a, b): A simple function that takes two arguments and returns their sum.\nsubtract_numbers(a, b): A simple function that takes two arguments and returns their difference.\nMain statement (if __name__ == \"__main__\":): This block ensures that the code inside it runs only when the script is executed directly, not when it’s imported as a module.\nCalling the functions: Inside the main block, it calls the add_numbers and subtract_numbers functions with num1 and num2 as arguments.\nPrint results: Displays the results of the addition and subtraction operations.",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The following info is fetched from timeedit:",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "about.html#workshop-schedule",
    "href": "about.html#workshop-schedule",
    "title": "About",
    "section": "",
    "text": "The following info is fetched from timeedit:",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "about.html#workshop-map",
    "href": "about.html#workshop-map",
    "title": "About",
    "section": "Workshop map",
    "text": "Workshop map\n\n\n\nworkshop locations",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "pyws02-2-data-analysis.html",
    "href": "pyws02-2-data-analysis.html",
    "title": "result visualizations",
    "section": "",
    "text": "example gallery matplotlib\nexample gallery seaborn\nexample gallery plotnine\nexample gallery plotly\nexample gallery altair\ndownload jupyter notebook",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "result visualizations"
    ]
  },
  {
    "objectID": "pyws02-2-data-analysis.html#descriptive-visualizations",
    "href": "pyws02-2-data-analysis.html#descriptive-visualizations",
    "title": "result visualizations",
    "section": "descriptive visualizations",
    "text": "descriptive visualizations",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "result visualizations"
    ]
  },
  {
    "objectID": "pyws02-2-data-analysis.html#inferential-visualizations",
    "href": "pyws02-2-data-analysis.html#inferential-visualizations",
    "title": "result visualizations",
    "section": "inferential visualizations",
    "text": "inferential visualizations\n\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\n\n# Load the example tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Draw a nested boxplot to show bills by day and time\nsns.boxplot(x=\"day\", y=\"total_bill\",\n            hue=\"smoker\", palette=[\"m\", \"g\"],\n            data=tips)\nsns.despine(offset=10, trim=True)",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "result visualizations"
    ]
  },
  {
    "objectID": "pyws02-2-data-analysis.html#testing",
    "href": "pyws02-2-data-analysis.html#testing",
    "title": "result visualizations",
    "section": "testing",
    "text": "testing\n\n# prompt: acquire dataset penguins from seaborn package, read it into a polars dataframe, generate a data analysis pipeline where bill_length is aggregated by species, pipe the aggregated data out to a plotnine bar chart with error bars\n\n#!pip install polars plotnine\n\nimport seaborn as sns\nimport polars as pl\nfrom plotnine import *\n\n# Load the penguins dataset\npenguins = sns.load_dataset('penguins')\n\n# Convert to a Polars DataFrame\ndf = pl.from_pandas(penguins)\n\n# Define the data analysis pipeline\n(\n    df\n    .group_by('species')\n    .agg([\n        pl.col('bill_length_mm').mean().alias('mean_bill_length'),\n        pl.col('bill_length_mm').std().alias('std_bill_length')\n    ])\n    .pipe(lambda df: (\n        ggplot(df, aes(x='species', y='mean_bill_length', fill='species'))\n        + geom_bar(stat='identity', position='dodge')\n        + geom_errorbar(aes(ymin='mean_bill_length - std_bill_length', ymax='mean_bill_length + std_bill_length'), width=0.2)\n        + theme_bw()\n        + labs(x='Species', y='Mean Bill Length (mm)', title='Mean Bill Length by Species')\n    ))\n    .draw()\n)",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "result visualizations"
    ]
  },
  {
    "objectID": "pyws03-0-text-analysis.html",
    "href": "pyws03-0-text-analysis.html",
    "title": "part 3: text analysis",
    "section": "",
    "text": "Reading and analyzing unstructured text presents a variety of challenges, particularly when dealing with different types of content such as articles, social media posts, or even longer documents like reports. Unstructured text, unlike tabular data, lacks a predefined format, which means it may contain irregularities such as spelling errors, slang, or varied sentence structures. For example, reading short texts like social media posts requires handling informal language, abbreviations, and emojis, whereas longer texts such as research papers or survey responses involve the challenge of organizing and summarizing large amounts of information. Rendered texts, such as those from PDFs or HTML files, often come with additional complexities like layout formatting, metadata, or embedded images, which may need to be stripped out or interpreted correctly before the actual text can be processed. Handling such varied lengths and formats requires careful preprocessing, including cleaning and standardizing the text data to make it suitable for analysis.",
    "crumbs": [
      "workshop",
      "part 3: text analysis"
    ]
  },
  {
    "objectID": "pyws03-0-text-analysis.html#reading-text-content",
    "href": "pyws03-0-text-analysis.html#reading-text-content",
    "title": "part 3: text analysis",
    "section": "",
    "text": "Reading and analyzing unstructured text presents a variety of challenges, particularly when dealing with different types of content such as articles, social media posts, or even longer documents like reports. Unstructured text, unlike tabular data, lacks a predefined format, which means it may contain irregularities such as spelling errors, slang, or varied sentence structures. For example, reading short texts like social media posts requires handling informal language, abbreviations, and emojis, whereas longer texts such as research papers or survey responses involve the challenge of organizing and summarizing large amounts of information. Rendered texts, such as those from PDFs or HTML files, often come with additional complexities like layout formatting, metadata, or embedded images, which may need to be stripped out or interpreted correctly before the actual text can be processed. Handling such varied lengths and formats requires careful preprocessing, including cleaning and standardizing the text data to make it suitable for analysis.",
    "crumbs": [
      "workshop",
      "part 3: text analysis"
    ]
  },
  {
    "objectID": "pyws03-0-text-analysis.html#text-tokenization",
    "href": "pyws03-0-text-analysis.html#text-tokenization",
    "title": "part 3: text analysis",
    "section": "text tokenization",
    "text": "text tokenization\nText tokenization is the foundational step in processing unstructured text, where the text is broken down into smaller units like words or phrases. This allows for basic analysis, such as word frequency counts or simple keyword extraction. However, more advanced techniques go beyond tokenization to capture deeper insights from text. Topic modeling, for instance, identifies latent themes within large text corpora by analyzing patterns in word co-occurrences. It helps uncover hidden structures in the data, which can be particularly useful when working with large datasets of documents, like survey responses or news articles. Sentiment analysis is another advanced technique that goes beyond simple tokenization by determining the emotional tone behind the text, whether it is positive, negative, or neutral. This is especially useful for analyzing customer feedback or social media sentiment. Finally, word vectorization techniques such as Word2Vec or GloVe transform words into numerical vectors based on their context, enabling more sophisticated tasks like measuring semantic similarity between words or phrases, clustering similar documents, or feeding text data into machine learning models. While tokenization provides a starting point, these more advanced techniques enable richer and more meaningful interpretations of textual data.",
    "crumbs": [
      "workshop",
      "part 3: text analysis"
    ]
  },
  {
    "objectID": "pyws03-2-text-analysis.html",
    "href": "pyws03-2-text-analysis.html",
    "title": "text tokenization",
    "section": "",
    "text": "code examples nltk\ncode examples spacy\ndownload jupyter notebook",
    "crumbs": [
      "workshop",
      "part 3: text analysis",
      "text tokenization"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Welcome! This intro to Python workshop is oriented around the principles of Open Science. Python, with its versatile ecosystem and powerful libraries, is a cornerstone for computational data analysis in social science research, enabling key Open Science principles like replicability, transparency, collaboration, and the use of open-source software. Python scripts allow researchers to automate and document their analysis, ensuring replicability—other researchers can run the same code on the same data to verify findings. Transparency is achieved by openly sharing the code, data, and methodology, so that every step of the research process is accessible and understandable. Python also fosters collaboration by allowing multiple researchers to contribute to the same project, often through platforms like GitHub, where code is shared, versioned, and improved. Finally, Python itself is open-source software, meaning anyone can use, modify, and distribute it freely, promoting equitable access to powerful research tools and fostering an inclusive scientific community.\nWorkshop link and Schedule\n\nPart 1: Getting Started\nIn this part, participants will be introduced to Python environments, with a focus on using Google Colab Notebooks, an accessible and powerful tool for coding in Python. We will cover the installation and setup process, ensuring everyone is ready to run Python code in their browsers. The session will also explore basic Python syntax, including variables, data types, and control structures like loops and conditionals, laying the foundation for more advanced applications in social science research.\n\n\nPart 2: Data Analysis\nThis section focuses on performing data analysis with Python, using libraries like Pandas for dataframe manipulation. Participants will learn how to load, clean, and transform data, enabling them to analyze datasets commonly used in social science. We’ll also dive into results visualization using Matplotlib and Seaborn, teaching participants how to create informative charts and graphs to present their findings effectively.\n\n\nPart 3: Text Analysis\nIn this part, participants will explore the basics of text analysis in Python, starting with reading text data from various sources such as documents or online content. We will cover text tokenization, the process of breaking text into individual words or phrases, using libraries like spaCy and NLTK. This will allow participants to process, analyze, and extract meaningful insights from large volumes of textual data, such as social media posts or survey responses.\n\n\nPart 4: Image Analysis\nThe image analysis section will introduce participants to working with visual data in Python. We’ll cover how to read and process image content using libraries like OpenCV and Pillow. Participants will also explore basic object recognition techniques, learning how to detect and classify objects within images, which is particularly useful for social science fields that rely on visual data, such as media studies or behavioral analysis.\n\n\nPart 5: Data Collection\nThe final part of the workshop focuses on data collection techniques using Python. Participants will learn web scraping methods to gather data from websites using tools like BeautifulSoup and Scrapy. Additionally, we will explore how to design and run web experiments, allowing researchers to collect behavioral data from users in real-time. These skills will empower participants to gather the data they need for their social science research projects.\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "start"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The following info is fetched from timeedit:"
  },
  {
    "objectID": "pyws02-1-data-analysis.html",
    "href": "pyws02-1-data-analysis.html",
    "title": "dataframe manipulation",
    "section": "",
    "text": "dataframe examples pandas\ndataframe examples polars\ndownload jupyter notebook",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "dataframe manipulation"
    ]
  },
  {
    "objectID": "pyws02-1-data-analysis.html#read-and-explore-data",
    "href": "pyws02-1-data-analysis.html#read-and-explore-data",
    "title": "dataframe manipulation",
    "section": "read and explore data",
    "text": "read and explore data\nHere is an analysis of your code, separated into logical steps with each step in its own code chunk, followed by explanations:\n\n1. Import necessary libraries\n\nimport seaborn as sns\nimport pandas as pd\n\nExplanation:\nThis step imports the required libraries for the script. seaborn is a data visualization library that provides built-in datasets, and pandas is used for data manipulation and analysis. Importing these libraries allows the script to use their functions, such as loading a dataset and manipulating DataFrames.\n\n\n\n2. Load the dataset and select specific columns\n\ndf = sns.load_dataset(\"penguins\")\ndf_selected = df[['body_mass_g', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'island']]\n\nExplanation:\nHere, the penguins dataset from Seaborn is loaded into a Pandas DataFrame (df). The next step selects specific columns of interest (body_mass_g, bill_length_mm, bill_depth_mm, flipper_length_mm, and island) and stores them in df_selected. These columns represent both the dependent and independent variables used for further analysis.\n\n\n\n3. Rename selected columns for easier reference\n\ndf_renamed = df_selected.rename(columns={\n    'body_mass_g': 'dep_var', \n    'bill_length_mm': 'indep_var_1', \n    'bill_depth_mm': 'indep_var_2', \n    'flipper_length_mm': 'indep_var_3', \n    'island': 'indep_var_4'\n})\n\nExplanation:\nThis step renames the columns of df_selected to more generic names for easier reference. The dependent variable (body_mass_g) is renamed to dep_var, and the independent variables are renamed to indep_var_1, indep_var_2, indep_var_3, and indep_var_4 (for island). This renamed DataFrame (df_renamed) is used for subsequent analysis.\n\n\n\n4. Save the renamed DataFrame as a TSV file\n\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/csv/data.tsv'\ndf_renamed.to_csv(output_file_path, sep='\\t', index=False)\n\nExplanation:\nThe renamed DataFrame is saved to a TSV (tab-separated values) file at the specified path. The to_csv() method is used with the sep='\\t' argument to ensure that the file is saved in TSV format. The index=False option prevents the DataFrame index from being written to the file.\n\n\n\n5. Generate summary statistics of the DataFrame\n\nsummary_stats = df_renamed.describe()\n\nExplanation:\nThis step generates summary statistics for all numeric columns in the DataFrame using the describe() function. The resulting DataFrame (summary_stats) contains descriptive statistics such as count, mean, standard deviation, minimum, and maximum values, as well as the quartile ranges for the selected variables.\n\n\n\n6. Retrieve the first five records of the DataFrame\n\nfirst_five_records = df_renamed.head()\n\nExplanation:\nThe head() function retrieves the first five rows of the DataFrame. This is useful for a quick inspection of the dataset to verify that the data was loaded and renamed correctly. The first_five_records DataFrame contains the first five records of the renamed DataFrame.\n\n\n\n7. Convert a column to a categorical data type\n\ndf_renamed['indep_var_4'] = df_renamed['indep_var_4'].astype('category')\n\nExplanation:\nThis step converts the indep_var_4 column (formerly island) to a categorical data type using astype('category'). Categorical data types are more memory efficient and appropriate when dealing with a limited number of distinct values, such as categorical variables in a dataset.\n\n\n\n8. Fill missing values in numeric columns with their mean\n\ndf_filled = df_renamed.fillna(df_renamed.mean(numeric_only=True))\n\nExplanation:\nIn this step, any missing values in the numeric columns of df_renamed are filled with the mean value of each column using fillna(). The mean(numeric_only=True) calculates the mean for only numeric columns, and fillna() replaces the missing values with these means. The modified DataFrame is stored as df_filled.\n\n\n\n9. Remove rows with any remaining missing values\n\ndf_no_missing = df_filled.dropna()\n\nExplanation:\nHere, the dropna() function is used to remove any rows that still contain missing values in the DataFrame after filling the numeric columns. Rows with missing values in non-numeric columns will be dropped. The cleaned DataFrame is stored as df_no_missing.\n\n\n\n10. Remove duplicate records\n\ndf_no_duplicates = df_no_missing.drop_duplicates()\n\nExplanation:\nThis step removes any duplicate rows from the DataFrame using the drop_duplicates() method. Duplicate rows are those where all column values are identical. The resulting DataFrame (df_no_duplicates) contains only unique rows, ensuring there are no duplicate records in the dataset.\n\n\n\n11. Output summary statistics and first five records\n\nprint(summary_stats)\nprint(first_five_records)\n\nExplanation:\nThe final step prints the summary statistics generated earlier (summary_stats) and the first five records (first_five_records) of the renamed DataFrame. This provides an overview of the dataset and allows verification that the data processing steps were applied correctly.",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "dataframe manipulation"
    ]
  },
  {
    "objectID": "pyws02-1-data-analysis.html#select-and-group-data",
    "href": "pyws02-1-data-analysis.html#select-and-group-data",
    "title": "dataframe manipulation",
    "section": "select and group data",
    "text": "select and group data\nHere is an analysis of your Python code, separated into logical steps with each step in its own code chunk, followed by explanations:\n\n1. Import the necessary library\n\nimport pandas as pd\n\nExplanation:\nThis line imports the Pandas library, a powerful Python tool for data manipulation and analysis. By importing Pandas, the script gains access to functions like reading data, filtering, grouping, and saving results.\n\n\n\n2. Load DataFrame from TSV File\n\ninput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/csv/data.tsv'\ndf = pd.read_csv(input_file_path, sep='\\t')\n\nExplanation:\nThis block reads a TSV (tab-separated values) file from the specified file path (input_file_path) into a Pandas DataFrame (df). The sep='\\t' argument specifies that the file uses tabs as delimiters. The result is a DataFrame containing the data from the TSV file, with columns and rows ready for further manipulation.\n\n\n\n3. Filter the Data\n\nfiltered_df = df[df['dep_var'] &gt; 3500]\n\nExplanation:\nHere, the DataFrame is filtered to include only rows where the value in the dep_var column is greater than 3500. The resulting filtered DataFrame (filtered_df) contains a subset of the original data that meets this condition.\n\n\n\n4. Select Specific Columns\n\nselected_columns = filtered_df[['dep_var', 'indep_var_1', 'indep_var_4']]\n\nExplanation:\nIn this step, a new DataFrame (selected_columns) is created by selecting only the specified columns (dep_var, indep_var_1, and indep_var_4) from the previously filtered DataFrame. This reduces the dataset to just the relevant columns needed for further analysis.\n\n\n\n5. Group Data and Calculate the Mean\n\ngrouped_data = df.groupby('indep_var_4')['dep_var'].mean().reset_index()\n\nExplanation:\nThis block groups the data by the indep_var_4 column and calculates the mean of the dep_var column for each group. The result is stored in grouped_data, a DataFrame containing the unique values of indep_var_4 and their corresponding mean dep_var values. The reset_index() function ensures the grouped values are converted back into a DataFrame format.\n\n\n\n6. Merge DataFrames\n\nmerged_df = pd.merge(df, grouped_data, on='indep_var_4', suffixes=('', '_mean'))\n\nExplanation:\nThis step merges the original DataFrame (df) with the grouped_data DataFrame on the indep_var_4 column. The result (merged_df) contains all original columns from df along with the mean dep_var for each group. The suffixes=('', '_mean') ensures that the new dep_var_mean column has a distinct name.\n\n\n\n7. Calculate a New Column\n\nmerged_df['dep_var_diff'] = merged_df['dep_var'] - merged_df['dep_var_mean']\n\nExplanation:\nA new column (dep_var_diff) is added to the merged_df DataFrame. This column represents the difference between the original dep_var values and the mean dep_var values for each group (from the merged dep_var_mean column). The result provides insight into how each dep_var deviates from the group mean.\n\n\n\n8. Create a Pivot Table\n\npivot_table = merged_df.pivot_table(values='dep_var', index='indep_var_4', aggfunc='mean')\n\nExplanation:\nThis block creates a pivot table from merged_df, where indep_var_4 becomes the index, and the mean of dep_var is calculated for each value of indep_var_4. The result is stored in the pivot_table DataFrame, which aggregates the data by indep_var_4 and provides a summary of the mean dep_var.\n\n\n\n9. Save the Pivot Table as a TSV File\n\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/csv/data_pivot.tsv'\npivot_table.to_csv(output_file_path, sep='\\t')\n\nExplanation:\nIn this step, the pivot_table DataFrame is saved as a TSV file to the specified path (output_file_path). The sep='\\t' argument ensures that the data is saved in tab-separated format. This allows the pivot table to be stored and used for further analysis or reporting.\n\n\n\n10. Output Results to Verify\n\nprint(filtered_df.head())\nprint(selected_columns.head())\nprint(grouped_data.head())\nprint(merged_df.head())\nprint(pivot_table)\n\nExplanation:\nThis final block prints the first few rows (head()) of various DataFrames, including the filtered data, selected columns, grouped data, merged data, and the pivot table. These print statements allow you to verify the results of each step in the analysis.",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "dataframe manipulation"
    ]
  },
  {
    "objectID": "pyws01-1-getting-started.html",
    "href": "pyws01-1-getting-started.html",
    "title": "python environments",
    "section": "",
    "text": "sign in to google account to connect colab cloud app\n\n\n\nhttps://colab.research.google.com/",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "python environments"
    ]
  },
  {
    "objectID": "pyws01-1-getting-started.html#google-colab",
    "href": "pyws01-1-getting-started.html#google-colab",
    "title": "python environments",
    "section": "",
    "text": "sign in to google account to connect colab cloud app\n\n\n\nhttps://colab.research.google.com/",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "python environments"
    ]
  },
  {
    "objectID": "pyws01-1-getting-started.html#visual-studio-code",
    "href": "pyws01-1-getting-started.html#visual-studio-code",
    "title": "python environments",
    "section": "visual studio code",
    "text": "visual studio code\n\n\n\ndownload and install vscode app on local computer\n\n\n\nhttps://code.visualstudio.com/download\nhttps://www.python.org/downloads/",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "python environments"
    ]
  },
  {
    "objectID": "pyws01-1-getting-started.html#anaconda",
    "href": "pyws01-1-getting-started.html#anaconda",
    "title": "python environments",
    "section": "anaconda",
    "text": "anaconda\n\n\n\ndownload and install anaconda on local computer\n\n\n\nhttps://www.anaconda.com/download",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "python environments"
    ]
  },
  {
    "objectID": "pyws02-0-data-analysis.html",
    "href": "pyws02-0-data-analysis.html",
    "title": "part 2: data analysis",
    "section": "",
    "text": "Dataframe operations in Python, particularly with the Pandas library, allow for highly flexible and efficient data manipulation, making it a cornerstone of data analysis in fields like social science. One of the most common operations involves loading data from various sources such as CSV files, databases, or APIs, transforming it into a structured dataframe for analysis. From there, you can perform cleaning operations, such as handling missing values, filtering rows or columns, and converting data types. These cleaning steps are essential in real-world data analysis, where datasets are rarely perfect. Once the data is clean, more advanced dataframe operations come into play, such as grouping and aggregating data by specific columns, which allows users to extract insights like means, counts, or sums based on key categories. Pandas also supports joining and merging data from multiple sources, enabling researchers to combine different datasets for more comprehensive analyses. These operations contrast with the more manual approaches used in tools like Excel, where data manipulation might involve repetitive tasks like copy-pasting or formula adjustments, which can be error-prone and less scalable.",
    "crumbs": [
      "workshop",
      "part 2: data analysis"
    ]
  },
  {
    "objectID": "pyws02-0-data-analysis.html#dataframe-manipulation",
    "href": "pyws02-0-data-analysis.html#dataframe-manipulation",
    "title": "part 2: data analysis",
    "section": "",
    "text": "Dataframe operations in Python, particularly with the Pandas library, allow for highly flexible and efficient data manipulation, making it a cornerstone of data analysis in fields like social science. One of the most common operations involves loading data from various sources such as CSV files, databases, or APIs, transforming it into a structured dataframe for analysis. From there, you can perform cleaning operations, such as handling missing values, filtering rows or columns, and converting data types. These cleaning steps are essential in real-world data analysis, where datasets are rarely perfect. Once the data is clean, more advanced dataframe operations come into play, such as grouping and aggregating data by specific columns, which allows users to extract insights like means, counts, or sums based on key categories. Pandas also supports joining and merging data from multiple sources, enabling researchers to combine different datasets for more comprehensive analyses. These operations contrast with the more manual approaches used in tools like Excel, where data manipulation might involve repetitive tasks like copy-pasting or formula adjustments, which can be error-prone and less scalable.",
    "crumbs": [
      "workshop",
      "part 2: data analysis"
    ]
  },
  {
    "objectID": "pyws02-0-data-analysis.html#result-visualizations",
    "href": "pyws02-0-data-analysis.html#result-visualizations",
    "title": "part 2: data analysis",
    "section": "result visualizations",
    "text": "result visualizations\nWhen it comes to data visualization, it’s important to distinguish between descriptive and inferential visualizations, both of which play a key role in data analysis. Descriptive visualizations, often created using libraries like Matplotlib and Seaborn, are used to summarize data in a visually intuitive way. Examples include bar charts, histograms, and scatter plots, which help the analyst understand the distribution, trends, or relationships within a dataset. These types of visualizations are valuable for exploratory data analysis, where the goal is to make sense of the data’s underlying patterns and features. On the other hand, inferential visualizations take things a step further by incorporating statistical models to make predictions or generalizations about a population based on sample data. Examples include confidence intervals, regression lines, or p-value plots, which are often layered on top of standard visualizations to highlight the uncertainty or significance of findings. While descriptive visualizations provide an overview of the data at hand, inferential visualizations are essential for drawing conclusions that extend beyond the immediate dataset, particularly in social science research, where researchers often need to infer trends across larger populations.",
    "crumbs": [
      "workshop",
      "part 2: data analysis"
    ]
  },
  {
    "objectID": "pyws04-1-image-analysis.html",
    "href": "pyws04-1-image-analysis.html",
    "title": "reading image content",
    "section": "",
    "text": "code examples pil, pillow\ncode examples opencv\ndownload jupyter notebook",
    "crumbs": [
      "workshop",
      "part 4: image analysis",
      "reading image content"
    ]
  }
]