[
  {
    "objectID": "pyws01-0-getting-started.html",
    "href": "pyws01-0-getting-started.html",
    "title": "part 1: getting started",
    "section": "",
    "text": "Google Colab Notebooks offer a unique, cloud-based Python environment, making it particularly convenient for data science and research applications. Since Colab runs entirely in the cloud, participants don’t need to worry about local installations or hardware limitations. With Colab, you can access a fully functioning Python environment directly through your browser, complete with popular libraries like NumPy, Pandas, and Matplotlib pre-installed. This makes it ideal for users who may have limited access to high-performance hardware, as Google’s servers handle the heavy lifting. Additionally, Colab supports collaboration, allowing multiple users to work on the same notebook in real-time, much like Google Docs. This contrasts with traditional local Python environments such as Anaconda or PyCharm, where you must install Python and manage dependencies on your own machine. While local environments offer greater control and customization, they can also be more challenging to configure for beginners, particularly when it comes to managing different Python packages and versions.",
    "crumbs": [
      "workshop",
      "part 1: getting started"
    ]
  },
  {
    "objectID": "pyws01-0-getting-started.html#python-environments",
    "href": "pyws01-0-getting-started.html#python-environments",
    "title": "part 1: getting started",
    "section": "",
    "text": "Google Colab Notebooks offer a unique, cloud-based Python environment, making it particularly convenient for data science and research applications. Since Colab runs entirely in the cloud, participants don’t need to worry about local installations or hardware limitations. With Colab, you can access a fully functioning Python environment directly through your browser, complete with popular libraries like NumPy, Pandas, and Matplotlib pre-installed. This makes it ideal for users who may have limited access to high-performance hardware, as Google’s servers handle the heavy lifting. Additionally, Colab supports collaboration, allowing multiple users to work on the same notebook in real-time, much like Google Docs. This contrasts with traditional local Python environments such as Anaconda or PyCharm, where you must install Python and manage dependencies on your own machine. While local environments offer greater control and customization, they can also be more challenging to configure for beginners, particularly when it comes to managing different Python packages and versions.",
    "crumbs": [
      "workshop",
      "part 1: getting started"
    ]
  },
  {
    "objectID": "pyws01-0-getting-started.html#basic-python-syntax",
    "href": "pyws01-0-getting-started.html#basic-python-syntax",
    "title": "part 1: getting started",
    "section": "basic python syntax",
    "text": "basic python syntax\nWhen comparing Python syntax to other popular data science platforms such as Excel or SPSS, Python provides much more flexibility and scalability. Excel is widely used for smaller datasets and offers an intuitive, visual interface for users without coding experience. However, it can become cumbersome for handling large datasets or performing more complex operations, like advanced statistical analysis or machine learning. In contrast, Python’s syntax is relatively simple but powerful, allowing users to write reusable scripts for tasks like data manipulation, cleaning, and analysis. Control structures in Python, such as loops and conditionals, provide much greater control over data operations compared to the rigid, formula-based system of Excel. Moreover, Python can handle more diverse data types and larger datasets more efficiently than Excel, which tends to slow down or crash with extensive data. Compared to SPSS, a statistical software package, Python offers greater flexibility with open-source libraries like SciPy and StatsModels, though SPSS remains easier for non-programmers due to its point-and-click interface. Ultimately, Python is an excellent choice for users looking to scale their work, automate processes, or engage in more complex data science tasks.",
    "crumbs": [
      "workshop",
      "part 1: getting started"
    ]
  },
  {
    "objectID": "pyws03-1-text-analysis.html",
    "href": "pyws03-1-text-analysis.html",
    "title": "reading text content",
    "section": "",
    "text": "code examples nltk\ncode examples spacy\ndownload jupyter notebook\n# run inside google colab\n#!git clone https://github.com/cca-cce/osm-cca-nlp.git",
    "crumbs": [
      "workshop",
      "part 3: text analysis",
      "reading text content"
    ]
  },
  {
    "objectID": "pyws03-1-text-analysis.html#nltk-and-text-corpora",
    "href": "pyws03-1-text-analysis.html#nltk-and-text-corpora",
    "title": "reading text content",
    "section": "nltk and text corpora",
    "text": "nltk and text corpora\n\nImport Libraries and Download NLTK Data\nIn this step, we import the necessary libraries and download the required NLTK data packages. Specifically, we use NLTK’s download function to ensure the ‘gutenberg’ corpus and the ‘punkt’ tokenizer are available for use. The ‘punkt’ tokenizer is essential for splitting text into sentences and words.\n\nimport nltk\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfrom IPython.display import display\n\n# Download necessary NLTK data files\nnltk.download('gutenberg')\nnltk.download('punkt')\nnltk.download('punkt_tab')\n\nTrue\n\n\n\n\nLoad the Gutenberg Corpus\nHere, we import the Gutenberg corpus from NLTK’s corpus module. The Gutenberg corpus is a collection of literary texts that we will analyze. We retrieve the list of file IDs available in the corpus using gutenberg.fileids(), which provides us with the filenames of the texts in the corpus.\n\nfrom nltk.corpus import gutenberg\n\n# Get list of file IDs from the Gutenberg corpus\nfile_ids = gutenberg.fileids()\n\n\n\nAnalyze Each Text in the Corpus\nIn this section, we iterate over each text in the Gutenberg corpus to compute various linguistic statistics. We use NLTK’s raw() method to get the raw text, word_tokenize() to split the text into words, and sent_tokenize() to split the text into sentences. These NLTK tokenizers are essential for textual analysis.\n\n# Initialize a list to store statistics\nstats_list = []\n\n# Analyze each text in the corpus\nfor file_id in file_ids:\n    raw_text = gutenberg.raw(file_id)\n    words = nltk.word_tokenize(raw_text)\n    sentences = nltk.sent_tokenize(raw_text)\n    num_words = len(words)\n    num_sentences = len(sentences)\n    avg_word_length = sum(len(word) for word in words) / num_words\n    vocab_size = len(set(words))\n    lexical_diversity = vocab_size / num_words\n    stats_list.append({\n        'Title': file_id,\n        'Num_Words': num_words,\n        'Num_Sentences': num_sentences,\n        'Avg_Word_Length': avg_word_length,\n        'Vocab_Size': vocab_size,\n        'Lexical_Diversity': lexical_diversity\n    })\n\n\n\nCreate and Display the DataFrame\nWe create a pandas DataFrame from the collected statistics for easier analysis and display it within the notebook using display(). This allows us to view the computed statistics in a structured tabular format.\n\n# Create a DataFrame to hold the statistics\nstats_df = pd.DataFrame(stats_list)\n\n# Display the statistics table\ndisplay(stats_df)\n\n\n\n\n\n\n\n\nTitle\nNum_Words\nNum_Sentences\nAvg_Word_Length\nVocab_Size\nLexical_Diversity\n\n\n\n\n0\nausten-emma.txt\n191855\n7493\n3.788288\n8376\n0.043658\n\n\n1\nausten-persuasion.txt\n97940\n3654\n3.896130\n6218\n0.063488\n\n\n2\nausten-sense.txt\n141440\n4833\n3.906908\n7078\n0.050042\n\n\n3\nbible-kjv.txt\n947008\n29812\n3.679540\n18120\n0.019134\n\n\n4\nblake-poems.txt\n8239\n355\n3.562690\n1815\n0.220294\n\n\n5\nbryant-stories.txt\n55649\n2715\n3.526982\n4460\n0.080145\n\n\n6\nburgess-busterbrown.txt\n18571\n1001\n3.608906\n1739\n0.093641\n\n\n7\ncarroll-alice.txt\n33535\n1625\n3.462711\n3157\n0.094140\n\n\n8\nchesterton-ball.txt\n97277\n4624\n3.849430\n9050\n0.093033\n\n\n9\nchesterton-brown.txt\n85489\n3712\n3.849747\n8482\n0.099217\n\n\n10\nchesterton-thursday.txt\n69443\n3588\n3.800081\n6916\n0.099592\n\n\n11\nedgeworth-parents.txt\n209310\n10096\n3.568989\n9845\n0.047035\n\n\n12\nmelville-moby_dick.txt\n255222\n9852\n3.926444\n20639\n0.080867\n\n\n13\nmilton-paradise.txt\n95716\n1835\n3.932947\n10986\n0.114777\n\n\n14\nshakespeare-caesar.txt\n25277\n1592\n3.519880\n3601\n0.142462\n\n\n15\nshakespeare-hamlet.txt\n36411\n2355\n3.554008\n5515\n0.151465\n\n\n16\nshakespeare-macbeth.txt\n22274\n1465\n3.600072\n4076\n0.182994\n\n\n17\nwhitman-leaves.txt\n149249\n3827\n3.835972\n15552\n0.104202\n\n\n\n\n\n\n\n\n\nSet Up the Output Directory\nHere, we define the output path where we’ll save the text files and figures. We use os.makedirs() with exist_ok=True to create the directory if it doesn’t already exist, ensuring that our output files have a designated location.\n\n# Define the output path for saving text files and figures\noutput_path = \"/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/tmp\"\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_path, exist_ok=True)\n\n\n\nGenerate and Display Plots\nIn this step, we create various plots to visualize the text statistics using Seaborn and Matplotlib. We display these plots inline in the notebook using plt.show(). The plots include:\n\nA bar plot of the number of words per text.\nA bar plot of the average word length per text.\nA scatter plot of vocabulary size versus the number of words.\n\nWe utilize NLTK’s tokenization outputs to extract the necessary values for plotting.\n\n# Set up seaborn style\nsns.set(style='whitegrid')\n\n# Bar plot of number of words per text\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Title', y='Num_Words', data=stats_df)\nplt.xticks(rotation=45)\nplt.title('Number of Words per Text')\nplt.tight_layout()\nplt.savefig(os.path.join(output_path, 'num_words_per_text.png'))\nplt.show()\n\n# Bar plot of average word length per text\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Title', y='Avg_Word_Length', data=stats_df)\nplt.xticks(rotation=45)\nplt.title('Average Word Length per Text')\nplt.tight_layout()\nplt.savefig(os.path.join(output_path, 'avg_word_length_per_text.png'))\nplt.show()\n\n# Scatter plot of vocabulary size vs. number of words\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='Num_Words', y='Vocab_Size', data=stats_df, hue='Title')\nplt.title('Vocabulary Size vs. Number of Words')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.savefig(os.path.join(output_path, 'vocab_size_vs_num_words.png'))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSave Texts to Disk\nFinally, we save each text from the Gutenberg corpus as a plain text file to the specified output directory. We use NLTK’s raw() method again to retrieve the full text of each file and write it to disk using standard file I/O operations.\n\n# Save each text as a plain text file to the output path\nfor file_id in file_ids:\n    raw_text = gutenberg.raw(file_id)\n    output_file_path = os.path.join(output_path, file_id)\n    with open(output_file_path, 'w', encoding='utf-8') as f:\n        f.write(raw_text)\n\n\n\nLoad Saved Texts into an NLTK Corpus\nIn this final step, we read the saved texts from the output directory back into an NLTK corpus using PlaintextCorpusReader. This allows us to treat the collection of saved texts as a corpus for further analysis. PlaintextCorpusReader is an NLTK class designed to read plain text files from a directory and create a corpus object.\n\nfrom nltk.corpus import PlaintextCorpusReader\n\n# Define the corpus root directory\ncorpus_root = output_path\n\n# Define the pattern to match the text files (e.g., all files with .txt extension)\nfile_pattern = '.*'  # Matches all files\n# Matches only text files\nfile_pattern = r'.*\\.txt'  # Matches all files ending with .txt\n\n# Create a PlaintextCorpusReader object\nnew_corpus = PlaintextCorpusReader(corpus_root, file_pattern)\n\n# Access the file IDs in the new corpus\nnew_file_ids = new_corpus.fileids()\nprint(\"Files in the new corpus:\", new_file_ids)\n\n# Example: Read words from a specific file\nwords_in_file = new_corpus.words(new_file_ids[0])\nprint(\"First 20 words in\", new_file_ids[0], \":\", words_in_file[:20])\n\nFiles in the new corpus: ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\nFirst 20 words in austen-emma.txt : ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich']\n\n\nBy using PlaintextCorpusReader, we can load all the saved text files into a new NLTK corpus. The fileids() method lists all the files in the corpus, and methods like words(), sents(), and paras() allow us to access words, sentences, and paragraphs, respectively. This demonstrates NLTK’s capability to handle custom corpora built from local text files, enabling further text processing and analysis on the newly created corpus.",
    "crumbs": [
      "workshop",
      "part 3: text analysis",
      "reading text content"
    ]
  },
  {
    "objectID": "pyws03-1-text-analysis.html#text-to-pandas-dataframe",
    "href": "pyws03-1-text-analysis.html#text-to-pandas-dataframe",
    "title": "reading text content",
    "section": "text to pandas dataframe",
    "text": "text to pandas dataframe\n\nImport Libraries and Define Text Cleaning Function\nIn this step, we import the necessary libraries and define a function to clean text data. We use the os module for file and directory operations, re for regular expressions, pandas for data manipulation, and spacy for natural language processing tasks.\n\nimport os\nimport re\nimport pandas as pd\nimport spacy\n\n!python -m spacy download en_core_web_sm\n\n# Function to clean text by removing non-ASCII characters\ndef clean_text(text):\n    # Remove non-ASCII characters (commented out to preserve UTF-8 text)\n    # cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    cleaned_text = text\n    return cleaned_text\n\nThe clean_text function is intended to remove non-ASCII characters using re.sub. However, since we are dealing with UTF-8 encoded text (e.g., Swedish text data), we retain the original text by commenting out the removal line.\n\n\n\nSet Directory Paths and Initialize Data Structures\nHere, we specify the directory paths where the text files are located and initialize data structures for storing the text data. The directory_path variable holds the path to the directory containing the text files. We also initialize an empty list data to store the text information and a counter unique_id for assigning unique identifiers to each text.\n\n# Directory containing text files\ndirectory_path = '/content/osm-cca-nlp/res'\ndirectory_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/res'\n\n# Initialize an empty list to store the data\ndata = []\n\n# Initialize a unique ID counter\nunique_id = 1\n\nThe os module functions will later use directory_path to access the files. The unique_id will increment for each file, ensuring each text has a unique identifier.\n\n\n\nRead and Clean Text Files\nIn this section, we iterate over the text files in the specified directory, read their contents, clean the text using the clean_text function, and store the data in the data list. The os.listdir function lists all files in the directory, and os.path.join constructs the full file path.\n\n# Iterate over the text files in the directory\nfor filename in os.listdir(directory_path):\n    # Consider only plain text files\n    if filename.endswith(\".txt\") or filename.endswith(\".md\"):\n        file_path = os.path.join(directory_path, filename)\n\n        # Read the file content\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n\n        # Clean the text\n        cleaned_text = clean_text(text)\n\n        # Append the data as a dictionary with a unique ID\n        data.append({\n            'id': unique_id,\n            'filename': filename,\n            'original_text': text,\n            'cleaned_text': cleaned_text\n        })\n\n        # Increment the unique ID\n        unique_id += 1\n\nWe use open with encoding='utf-8' to read the files, ensuring that UTF-8 characters are handled correctly. The cleaned text and metadata are stored as dictionaries in the data list.\n\n\n\nCreate and Save DataFrame\nWe convert the collected data into a Pandas DataFrame for easier manipulation and analysis. We then save this DataFrame as a TSV (Tab-Separated Values) file using the to_csv method with sep='\\t'. The index=False parameter ensures that the DataFrame index is not included in the output file.\n\n# Create a Pandas DataFrame\ntext_df = pd.DataFrame(data)\n\n# Save the DataFrame as a TSV file in the 'csv' subdirectory\noutput_file_path = '/content/osm-cca-nlp/csv/text_data.tsv'\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/text_data.tsv'\n\n# Save the DataFrame to a TSV file\ntext_df.to_csv(output_file_path, sep='\\t', index=False)\n\n# Display the DataFrame\nprint(text_df)\n\n   id                                           filename  \\\n0   1  strategy-sustainable-development-lund-universi...   \n1   2          lu-sustainability-communication-01-min.md   \n\n                                       original_text  \\\n0  Page 1\\n\\nSTRATEGY  \\n\\n19 September 2019  \\n\\...   \n1  Your browser has javascript turned off or bloc...   \n\n                                        cleaned_text  \n0  Page 1\\n\\nSTRATEGY  \\n\\n19 September 2019  \\n\\...  \n1  Your browser has javascript turned off or bloc...  \n\n\nThis step utilizes Pandas’ data handling capabilities to structure our text data effectively and save it for future use.\n\n\n\nLoad spaCy Model\nWe load a spaCy language model to perform natural language processing tasks. The spacy.load function loads the specified model into memory. In this case, we use the small English model en_core_web_sm.\n\n# Load the spaCy model (small English model is used here)\nnlp = spacy.load(\"en_core_web_sm\")\n\nThe loaded nlp object provides access to spaCy’s powerful NLP features, including tokenization, part-of-speech tagging, and sentence segmentation.\n\n\n\nCompute Text Statistics\nWe calculate word counts, character counts, and sentence counts for each cleaned text in the DataFrame. Pandas’ apply function applies a lambda function to each row in the cleaned_text column. For sentence counting, we use spaCy’s sentence segmentation by processing the text with nlp and accessing the .sents attribute.\n\n# Perform word count and character count on each cleaned text in the DataFrame\ntext_df['word_count'] = text_df['cleaned_text'].apply(lambda x: len(x.split()))\ntext_df['character_count'] = text_df['cleaned_text'].apply(lambda x: len(x))\n\n# Perform sentence count using spaCy\ntext_df['sentence_count'] = text_df['cleaned_text'].apply(lambda x: len(list(nlp(x).sents)))\n\nThe len(x.split()) calculates the number of words by splitting the text on whitespace. The character count is obtained with len(x). For sentence count, we process the text with the spaCy model and convert the sents generator to a list to count the sentences.\n\n\n\nDisplay DataFrame with Selected Columns\nFinally, we display the DataFrame, excluding the ‘original_text’ and ‘cleaned_text’ columns for brevity. The columns.difference function identifies columns to exclude, and we use this to select the remaining columns for display.\n\n# Select and print all columns except 'original_text' and 'cleaned_text'\ncolumns_to_display = text_df.columns.difference(['original_text', 'cleaned_text'])\nprint(text_df[columns_to_display])\n\n   character_count                                           filename  id  \\\n0            13359  strategy-sustainable-development-lund-universi...   1   \n1             9778          lu-sustainability-communication-01-min.md   2   \n\n   sentence_count  word_count  \n0              83        1848  \n1              80        1073  \n\n\nThis step showcases the metadata and statistical information we’ve gathered, such as the unique ID, filename, word count, character count, and sentence count, without displaying the potentially lengthy text content.\n\nSummary:\n\nos module: Used for interacting with the operating system, listing directory contents, and constructing file paths.\nre module: Provides regular expression matching operations for text cleaning (though in this code, the regex is commented out).\npandas: Used for creating and manipulating the DataFrame to store text data and computed statistics.\nspacy: Provides advanced NLP capabilities; we load a language model to perform sentence segmentation for counting sentences.\napply and lambda functions in pandas: Used to apply functions to DataFrame columns for calculating word counts, character counts, and sentence counts.\n\nThis modular approach allows for easy understanding and maintenance of the code, with each section handling a specific part of the text processing pipeline.",
    "crumbs": [
      "workshop",
      "part 3: text analysis",
      "reading text content"
    ]
  },
  {
    "objectID": "pyws05-0-data-collection.html",
    "href": "pyws05-0-data-collection.html",
    "title": "part 5: data collection",
    "section": "",
    "text": "Web scraping is an essential technique for gathering data from websites, and it can be approached differently depending on whether the target is static or dynamic content. Static web content is straightforward to scrape because the content is directly embedded in the HTML of the webpage, and tools like BeautifulSoup can easily parse this HTML to extract useful information, such as financial data, text, or images. For instance, scraping static content from a financial news site could involve fetching stock prices, headlines, and metadata, all available in the page’s source code. However, dynamic web content presents additional challenges. This type of content is often rendered via JavaScript after the initial page load, making traditional HTML scraping tools ineffective. To handle dynamic content, you need tools like Selenium or Playwright, which can simulate a browser environment, interact with the page, and extract data after it has been dynamically loaded. For example, scraping a site with real-time financial data, such as stock trading platforms or cryptocurrency exchanges, requires navigating dynamic elements like charts and live updates, all of which are powered by JavaScript.",
    "crumbs": [
      "workshop",
      "part 5: data collection"
    ]
  },
  {
    "objectID": "pyws05-0-data-collection.html#web-scraping",
    "href": "pyws05-0-data-collection.html#web-scraping",
    "title": "part 5: data collection",
    "section": "",
    "text": "Web scraping is an essential technique for gathering data from websites, and it can be approached differently depending on whether the target is static or dynamic content. Static web content is straightforward to scrape because the content is directly embedded in the HTML of the webpage, and tools like BeautifulSoup can easily parse this HTML to extract useful information, such as financial data, text, or images. For instance, scraping static content from a financial news site could involve fetching stock prices, headlines, and metadata, all available in the page’s source code. However, dynamic web content presents additional challenges. This type of content is often rendered via JavaScript after the initial page load, making traditional HTML scraping tools ineffective. To handle dynamic content, you need tools like Selenium or Playwright, which can simulate a browser environment, interact with the page, and extract data after it has been dynamically loaded. For example, scraping a site with real-time financial data, such as stock trading platforms or cryptocurrency exchanges, requires navigating dynamic elements like charts and live updates, all of which are powered by JavaScript.",
    "crumbs": [
      "workshop",
      "part 5: data collection"
    ]
  },
  {
    "objectID": "pyws05-0-data-collection.html#web-experiments",
    "href": "pyws05-0-data-collection.html#web-experiments",
    "title": "part 5: data collection",
    "section": "web experiments",
    "text": "web experiments\nBeyond scraping, the rise of platforms like Streamlit and GitHub Codespaces offers powerful possibilities for hosting web experiments that collect user interaction and behavioral data. Streamlit is a Python-based framework that simplifies the creation of interactive web applications, making it easy for researchers to design experiments that capture user input in real-time. For example, researchers in social science could build a survey that adjusts dynamically based on user responses or a task-based experiment where user behavior is logged and analyzed. Streamlit’s simplicity allows for fast deployment of experiments that run directly in the browser, eliminating the need for complex backend infrastructure. On the other hand, GitHub Codespaces provides a full development environment in the cloud, enabling researchers to collaborate on and host interactive experiments. By setting up a Codespace, researchers can deploy real-time applications with persistent storage, making it possible to record user behaviors such as clicks, navigation patterns, and text input during the experiment. The ability to run experiments in the cloud with either platform means data collection can scale easily, and researchers can access a broader pool of participants without requiring them to install software or participate in person. Both platforms offer streamlined ways to collect, store, and analyze behavioral data, which can be particularly useful for conducting social science research in a modern, online setting.",
    "crumbs": [
      "workshop",
      "part 5: data collection"
    ]
  },
  {
    "objectID": "pyws04-0-image-analysis.html",
    "href": "pyws04-0-image-analysis.html",
    "title": "part 4: image analysis",
    "section": "",
    "text": "Handling images in Python involves a variety of intricacies, especially when processing raw visual data to extract meaningful information. Unlike text, which is inherently unstructured, images contain pixel-based data that needs to be interpreted and organized. When reading and processing image content using libraries like OpenCV and Pillow, common challenges include handling different image formats (such as JPEG, PNG, or TIFF), varying image resolutions, and potential distortions like noise or compression artifacts. Additionally, images can come in different color spaces (RGB, grayscale, etc.), and choosing the right one for analysis is crucial depending on the task at hand. For instance, converting an image to grayscale simplifies it by removing color information, making it easier to focus on structural aspects of the image like edges or shapes. Another complexity arises in how to break down the image into meaningful units—whether it’s identifying objects, segmenting regions, or extracting pixel values for quantitative analysis. This process requires a deep understanding of how visual information is encoded and how it can be transformed for different analytical purposes.",
    "crumbs": [
      "workshop",
      "part 4: image analysis"
    ]
  },
  {
    "objectID": "pyws04-0-image-analysis.html#reading-image-content",
    "href": "pyws04-0-image-analysis.html#reading-image-content",
    "title": "part 4: image analysis",
    "section": "",
    "text": "Handling images in Python involves a variety of intricacies, especially when processing raw visual data to extract meaningful information. Unlike text, which is inherently unstructured, images contain pixel-based data that needs to be interpreted and organized. When reading and processing image content using libraries like OpenCV and Pillow, common challenges include handling different image formats (such as JPEG, PNG, or TIFF), varying image resolutions, and potential distortions like noise or compression artifacts. Additionally, images can come in different color spaces (RGB, grayscale, etc.), and choosing the right one for analysis is crucial depending on the task at hand. For instance, converting an image to grayscale simplifies it by removing color information, making it easier to focus on structural aspects of the image like edges or shapes. Another complexity arises in how to break down the image into meaningful units—whether it’s identifying objects, segmenting regions, or extracting pixel values for quantitative analysis. This process requires a deep understanding of how visual information is encoded and how it can be transformed for different analytical purposes.",
    "crumbs": [
      "workshop",
      "part 4: image analysis"
    ]
  },
  {
    "objectID": "pyws04-0-image-analysis.html#object-recognition",
    "href": "pyws04-0-image-analysis.html#object-recognition",
    "title": "part 4: image analysis",
    "section": "object recognition",
    "text": "object recognition\nObject recognition, a more advanced image analysis technique, involves detecting and classifying objects within an image, often using machine learning models. This contrasts with more basic types of image analysis, such as finding contours or corners, which are simpler geometric features. Contour detection in an image focuses on identifying the boundaries or edges of objects, which can be useful for shape analysis, but it doesn’t provide any understanding of the object’s identity or function. Corner detection, on the other hand, finds points in the image where there is a sharp change in direction, such as the corners of a rectangle, which can be helpful in tasks like motion tracking or object detection based on feature points. While both contour and corner detection are essential for breaking down images into simpler shapes or features, they do not attempt to interpret or classify the objects in the scene. Object recognition takes this further by using algorithms or trained models to not only detect an object but also to classify it—whether it’s identifying a car, a person, or another object in the image. This step is critical in fields like behavioral analysis or media studies, where understanding what is in the image, rather than just its shape or structure, is essential for deriving insights from visual data.",
    "crumbs": [
      "workshop",
      "part 4: image analysis"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html",
    "href": "pyws01-2-getting-started.html",
    "title": "basic python syntax",
    "section": "",
    "text": "interactive learnpython\ndownload jupyter notebook",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html#strings-and-numbers",
    "href": "pyws01-2-getting-started.html#strings-and-numbers",
    "title": "basic python syntax",
    "section": "strings and numbers",
    "text": "strings and numbers\nHere are 10 Python code lines with different types of strings and numbers, each followed by an explanation:\n\nCode:\n\n# This is a comment explaining the next line\n\nExplanation:\nThis line starts with a #, making it a comment. Python ignores this line during execution. It’s used to explain code or leave notes for other programmers.\nCode:\n\nprint('Hello, World!')\n\nExplanation:\nThis prints the string 'Hello, World!' to the console. Single quotes enclose the string. In Python, single and double quotes are interchangeable for defining strings.\nCode:\n\nprint(\"Python is fun\")\n\nExplanation:\nHere, double quotes are used to define the string \"Python is fun\". Python treats strings defined with single or double quotes the same way.\nCode:\n\nprint(\"He said, \\\"Python is cool\\\"\")\n\nExplanation:\nThis line prints He said, \"Python is cool\". The backslash \\ before the double quotes escapes them, telling Python to treat them as part of the string instead of ending it.\nCode:\n\nprint(\"Line one\\nLine two\")\n\nExplanation:\nThe \\n is a newline escape character, so this will output:\nLine one\nLine two\nThe \\n tells Python to move to a new line.\nCode:\n\nprint(\"Hello\" + \" \" + \"World\")\n\nExplanation:\nThis line concatenates three strings: \"Hello\", a space (\" \"), and \"World\", resulting in Hello World. The + operator combines strings.\nCode:\n\nprint(5 + 3)\n\nExplanation:\nThis performs an arithmetic operation, adding two integers 5 and 3, resulting in the output 8. Python interprets + as an addition operator when used with numbers.\nCode:\n\nprint(5.0 + 3)\n\nExplanation:\nThis adds a floating-point number 5.0 and an integer 3. Python automatically converts the integer to a float and outputs 8.0, demonstrating Python’s support for mixed-type arithmetic.\nCode:\n\nprint(7 / 2)\n\nExplanation:\nThis division operation between two integers results in 3.5. In Python 3, division with / always results in a float, even when dividing two integers.\nCode:\n\nprint(type(3.14))\n\nExplanation:\nThis uses the type() function, which returns the data type of the value passed to it. Here, 3.14 is a floating-point number, so the output will be &lt;class 'float'&gt;, indicating the value is of type float.\n\nThese examples cover key features of Python strings, numbers, and the type() function, along with how comments and escape characters work.",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html#lists-and-dictionaries",
    "href": "pyws01-2-getting-started.html#lists-and-dictionaries",
    "title": "basic python syntax",
    "section": "lists and dictionaries",
    "text": "lists and dictionaries\nHere are 10 Python code lines illustrating different types of lists and dictionaries, with explanations:\n\nCode:\n\nmy_list = [1, 2, 3, 4]\n\nExplanation:\nThis creates a list called my_list containing four integer elements: [1, 2, 3, 4]. Lists are ordered and mutable collections in Python, allowing for element addition, removal, and modification. Each element can be accessed by its index, starting from 0.\nCode:\n\nmy_dict = {'name': 'Alice', 'age': 30}\n\nExplanation:\nThis creates a dictionary my_dict with two key-value pairs: 'name': 'Alice' and 'age': 30. Dictionaries are unordered collections that map keys to values, and values can be accessed using the keys.\nCode:\n\nnested_list = [[1, 2], [3, 4], [5, 6]]\n\nExplanation:\nThis creates a 2D list nested_list, where each element is another list. Accessing elements can be done using two indices, such as nested_list[0][1] to get the value 2.\nCode:\n\nnested_dict = {'person1': {'name': 'Alice', 'age': 30}, 'person2': {'name': 'Bob', 'age': 25}}\n\nExplanation:\nThis is a 2D dictionary, where each key ('person1', 'person2') maps to another dictionary. For example, you can access Alice’s age by using nested_dict['person1']['age'], which returns 30.\nCode:\n\nmy_list.append(5)\n\nExplanation:\nThis appends the value 5 to the end of my_list. The append() method is a built-in function for adding elements to a list, modifying it in place.\nCode:\n\nlast_item = my_list.pop()\n\nExplanation:\nThis removes and returns the last element from my_list using the pop() method. If my_list = [1, 2, 3, 4, 5], after popping, my_list becomes [1, 2, 3, 4] and last_item is assigned the value 5.\nCode:\n\nsecond_item = my_list[1]\n\nExplanation:\nThis accesses the second element of my_list using the index 1 (Python uses 0-based indexing). For example, if my_list = [1, 2, 3, 4], second_item will be 2.\nCode:\n\nmy_dict['city'] = 'New York'\n\nExplanation:\nThis adds a new key-value pair 'city': 'New York' to my_dict. Dictionaries allow dynamic insertion of key-value pairs. If my_dict already contains 'city', this will update its value.\nCode:\n\nremoved_value = my_dict.pop('age')\n\nExplanation:\nThis removes the key 'age' from my_dict and returns its value (30 in this case). The pop() method removes the specified key-value pair and modifies the dictionary.\nCode:\n\nprint(type(my_list))\n\nExplanation:\nThis uses the type() function to check the data type of my_list. The output will be &lt;class 'list'&gt;, indicating that my_list is a list. Similarly, calling type(my_dict) would return &lt;class 'dict'&gt;, showing that my_dict is a dictionary.\n\nThese examples illustrate key operations with lists and dictionaries, including element access, appending, popping, and the use of the type() function to check data types.",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html#loops-and-conditionals",
    "href": "pyws01-2-getting-started.html#loops-and-conditionals",
    "title": "basic python syntax",
    "section": "loops and conditionals",
    "text": "loops and conditionals\nHere are 10 Python code chunks demonstrating different types of loops and conditionals, with explanations:\n\nCode:\n\nfor i in range(5):\n    print(i)\n\nExplanation:\nThis is a basic for loop that iterates over the range 0 to 4 (Python ranges are zero-indexed and exclusive of the stop value). It prints each value of i in the loop: 0, 1, 2, 3, 4.\nCode:\n\nwhile True:\n    print(\"Looping...\")\n    break\n\nExplanation:\nThis is a while loop with a True condition, which would normally create an infinite loop. However, the break statement exits the loop after the first iteration. Without break, it would continuously print \"Looping...\".\nCode:\n\nif 10 &gt; 5:\n    print(\"10 is greater than 5\")\nelse:\n    print(\"5 is greater than or equal to 10\")\n\nExplanation:\nThis is an if-else statement. The condition 10 &gt; 5 evaluates to True, so the first block is executed, printing \"10 is greater than 5\". The else block would run if the condition were False.\nCode:\n\nnumber = 7\nif number % 2 == 0:\n    print(\"Even\")\nelse:\n    print(\"Odd\")\n\nExplanation:\nThis checks if the variable number is even or odd using the modulo operator (%). If number % 2 == 0, it prints \"Even\", otherwise, it prints \"Odd\". In this case, it prints \"Odd\" because 7 is not divisible by 2.\nCode:\n\nfor x in range(10):\n    if x % 2 == 0:\n        continue\n    print(x)\n\nExplanation:\nThis for loop prints all odd numbers from 0 to 9. The continue statement skips the rest of the loop when x is even, so only odd values (1, 3, 5, 7, 9) are printed.\nCode:\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor index, fruit in enumerate(fruits):\n    print(f\"Fruit {index}: {fruit}\")\n\nExplanation:\nThe enumerate() function provides both the index and value of each element in the fruits list. The loop iterates over the list, printing each fruit along with its index:\nFruit 0: apple\nFruit 1: banana\nFruit 2: cherry\nCode:\n\nis_raining = True\nis_sunny = False\nif is_raining and not is_sunny:\n    print(\"It's raining but not sunny\")\nelif is_sunny and not is_raining:\n    print(\"It's sunny but not raining\")\nelse:\n    print(\"It's either both raining and sunny or neither\")\n\nExplanation:\nThis demonstrates a compound conditional using Boolean variables. Since is_raining is True and is_sunny is False, the first block is executed, printing \"It's raining but not sunny\".\nCode:\n\neven_numbers = [x for x in range(10) if x % 2 == 0]\nprint(even_numbers)\n\nExplanation:\nThis is a list comprehension that creates a list of even numbers from 0 to 9. It iterates over the range of numbers and only includes those where x % 2 == 0. The output is [0, 2, 4, 6, 8].\nCode:\n\ntry:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero\")\n\nExplanation:\nThis is an example of a try-except block for exception handling. The try block contains code that could raise an exception (division by zero), and the except block catches the ZeroDivisionError and prints \"Cannot divide by zero\". Without the exception handling, the program would crash.\nCode:\n\nfor i in range(5):\n   try:\n       print(10 / i)\n   except ZeroDivisionError:\n       print(\"Division by zero is not allowed\")\n\nExplanation:\nThis loop attempts to divide 10 by i for values from 0 to 4. When i is 0, a ZeroDivisionError occurs, which is caught by the except block, printing \"Division by zero is not allowed\". For other values of i, the result of the division is printed.\n\nThese examples cover various types of loops, conditionals, list comprehensions, Boolean logic, and exception handling in Python.",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html#user-defined-functions",
    "href": "pyws01-2-getting-started.html#user-defined-functions",
    "title": "basic python syntax",
    "section": "user defined functions",
    "text": "user defined functions\nHere are 10 Python code chunks demonstrating different types of user-defined functions, with explanations:\n\nCode:\n\ndef greet():\n    \"\"\"This function prints a simple greeting message.\"\"\"\n    print(\"Hello, welcome!\")\ngreet()\n\nExplanation:\nThis is a simple function greet() that takes no arguments and prints a greeting message. It is called using greet(). The triple quotes \"\"\" define a docstring, which serves as the function’s documentation. When called, it prints \"Hello, welcome!\".\nCode:\n\ndef greet_person(name):\n    \"\"\"This function greets a person by name.\"\"\"\n    print(f\"Hello, {name}!\")\ngreet_person(\"Alice\")\n\nExplanation:\nThis function greet_person(name) accepts a single argument, name, and prints a personalized greeting. When you call greet_person(\"Alice\"), it prints \"Hello, Alice!\". The docstring explains what the function does.\nCode:\n\ndef add_numbers(a, b):\n    \"\"\"Returns the sum of two numbers.\"\"\"\n    return a + b\nresult = add_numbers(5, 3)\nprint(result)\n\nExplanation:\nadd_numbers(a, b) is a function that takes two arguments, a and b, and returns their sum. In this case, add_numbers(5, 3) returns 8, which is printed. The return keyword is used to send the result back to the calling code.\nCode:\n\ndef multiply(a, b=2):\n    \"\"\"Multiplies two numbers, with the second number having a default value of 2.\"\"\"\n    return a * b\nprint(multiply(4))  # uses default value for b\nprint(multiply(4, 3))  # overrides default value for b\n\nExplanation:\nThis function multiply(a, b=2) takes two arguments but assigns a default value of 2 to b. If only one argument is passed, the function uses the default value. Calling multiply(4) returns 8, while multiply(4, 3) returns 12.\nCode:\n\ndef divide(a, b):\n    \"\"\"Divides a by b and handles division by zero.\"\"\"\n    if b == 0:\n        return \"Cannot divide by zero!\"\n    return a / b\nprint(divide(10, 2))\nprint(divide(10, 0))\n\nExplanation:\ndivide(a, b) takes two arguments and returns the result of dividing a by b. It includes a conditional to check for division by zero. If b is 0, it returns an error message. Calling divide(10, 2) returns 5.0, while divide(10, 0) returns \"Cannot divide by zero!\".\nCode:\n\ndef square_elements(numbers):\n    \"\"\"Takes a list of numbers and returns a list of their squares.\"\"\"\n    return [x ** 2 for x in numbers]\nprint(square_elements([1, 2, 3, 4]))\n\nExplanation:\nThis function square_elements(numbers) takes a list of numbers and returns a new list containing the squares of those numbers. The function uses list comprehension. Calling square_elements([1, 2, 3, 4]) returns [1, 4, 9, 16].\nCode:\n\ndef factorial(n):\n    \"\"\"Recursively calculates the factorial of n.\"\"\"\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\nprint(factorial(5))\n\nExplanation:\nThis function factorial(n) uses recursion to calculate the factorial of a number. If n is 0, it returns 1 (base case). Otherwise, it multiplies n by factorial(n - 1). Calling factorial(5) returns 120.\nCode:\n\ndef is_even(number):\n    \"\"\"Checks if a number is even.\"\"\"\n    return number % 2 == 0\nprint(is_even(4))  # True\nprint(is_even(7))  # False\n\nExplanation:\nThe function is_even(number) checks if a number is even by using the modulo operator (%). If the remainder is 0, it returns True, otherwise False. Calling is_even(4) returns True, and is_even(7) returns False.\nCode:\n\ndef describe_person(name, age, *hobbies):\n    \"\"\"Takes a name, age, and any number of hobbies, and prints a description.\"\"\"\n    print(f\"{name} is {age} years old and enjoys {', '.join(hobbies)}.\")\ndescribe_person(\"Alice\", 30, \"reading\", \"hiking\", \"cooking\")\n\nExplanation:\nThis function describe_person(name, age, *hobbies) accepts a variable number of hobby arguments using the * syntax, which collects extra arguments into a tuple. The join() method creates a string from the hobbies. Calling describe_person(\"Alice\", 30, \"reading\", \"hiking\", \"cooking\") prints \"Alice is 30 years old and enjoys reading, hiking, cooking.\"\nCode:\n\ndef calculate_average(*numbers):\n    \"\"\"Calculates the average of any number of values.\"\"\"\n    if len(numbers) == 0:\n        return 0\n    return sum(numbers) / len(numbers)\nprint(calculate_average(5, 10, 15))\nprint(calculate_average())\n\nExplanation:\nThe calculate_average(*numbers) function calculates the average of any number of arguments. It first checks if any numbers were provided (if the length of numbers is 0, it returns 0), then calculates the average by dividing the sum by the length. Calling calculate_average(5, 10, 15) returns 10.0, and calculate_average() returns 0.\n\nThese examples show different ways to define functions with varying arguments, handling edge cases, using recursion, and incorporating function documentation.",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "pyws01-2-getting-started.html#modules-main-statement",
    "href": "pyws01-2-getting-started.html#modules-main-statement",
    "title": "basic python syntax",
    "section": "modules, main statement",
    "text": "modules, main statement\nHere is a sample Python script that defines two simple functions and includes a main statement to call them:\n\n# Function to add two numbers\ndef add_numbers(a, b):\n    return a + b\n\n# Function to subtract two numbers\ndef subtract_numbers(a, b):\n    return a - b\n\n# Main statement\nif __name__ == \"__main__\":\n    num1 = 10\n    num2 = 5\n\n    # Calling the functions\n    sum_result = add_numbers(num1, num2)\n    diff_result = subtract_numbers(num1, num2)\n\n    # Printing the results\n    print(f\"The sum of {num1} and {num2} is: {sum_result}\")\n    print(f\"The difference between {num1} and {num2} is: {diff_result}\")\n\nExplanation:\n\nadd_numbers(a, b): A simple function that takes two arguments and returns their sum.\nsubtract_numbers(a, b): A simple function that takes two arguments and returns their difference.\nMain statement (if __name__ == \"__main__\":): This block ensures that the code inside it runs only when the script is executed directly, not when it’s imported as a module.\nCalling the functions: Inside the main block, it calls the add_numbers and subtract_numbers functions with num1 and num2 as arguments.\nPrint results: Displays the results of the addition and subtraction operations.",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "basic python syntax"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The following info is fetched from timeedit:",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "about.html#workshop-schedule",
    "href": "about.html#workshop-schedule",
    "title": "About",
    "section": "",
    "text": "The following info is fetched from timeedit:",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "about.html#workshop-map",
    "href": "about.html#workshop-map",
    "title": "About",
    "section": "Workshop map",
    "text": "Workshop map\n\n\n\nworkshop locations",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "pyws02-2-data-analysis.html",
    "href": "pyws02-2-data-analysis.html",
    "title": "result visualizations",
    "section": "",
    "text": "example gallery matplotlib\nexample gallery seaborn\nexample gallery plotnine\nexample gallery plotly\nexample gallery altair\ndownload jupyter notebook\n# run inside google colab\n#!git clone https://github.com/cca-cce/osm-cda.git",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "result visualizations"
    ]
  },
  {
    "objectID": "pyws02-2-data-analysis.html#seaborn-descriptive",
    "href": "pyws02-2-data-analysis.html#seaborn-descriptive",
    "title": "result visualizations",
    "section": "seaborn, descriptive",
    "text": "seaborn, descriptive\n\n# Import necessary libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nIn this code chunk, we import the essential Python libraries needed for data manipulation and visualization:\n\npandas is used for handling data structures like DataFrames and provides functions for data manipulation.\nseaborn is a statistical data visualization library built on top of matplotlib, offering a high-level interface for drawing attractive graphs.\nmatplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB, used here as plt for plotting.\n\n\n\n# Load the penguins dataset\npenguins = sns.load_dataset('penguins')\n\nHere, we load the ‘penguins’ dataset using seaborn’s load_dataset() function and store it in a pandas DataFrame named penguins. This dataset contains measurements for different penguin species, including features like bill length, flipper length, body mass, and more.\n\n\n# Set the output file path\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/fig/'\n\nThis line sets the directory path where the generated plots will be saved. The variable output_file_path holds the string representing the file path to ensure all saved figures are organized in the specified location.\n\n\n# First plot: Graph two categorical variables\n# Plot the count of penguins by species and island\nplt.figure(figsize=(8, 6))\nsns.countplot(x='species', hue='island', data=penguins)\nplt.title('Count of Penguin Species by Island')\nplt.xlabel('Species')\nplt.ylabel('Count')\nplt.legend(title='Island')\nplt.tight_layout()\nplt.savefig(f'{output_file_path}penguins_species_island.png')\nplt.savefig(f'{output_file_path}penguins_species_island.pdf')\nplt.close()\n\nIn this code chunk, we create a bar plot to visualize the relationship between two categorical variables, species and island:\n\nplt.figure(figsize=(8, 6)) initializes a new figure with a specified size.\nsns.countplot() generates a count plot showing the number of penguins for each species, differentiated by island using the hue parameter.\nplt.title(), plt.xlabel(), and plt.ylabel() set the plot’s title and axis labels.\nplt.legend(title='Island') adds a legend with the title ‘Island’ to differentiate categories.\nplt.tight_layout() adjusts the layout to prevent clipping of labels.\nplt.savefig() saves the figure in both PNG and PDF formats to the specified output path.\nplt.close() closes the current figure to free up memory.\n\n\n\n# Second plot: Graph one continuous variable by one categorical using simple barplot without error bars\n# Plot the average body mass by species\nplt.figure(figsize=(8, 6))\nsns.barplot(x='species', y='body_mass_g', data=penguins, ci=None)\nplt.title('Average Body Mass by Species')\nplt.xlabel('Species')\nplt.ylabel('Body Mass (g)')\nplt.tight_layout()\nplt.savefig(f'{output_file_path}penguins_body_mass_species.png')\nplt.savefig(f'{output_file_path}penguins_body_mass_species.pdf')\nplt.close()\n\nThis chunk generates a bar plot to visualize a continuous variable (body_mass_g) against a categorical variable (species):\n\nsns.barplot() creates a bar plot showing the average body mass for each species. Setting ci=None removes the error bars (confidence intervals) to display simple bars.\nThe plt functions add a title and axis labels to the plot.\nThe plot is saved as both PNG and PDF files in the specified directory using plt.savefig().\nplt.close() closes the figure to free memory.\n\n\n\n# Third plot: Graph two continuous variables\n# Plot flipper length vs. body mass\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='flipper_length_mm', y='body_mass_g', data=penguins)\nplt.title('Flipper Length vs. Body Mass')\nplt.xlabel('Flipper Length (mm)')\nplt.ylabel('Body Mass (g)')\nplt.tight_layout()\nplt.savefig(f'{output_file_path}penguins_flipper_length_body_mass.png')\nplt.savefig(f'{output_file_path}penguins_flipper_length_body_mass.pdf')\nplt.close()\n\nIn the final code chunk, we create a scatter plot to examine the relationship between two continuous variables, flipper_length_mm and body_mass_g:\n\nsns.scatterplot() plots data points representing each penguin’s flipper length versus body mass.\nTitles and axis labels are added using plt.title(), plt.xlabel(), and plt.ylabel().\nThe plot is saved in both PNG and PDF formats using plt.savefig().\nplt.close() closes the figure to ensure that subsequent plots are not affected by the current figure.\n\n\n\nseaborn, subplots\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load the penguins dataset into a DataFrame\ndf = sns.load_dataset('penguins')\n\n# Create a 1 row by 3 columns plot\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Left subplot: Graph two categorical variables (species and island) using a countplot\nsns.countplot(data=df, x='species', hue='island', ax=axes[0])\naxes[0].set_title('Count of Species by Island')\n\n# Middle subplot: Graph one continuous variable by one categorical using a barplot (without error bars)\nsns.barplot(data=df, x='species', y='body_mass_g', errorbar=None, ax=axes[1])\naxes[1].set_title('Average Body Mass by Species')\n\n# Right subplot: Graph two continuous variables (flipper_length_mm vs. body_mass_g) using a scatterplot\nsns.scatterplot(data=df, x='flipper_length_mm', y='body_mass_g', ax=axes[2])\naxes[2].set_title('Flipper Length vs. Body Mass')\n\n# Adjust layout\nplt.tight_layout()\n\n# Output directory\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/fig/'\n\n# Save the figure as PNG and PDF\nfig.savefig(f\"{output_file_path}seaborn_descriptive.png\")\nfig.savefig(f\"{output_file_path}seaborn_descriptive.pdf\")\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\nHere is the analysis of the Python code, broken down into separate chunks, with explanations for each step:\n\n\n1. Import Libraries\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nExplanation:\nThis step imports the necessary libraries. seaborn is used for data visualization, matplotlib.pyplot for creating plots, and pandas for data manipulation. These libraries provide powerful tools for analyzing and visualizing data in a variety of formats.\n\n\n\n2. Load the Dataset\n\ndf = sns.load_dataset('penguins')\n\nExplanation:\nThis line loads the “penguins” dataset into a Pandas DataFrame (df) using Seaborn’s load_dataset function. The dataset contains information about penguin species, measurements, and other attributes. By loading it into a DataFrame, you make it easier to perform data analysis and visualizations.\n\n\n\n3. Create the Plot Figure and Axes\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nExplanation:\nThis step creates a figure (fig) with three subplots (axes) arranged in a single row using plt.subplots(). The figsize=(18, 5) argument specifies the size of the entire plot in inches. This layout allows for multiple visualizations to be displayed side-by-side in one figure.\n\n\n\n4. Create the Left Subplot\n\nsns.countplot(data=df, x='species', hue='island', ax=axes[0])\naxes[0].set_title('Count of Species by Island')\n\nText(0.5, 1.0, 'Count of Species by Island')\n\n\nExplanation:\nThis block creates a count plot using Seaborn’s countplot() function, displaying the number of penguins by species with a hue for the island column. The plot is assigned to the first subplot (axes[0]). The set_title() method adds a title to this subplot, making it easier to interpret.\n\n\n\n5. Create the Middle Subplot\n\nsns.barplot(data=df, x='species', y='body_mass_g', errorbar=None, ax=axes[1])\naxes[1].set_title('Average Body Mass by Species')\n\nText(0.5, 1.0, 'Average Body Mass by Species')\n\n\nExplanation:\nThis block generates a bar plot that shows the average body mass (body_mass_g) for each penguin species using sns.barplot(). The errorbar=None argument ensures that no error bars are displayed. The plot is assigned to the second subplot (axes[1]), and a title is set for clarity.\n\n\n\n6. Create the Right Subplot\n\nsns.scatterplot(data=df, x='flipper_length_mm', y='body_mass_g', ax=axes[2])\naxes[2].set_title('Flipper Length vs. Body Mass')\n\nText(0.5, 1.0, 'Flipper Length vs. Body Mass')\n\n\nExplanation:\nHere, a scatter plot is created using sns.scatterplot() to show the relationship between flipper_length_mm (x-axis) and body_mass_g (y-axis). This plot is added to the third subplot (axes[2]). The title provides context for what the plot represents.\n\n\n\n7. Adjust the Layout\n\nplt.tight_layout()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nExplanation:\nThis line adjusts the layout of the subplots to ensure they do not overlap. plt.tight_layout() automatically adjusts the spacing between subplots to make the figure look cleaner and more readable.\n\n\n\n8. Define the Output Directory and Save the Figure\n\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/fig/'\n\nfig.savefig(f\"{output_file_path}seaborn_descriptive.png\")\nfig.savefig(f\"{output_file_path}seaborn_descriptive.pdf\")\n\nExplanation:\nThe output_file_path variable defines the directory where the plots will be saved. The fig.savefig() method saves the entire figure in both PNG and PDF formats to the specified directory. This allows you to keep a copy of the visualizations for future reference or inclusion in reports.\n\n\n\n9. Display the Plot\n\nplt.show()\n\nExplanation:\nThis line displays the figure with all three subplots in an interactive window (or inline if using a Jupyter notebook). This command is necessary to visualize the plots when running the script in an environment that supports graphical output.",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "result visualizations"
    ]
  },
  {
    "objectID": "pyws02-2-data-analysis.html#seaborn-inferential",
    "href": "pyws02-2-data-analysis.html#seaborn-inferential",
    "title": "result visualizations",
    "section": "seaborn, inferential",
    "text": "seaborn, inferential\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Read the penguins dataset into a pandas DataFrame\ndf = sns.load_dataset('penguins')\n\n# Create a color palette for species\nspecies_list = df['species'].dropna().unique()\npalette = sns.color_palette('Set1', n_colors=len(species_list))\nspecies_palette = dict(zip(species_list, palette))\n\n# Create a 1 row by 3 columns plot\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Left subplot: Barplot of two categorical variables with error bars (species by island)\nsns.barplot(data=df, x='species', y='body_mass_g', hue='island', errorbar='sd', palette=palette, ax=axes[0])\naxes[0].set_title('Body Mass by Species and Island')\n\n# Middle subplot: Boxplot of one continuous variable by one categorical (body_mass_g by species)\nsns.boxplot(data=df, x='species', y='body_mass_g', hue='species', palette=palette, ax=axes[1], legend=False)\naxes[1].set_title('Body Mass Distribution by Species')\n\n# Right subplot: Scatterplot with linear regression lines and confidence intervals differentiated by species\nsns.scatterplot(data=df, x='flipper_length_mm', y='body_mass_g', hue='species', palette=species_palette, ax=axes[2])\n\n# Add linear regression lines for each species\nfor species, color in species_palette.items():\n    sns.regplot(\n        data=df[df['species'] == species],\n        x='flipper_length_mm',\n        y='body_mass_g',\n        ax=axes[2],\n        scatter=False,\n        label=species,\n        ci=95,\n        line_kws={'color': color},\n    )\n\naxes[2].legend()\naxes[2].set_title('Flipper Length vs. Body Mass by Species (Linear Regression)')\n\n# Adjust layout\nplt.tight_layout()\n\n# Save the figure as PNG and PDF\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/fig/'\nfig.savefig(f\"{output_file_path}seaborn_inferential.png\")\nfig.savefig(f\"{output_file_path}seaborn_inferential.pdf\")\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\nHere is an analysis of the Python code, broken down into separate chunks, followed by explanations for each step:\n\n1. Import Libraries\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nExplanation:\nThis step imports the necessary libraries: seaborn for statistical data visualization, matplotlib.pyplot for plotting, and pandas for data manipulation. Importing these libraries allows access to their functions, which are used for loading datasets, creating plots, and saving figures.\n\n\n\n2. Load the Penguins Dataset\n\ndf = sns.load_dataset('penguins')\n\nExplanation:\nThis line loads the “penguins” dataset into a Pandas DataFrame (df) using the Seaborn library’s load_dataset function. The dataset contains information on penguin species, measurements (e.g., body mass, flipper length), and habitat. Loading the data into a DataFrame enables easier manipulation and visualization of the dataset.\n\n\n\n3. Create a Color Palette for Species\n\nspecies_list = df['species'].dropna().unique()\npalette = sns.color_palette('Set1', n_colors=len(species_list))\nspecies_palette = dict(zip(species_list, palette))\n\nExplanation:\nThis chunk creates a color palette to differentiate penguin species in the plots: 1. species_list extracts unique species names from the species column, excluding any missing values. 2. palette generates a color palette using Seaborn’s Set1 palette with a number of colors equal to the number of species. 3. species_palette creates a dictionary mapping each species to a specific color, enabling consistent coloring across multiple plots.\n\n\n\n4. Create a 1x3 Plot Grid\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nExplanation:\nThis line creates a figure with a grid of subplots arranged in one row and three columns using matplotlib. The figsize parameter sets the overall dimensions of the figure to 18 inches in width and 5 inches in height. The returned axes array will be used to plot the individual subplots.\n\n\n\n5. Create the Bar Plot (Left Subplot)\n\nsns.barplot(data=df, x='species', y='body_mass_g', hue='island', errorbar='sd', palette=palette, ax=axes[0])\naxes[0].set_title('Body Mass by Species and Island')\n\nText(0.5, 1.0, 'Body Mass by Species and Island')\n\n\nExplanation:\nThis block creates a bar plot in the leftmost subplot (axes[0]): - Uses sns.barplot() to plot body_mass_g (y-axis) for each species (x-axis), separated by island (hue). - The errorbar='sd' parameter adds standard deviation error bars to each bar. - palette specifies the color scheme, while ax=axes[0] assigns the plot to the first subplot. - The set_title() method sets the title of the subplot.\n\n\n\n6. Create the Box Plot (Middle Subplot)\n\nsns.boxplot(data=df, x='species', y='body_mass_g', hue='species', palette=palette, ax=axes[1], legend=False)\naxes[1].set_title('Body Mass Distribution by Species')\n\nText(0.5, 1.0, 'Body Mass Distribution by Species')\n\n\nExplanation:\nThis block creates a box plot in the middle subplot (axes[1]): - sns.boxplot() displays the distribution of body_mass_g for each species. - The hue='species' parameter assigns different colors to each species, using the same palette. - legend=False suppresses the legend for this plot to avoid redundancy. - The plot is assigned to the second subplot (ax=axes[1]), and set_title() adds a title to the plot.\n\n\n\n7. Create the Scatter Plot with Regression Lines (Right Subplot)\n\nsns.scatterplot(data=df, x='flipper_length_mm', y='body_mass_g', hue='species', palette=species_palette, ax=axes[2])\n\n&lt;Axes: title={'center': 'Flipper Length vs. Body Mass by Species (Linear Regression)'}, xlabel='flipper_length_mm', ylabel='body_mass_g'&gt;\n\n\nExplanation:\nThis line creates a scatter plot in the rightmost subplot (axes[2]): - sns.scatterplot() plots flipper_length_mm (x-axis) against body_mass_g (y-axis) for each species. - The hue='species' parameter color-codes the scatter points according to species using the previously defined species_palette. - ax=axes[2] places this plot in the third subplot.\n\n\n\n8. Add Linear Regression Lines for Each Species\n\nfor species, color in species_palette.items():\n    sns.regplot(\n        data=df[df['species'] == species],\n        x='flipper_length_mm',\n        y='body_mass_g',\n        ax=axes[2],\n        scatter=False,\n        label=species,\n        ci=95,\n        line_kws={'color': color},\n    )\n\nExplanation:\nThis block adds linear regression lines to the scatter plot for each species: - Iterates through the species_palette dictionary to plot a separate regression line for each species. - sns.regplot() fits a linear regression line with confidence intervals (ci=95), using scatter=False to suppress additional scatter points. - line_kws={'color': color} sets the color of the line to match the scatter plot dots. - Each line is labeled with the species name for legend purposes.\n\n\n\n9. Add Legend and Title for the Right Subplot\n\naxes[2].legend()\naxes[2].set_title('Flipper Length vs. Body Mass by Species (Linear Regression)')\n\nText(0.5, 1.0, 'Flipper Length vs. Body Mass by Species (Linear Regression)')\n\n\nExplanation:\nThis block adds a legend to the rightmost subplot (axes[2]) to indicate which color corresponds to each species. It also sets the title of the subplot for better context.\n\n\n\n10. Adjust Layout\n\nplt.tight_layout()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nExplanation:\nThis line adjusts the spacing between subplots to prevent overlap and ensure a clean, organized layout. plt.tight_layout() automatically manages the subplot parameters for a visually appealing output.\n\n\n\n11. Save the Figure\n\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/fig/'\nfig.savefig(f\"{output_file_path}penguins_inferential_statistics_linear.png\")\nfig.savefig(f\"{output_file_path}penguins_inferential_statistics_linear.pdf\")\n\nExplanation:\nThis block saves the figure in both PNG and PDF formats to the specified directory (output_file_path). The fig.savefig() method captures the entire figure, including all subplots, and writes it to the desired file format.\n\n\n\n12. Display the Plot\n\nplt.show()\n\nExplanation:\nThis line displays the generated figure with all three subplots. It is especially useful when running the script in an interactive environment (e.g., Jupyter notebooks) to visualize the plot output.",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "result visualizations"
    ]
  },
  {
    "objectID": "pyws02-2-data-analysis.html#plotnine-inferential",
    "href": "pyws02-2-data-analysis.html#plotnine-inferential",
    "title": "result visualizations",
    "section": "plotnine, inferential",
    "text": "plotnine, inferential\n\n# Import necessary libraries\nimport polars as pl\nfrom plotnine import *\nimport seaborn as sns\n\nIn this code chunk, we import the required Python libraries:\n\npolars as pl: Polars is a high-performance DataFrame library for data manipulation, similar to pandas but optimized for speed.\nplotnine: A grammar of graphics plotting library for Python, inspired by ggplot2 in R. We import all functions and classes from plotnine for plotting.\nseaborn as sns: We use seaborn to load the built-in ‘penguins’ dataset into a pandas DataFrame.\n\n\n\n# Load the penguins dataset into a polars DataFrame\npenguins_pd = sns.load_dataset('penguins')\npenguins = pl.from_pandas(penguins_pd)\n\nHere, we load the ‘penguins’ dataset using seaborn’s load_dataset() function, which returns a pandas DataFrame (penguins_pd). We then convert this pandas DataFrame into a polars DataFrame (penguins) using pl.from_pandas(). This allows us to leverage polars’ efficient data manipulation capabilities.\n\n\n# Set the output file path\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/fig/'\n\nThis line sets the directory path where the generated plots will be saved. The variable output_file_path holds the string representing the file path to ensure all saved figures are organized in the specified location.\n\n\n# Convert polars DataFrame back to pandas for plotting with plotnine\npenguins_df = penguins.to_pandas()\n\nSince plotnine works with pandas DataFrames, we convert the polars DataFrame back to a pandas DataFrame using the to_pandas() method, storing it in penguins_df. This allows us to use plotnine for creating the plots.\n\n\n# First plot: Graph two categorical variables as barplots with error bars\n# Calculate counts and proportions\ncounts = penguins_df.groupby(['species', 'island']).size().reset_index(name='count')\ntotal_counts = counts.groupby('species')['count'].transform('sum')\ncounts['proportion'] = counts['count'] / total_counts\ncounts['se'] = (counts['proportion'] * (1 - counts['proportion']) / total_counts) ** 0.5\n\nIn this code chunk, we prepare the data for the first plot:\n\nWe group the data by ‘species’ and ‘island’ and count the number of observations in each group using groupby() and size().\nWe reset the index to turn the grouped data into a DataFrame with reset_index(), naming the count column as ‘count’.\nWe calculate the total counts for each species using groupby() and transform('sum').\nWe compute the proportion of each island within each species group.\nWe calculate the standard error (se) for the proportions using the formula for the standard error of a proportion.\n\n\n\n# Create the bar plot with error bars\nplot1 = (\n    ggplot(counts, aes(x='species', y='proportion', fill='island')) +\n    geom_bar(stat='identity', position='dodge') +\n    geom_errorbar(aes(ymin='proportion - se', ymax='proportion + se'),\n                  position=position_dodge(0.9), width=0.25) +\n    labs(title='Proportion of Penguins by Species and Island',\n         x='Species', y='Proportion') +\n    theme_minimal()\n)\n\nHere, we create the first plot using plotnine:\n\nWe initialize a ggplot object with counts DataFrame and specify the aesthetics (aes), mapping ‘species’ to the x-axis, ‘proportion’ to the y-axis, and ‘island’ to the fill color.\ngeom_bar(stat='identity', position='dodge') creates a bar plot using the actual values of ‘proportion’ and positions the bars side by side for each species.\ngeom_errorbar() adds error bars to the bars, representing the confidence intervals calculated earlier.\nlabs() sets the title and axis labels of the plot.\ntheme_minimal() applies a minimal theme to the plot for a clean look.\n\n\n\n# Save the first plot\nplot1.save(filename=f'{output_file_path}penguins_species_island_barplot.png', dpi=300)\nplot1.save(filename=f'{output_file_path}penguins_species_island_barplot.pdf', dpi=300)\n\nThis code saves the first plot in both PNG and PDF formats to the specified output directory, with a resolution of 300 dots per inch (dpi).\n\n\n# Second plot: Graph one continuous variable by one categorical using boxplot\nplot2 = (\n    ggplot(penguins_df, aes(x='species', y='body_mass_g')) +\n    geom_boxplot() +\n    labs(title='Body Mass by Species',\n         x='Species', y='Body Mass (g)') +\n    theme_minimal()\n)\n\nIn the second plot, we create a boxplot to visualize the distribution of the continuous variable ‘body_mass_g’ across different categories of ‘species’:\n\nWe initialize the ggplot object with penguins_df and specify the aesthetics.\ngeom_boxplot() creates a boxplot for each species, showing the median, quartiles, and potential outliers.\nlabs() sets the plot title and axis labels.\ntheme_minimal() applies a minimal theme for aesthetics.\n\n\n\n# Save the second plot\nplot2.save(filename=f'{output_file_path}penguins_body_mass_species_boxplot.png', dpi=300)\nplot2.save(filename=f'{output_file_path}penguins_body_mass_species_boxplot.pdf', dpi=300)\n\nThis code saves the second plot in both PNG and PDF formats to the specified directory, ensuring high-quality images for reports or presentations.\n\n\n# Third plot: Graph two continuous variables with linear regression line and confidence intervals\nplot3 = (\n    ggplot(penguins_df, aes(x='flipper_length_mm', y='body_mass_g')) +\n    geom_point() +\n    geom_smooth(method='lm') +\n    labs(title='Flipper Length vs. Body Mass with Regression Line',\n         x='Flipper Length (mm)', y='Body Mass (g)') +\n    theme_minimal()\n)\n\nFor the third plot, we create a scatter plot of two continuous variables with a linear regression line:\n\ngeom_point() plots individual data points of ‘flipper_length_mm’ vs. ‘body_mass_g’.\ngeom_smooth(method='lm') adds a linear regression line with confidence intervals (shaded area), indicating the relationship between the two variables.\nlabs() and theme_minimal() are used to set titles and apply a clean theme.\n\n\n\n# Save the third plot\nplot3.save(filename=f'{output_file_path}penguins_flipper_length_body_mass_regression.png', dpi=300)\nplot3.save(filename=f'{output_file_path}penguins_flipper_length_body_mass_regression.pdf', dpi=300)\n\nFinally, we save the third plot in both PNG and PDF formats to the output directory, completing the data visualization tasks.\n\nIn each code chunk, we’ve carefully explained the purpose and functionality of the Python commands used, ensuring clarity and understanding of the data manipulation and plotting processes.",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "result visualizations"
    ]
  },
  {
    "objectID": "pyws02-2-data-analysis.html#plotnine-testing",
    "href": "pyws02-2-data-analysis.html#plotnine-testing",
    "title": "result visualizations",
    "section": "plotnine, testing",
    "text": "plotnine, testing\n\n# prompt: acquire dataset penguins from seaborn package, read it into a polars dataframe, generate a data analysis pipeline where bill_length is aggregated by species, pipe the aggregated data out to a plotnine bar chart with error bars\n\n#!pip install polars plotnine\n\nimport seaborn as sns\nimport polars as pl\nfrom plotnine import *\n\n# Load the penguins dataset\npenguins = sns.load_dataset('penguins')\n\n# Convert to a Polars DataFrame\ndf = pl.from_pandas(penguins)\n\n# Define the data analysis pipeline\n(\n    df\n    .group_by('species')\n    .agg([\n        pl.col('bill_length_mm').mean().alias('mean_bill_length'),\n        pl.col('bill_length_mm').std().alias('std_bill_length')\n    ])\n    .pipe(lambda df: (\n        ggplot(df, aes(x='species', y='mean_bill_length', fill='species'))\n        + geom_bar(stat='identity', position='dodge')\n        + geom_errorbar(aes(ymin='mean_bill_length - std_bill_length', ymax='mean_bill_length + std_bill_length'), width=0.2)\n        + theme_bw()\n        + labs(x='Species', y='Mean Bill Length (mm)', title='Mean Bill Length by Species')\n    ))\n    .draw()\n)",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "result visualizations"
    ]
  },
  {
    "objectID": "pyws03-0-text-analysis.html",
    "href": "pyws03-0-text-analysis.html",
    "title": "part 3: text analysis",
    "section": "",
    "text": "Reading and analyzing unstructured text presents a variety of challenges, particularly when dealing with different types of content such as articles, social media posts, or even longer documents like reports. Unstructured text, unlike tabular data, lacks a predefined format, which means it may contain irregularities such as spelling errors, slang, or varied sentence structures. For example, reading short texts like social media posts requires handling informal language, abbreviations, and emojis, whereas longer texts such as research papers or survey responses involve the challenge of organizing and summarizing large amounts of information. Rendered texts, such as those from PDFs or HTML files, often come with additional complexities like layout formatting, metadata, or embedded images, which may need to be stripped out or interpreted correctly before the actual text can be processed. Handling such varied lengths and formats requires careful preprocessing, including cleaning and standardizing the text data to make it suitable for analysis.",
    "crumbs": [
      "workshop",
      "part 3: text analysis"
    ]
  },
  {
    "objectID": "pyws03-0-text-analysis.html#reading-text-content",
    "href": "pyws03-0-text-analysis.html#reading-text-content",
    "title": "part 3: text analysis",
    "section": "",
    "text": "Reading and analyzing unstructured text presents a variety of challenges, particularly when dealing with different types of content such as articles, social media posts, or even longer documents like reports. Unstructured text, unlike tabular data, lacks a predefined format, which means it may contain irregularities such as spelling errors, slang, or varied sentence structures. For example, reading short texts like social media posts requires handling informal language, abbreviations, and emojis, whereas longer texts such as research papers or survey responses involve the challenge of organizing and summarizing large amounts of information. Rendered texts, such as those from PDFs or HTML files, often come with additional complexities like layout formatting, metadata, or embedded images, which may need to be stripped out or interpreted correctly before the actual text can be processed. Handling such varied lengths and formats requires careful preprocessing, including cleaning and standardizing the text data to make it suitable for analysis.",
    "crumbs": [
      "workshop",
      "part 3: text analysis"
    ]
  },
  {
    "objectID": "pyws03-0-text-analysis.html#text-tokenization",
    "href": "pyws03-0-text-analysis.html#text-tokenization",
    "title": "part 3: text analysis",
    "section": "text tokenization",
    "text": "text tokenization\nText tokenization is the foundational step in processing unstructured text, where the text is broken down into smaller units like words or phrases. This allows for basic analysis, such as word frequency counts or simple keyword extraction. However, more advanced techniques go beyond tokenization to capture deeper insights from text. Topic modeling, for instance, identifies latent themes within large text corpora by analyzing patterns in word co-occurrences. It helps uncover hidden structures in the data, which can be particularly useful when working with large datasets of documents, like survey responses or news articles. Sentiment analysis is another advanced technique that goes beyond simple tokenization by determining the emotional tone behind the text, whether it is positive, negative, or neutral. This is especially useful for analyzing customer feedback or social media sentiment. Finally, word vectorization techniques such as Word2Vec or GloVe transform words into numerical vectors based on their context, enabling more sophisticated tasks like measuring semantic similarity between words or phrases, clustering similar documents, or feeding text data into machine learning models. While tokenization provides a starting point, these more advanced techniques enable richer and more meaningful interpretations of textual data.",
    "crumbs": [
      "workshop",
      "part 3: text analysis"
    ]
  },
  {
    "objectID": "pyws03-2-text-analysis.html",
    "href": "pyws03-2-text-analysis.html",
    "title": "text tokenization",
    "section": "",
    "text": "code examples nltk\ncode examples spacy\ndownload jupyter notebook",
    "crumbs": [
      "workshop",
      "part 3: text analysis",
      "text tokenization"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Welcome! This intro to Python workshop is oriented around the principles of Open Science. Python, with its versatile ecosystem and powerful libraries, is a cornerstone for computational data analysis in social science research, enabling key Open Science principles like replicability, transparency, collaboration, and the use of open-source software. Python scripts allow researchers to automate and document their analysis, ensuring replicability—other researchers can run the same code on the same data to verify findings. Transparency is achieved by openly sharing the code, data, and methodology, so that every step of the research process is accessible and understandable. Python also fosters collaboration by allowing multiple researchers to contribute to the same project, often through platforms like GitHub, where code is shared, versioned, and improved. Finally, Python itself is open-source software, meaning anyone can use, modify, and distribute it freely, promoting equitable access to powerful research tools and fostering an inclusive scientific community.\nWorkshop link and Schedule\n\nPart 1: Getting Started\nIn this part, participants will be introduced to Python environments, with a focus on using Google Colab Notebooks, an accessible and powerful tool for coding in Python. We will cover the installation and setup process, ensuring everyone is ready to run Python code in their browsers. The session will also explore basic Python syntax, including variables, data types, and control structures like loops and conditionals, laying the foundation for more advanced applications in social science research.\n\n\nPart 2: Data Analysis\nThis section focuses on performing data analysis with Python, using libraries like Pandas for dataframe manipulation. Participants will learn how to load, clean, and transform data, enabling them to analyze datasets commonly used in social science. We’ll also dive into results visualization using Matplotlib and Seaborn, teaching participants how to create informative charts and graphs to present their findings effectively.\n\n\nPart 3: Text Analysis\nIn this part, participants will explore the basics of text analysis in Python, starting with reading text data from various sources such as documents or online content. We will cover text tokenization, the process of breaking text into individual words or phrases, using libraries like spaCy and NLTK. This will allow participants to process, analyze, and extract meaningful insights from large volumes of textual data, such as social media posts or survey responses.\n\n\nPart 4: Image Analysis\nThe image analysis section will introduce participants to working with visual data in Python. We’ll cover how to read and process image content using libraries like OpenCV and Pillow. Participants will also explore basic object recognition techniques, learning how to detect and classify objects within images, which is particularly useful for social science fields that rely on visual data, such as media studies or behavioral analysis.\n\n\nPart 5: Data Collection\nThe final part of the workshop focuses on data collection techniques using Python. Participants will learn web scraping methods to gather data from websites using tools like BeautifulSoup and Scrapy. Additionally, we will explore how to design and run web experiments, allowing researchers to collect behavioral data from users in real-time. These skills will empower participants to gather the data they need for their social science research projects.\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "start"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The following info is fetched from timeedit:"
  },
  {
    "objectID": "pyws02-1-data-analysis.html",
    "href": "pyws02-1-data-analysis.html",
    "title": "dataframe manipulation",
    "section": "",
    "text": "dataframe examples pandas\ndataframe examples polars\ndownload jupyter notebook\n# run inside google colab\n#!git clone https://github.com/cca-cce/osm-cda.git",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "dataframe manipulation"
    ]
  },
  {
    "objectID": "pyws02-1-data-analysis.html#read-and-explore-data",
    "href": "pyws02-1-data-analysis.html#read-and-explore-data",
    "title": "dataframe manipulation",
    "section": "read and explore data",
    "text": "read and explore data\nHere is an analysis of your code, separated into logical steps with each step in its own code chunk, followed by explanations:\n\n1. Import necessary libraries\n\nimport seaborn as sns\nimport pandas as pd\n\nExplanation:\nThis step imports the required libraries for the script. seaborn is a data visualization library that provides built-in datasets, and pandas is used for data manipulation and analysis. Importing these libraries allows the script to use their functions, such as loading a dataset and manipulating DataFrames.\n\n\n\n2. Load the dataset and select specific columns\n\ndf = sns.load_dataset(\"penguins\")\ndf_selected = df[['body_mass_g', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'island']]\n\nExplanation:\nHere, the penguins dataset from Seaborn is loaded into a Pandas DataFrame (df). The next step selects specific columns of interest (body_mass_g, bill_length_mm, bill_depth_mm, flipper_length_mm, and island) and stores them in df_selected. These columns represent both the dependent and independent variables used for further analysis.\n\n\n\n3. Rename selected columns for easier reference\n\ndf_renamed = df_selected.rename(columns={\n    'body_mass_g': 'dep_var', \n    'bill_length_mm': 'indep_var_1', \n    'bill_depth_mm': 'indep_var_2', \n    'flipper_length_mm': 'indep_var_3', \n    'island': 'indep_var_4'\n})\n\nExplanation:\nThis step renames the columns of df_selected to more generic names for easier reference. The dependent variable (body_mass_g) is renamed to dep_var, and the independent variables are renamed to indep_var_1, indep_var_2, indep_var_3, and indep_var_4 (for island). This renamed DataFrame (df_renamed) is used for subsequent analysis.\n\n\n\n4. Save the renamed DataFrame as a TSV file\n\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/csv/data.tsv'\ndf_renamed.to_csv(output_file_path, sep='\\t', index=False)\n\nExplanation:\nThe renamed DataFrame is saved to a TSV (tab-separated values) file at the specified path. The to_csv() method is used with the sep='\\t' argument to ensure that the file is saved in TSV format. The index=False option prevents the DataFrame index from being written to the file.\n\n\n\n5. Generate summary statistics of the DataFrame\n\nsummary_stats = df_renamed.describe()\n\nExplanation:\nThis step generates summary statistics for all numeric columns in the DataFrame using the describe() function. The resulting DataFrame (summary_stats) contains descriptive statistics such as count, mean, standard deviation, minimum, and maximum values, as well as the quartile ranges for the selected variables.\n\n\n\n6. Retrieve the first five records of the DataFrame\n\nfirst_five_records = df_renamed.head()\n\nExplanation:\nThe head() function retrieves the first five rows of the DataFrame. This is useful for a quick inspection of the dataset to verify that the data was loaded and renamed correctly. The first_five_records DataFrame contains the first five records of the renamed DataFrame.\n\n\n\n7. Convert a column to a categorical data type\n\ndf_renamed['indep_var_4'] = df_renamed['indep_var_4'].astype('category')\n\nExplanation:\nThis step converts the indep_var_4 column (formerly island) to a categorical data type using astype('category'). Categorical data types are more memory efficient and appropriate when dealing with a limited number of distinct values, such as categorical variables in a dataset.\n\n\n\n8. Fill missing values in numeric columns with their mean\n\ndf_filled = df_renamed.fillna(df_renamed.mean(numeric_only=True))\n\nExplanation:\nIn this step, any missing values in the numeric columns of df_renamed are filled with the mean value of each column using fillna(). The mean(numeric_only=True) calculates the mean for only numeric columns, and fillna() replaces the missing values with these means. The modified DataFrame is stored as df_filled.\n\n\n\n9. Remove rows with any remaining missing values\n\ndf_no_missing = df_filled.dropna()\n\nExplanation:\nHere, the dropna() function is used to remove any rows that still contain missing values in the DataFrame after filling the numeric columns. Rows with missing values in non-numeric columns will be dropped. The cleaned DataFrame is stored as df_no_missing.\n\n\n\n10. Remove duplicate records\n\ndf_no_duplicates = df_no_missing.drop_duplicates()\n\nExplanation:\nThis step removes any duplicate rows from the DataFrame using the drop_duplicates() method. Duplicate rows are those where all column values are identical. The resulting DataFrame (df_no_duplicates) contains only unique rows, ensuring there are no duplicate records in the dataset.\n\n\n\n11. Output summary statistics and first five records\n\nprint(summary_stats)\nprint(first_five_records)\n\n           dep_var  indep_var_1  indep_var_2  indep_var_3\ncount   342.000000   342.000000   342.000000   342.000000\nmean   4201.754386    43.921930    17.151170   200.915205\nstd     801.954536     5.459584     1.974793    14.061714\nmin    2700.000000    32.100000    13.100000   172.000000\n25%    3550.000000    39.225000    15.600000   190.000000\n50%    4050.000000    44.450000    17.300000   197.000000\n75%    4750.000000    48.500000    18.700000   213.000000\nmax    6300.000000    59.600000    21.500000   231.000000\n   dep_var  indep_var_1  indep_var_2  indep_var_3 indep_var_4\n0   3750.0         39.1         18.7        181.0   Torgersen\n1   3800.0         39.5         17.4        186.0   Torgersen\n2   3250.0         40.3         18.0        195.0   Torgersen\n3      NaN          NaN          NaN          NaN   Torgersen\n4   3450.0         36.7         19.3        193.0   Torgersen\n\n\nExplanation:\nThe final step prints the summary statistics generated earlier (summary_stats) and the first five records (first_five_records) of the renamed DataFrame. This provides an overview of the dataset and allows verification that the data processing steps were applied correctly.",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "dataframe manipulation"
    ]
  },
  {
    "objectID": "pyws02-1-data-analysis.html#select-and-group-data",
    "href": "pyws02-1-data-analysis.html#select-and-group-data",
    "title": "dataframe manipulation",
    "section": "select and group data",
    "text": "select and group data\nHere is an analysis of your Python code, separated into logical steps with each step in its own code chunk, followed by explanations:\n\n1. Import the necessary library\n\nimport pandas as pd\n\nExplanation:\nThis line imports the Pandas library, a powerful Python tool for data manipulation and analysis. By importing Pandas, the script gains access to functions like reading data, filtering, grouping, and saving results.\n\n\n\n2. Load DataFrame from TSV File\n\ninput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/csv/data.tsv'\ndf = pd.read_csv(input_file_path, sep='\\t')\n\nExplanation:\nThis block reads a TSV (tab-separated values) file from the specified file path (input_file_path) into a Pandas DataFrame (df). The sep='\\t' argument specifies that the file uses tabs as delimiters. The result is a DataFrame containing the data from the TSV file, with columns and rows ready for further manipulation.\n\n\n\n3. Filter the Data\n\nfiltered_df = df[df['dep_var'] &gt; 3500]\n\nExplanation:\nHere, the DataFrame is filtered to include only rows where the value in the dep_var column is greater than 3500. The resulting filtered DataFrame (filtered_df) contains a subset of the original data that meets this condition.\n\n\n\n4. Select Specific Columns\n\nselected_columns = filtered_df[['dep_var', 'indep_var_1', 'indep_var_4']]\n\nExplanation:\nIn this step, a new DataFrame (selected_columns) is created by selecting only the specified columns (dep_var, indep_var_1, and indep_var_4) from the previously filtered DataFrame. This reduces the dataset to just the relevant columns needed for further analysis.\n\n\n\n5. Group Data and Calculate the Mean\n\ngrouped_data = df.groupby('indep_var_4')['dep_var'].mean().reset_index()\n\nExplanation:\nThis block groups the data by the indep_var_4 column and calculates the mean of the dep_var column for each group. The result is stored in grouped_data, a DataFrame containing the unique values of indep_var_4 and their corresponding mean dep_var values. The reset_index() function ensures the grouped values are converted back into a DataFrame format.\n\n\n\n6. Merge DataFrames\n\nmerged_df = pd.merge(df, grouped_data, on='indep_var_4', suffixes=('', '_mean'))\n\nExplanation:\nThis step merges the original DataFrame (df) with the grouped_data DataFrame on the indep_var_4 column. The result (merged_df) contains all original columns from df along with the mean dep_var for each group. The suffixes=('', '_mean') ensures that the new dep_var_mean column has a distinct name.\n\n\n\n7. Calculate a New Column\n\nmerged_df['dep_var_diff'] = merged_df['dep_var'] - merged_df['dep_var_mean']\n\nExplanation:\nA new column (dep_var_diff) is added to the merged_df DataFrame. This column represents the difference between the original dep_var values and the mean dep_var values for each group (from the merged dep_var_mean column). The result provides insight into how each dep_var deviates from the group mean.\n\n\n\n8. Create a Pivot Table\n\npivot_table = merged_df.pivot_table(values='dep_var', index='indep_var_4', aggfunc='mean')\n\nExplanation:\nThis block creates a pivot table from merged_df, where indep_var_4 becomes the index, and the mean of dep_var is calculated for each value of indep_var_4. The result is stored in the pivot_table DataFrame, which aggregates the data by indep_var_4 and provides a summary of the mean dep_var.\n\n\n\n9. Save the Pivot Table as a TSV File\n\noutput_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cda/csv/data_pivot.tsv'\npivot_table.to_csv(output_file_path, sep='\\t')\n\nExplanation:\nIn this step, the pivot_table DataFrame is saved as a TSV file to the specified path (output_file_path). The sep='\\t' argument ensures that the data is saved in tab-separated format. This allows the pivot table to be stored and used for further analysis or reporting.\n\n\n\n10. Output Results to Verify\n\nprint(filtered_df.head())\nprint(selected_columns.head())\nprint(grouped_data.head())\nprint(merged_df.head())\nprint(pivot_table)\n\n   dep_var  indep_var_1  indep_var_2  indep_var_3 indep_var_4\n0   3750.0         39.1         18.7        181.0   Torgersen\n1   3800.0         39.5         17.4        186.0   Torgersen\n5   3650.0         39.3         20.6        190.0   Torgersen\n6   3625.0         38.9         17.8        181.0   Torgersen\n7   4675.0         39.2         19.6        195.0   Torgersen\n   dep_var  indep_var_1 indep_var_4\n0   3750.0         39.1   Torgersen\n1   3800.0         39.5   Torgersen\n5   3650.0         39.3   Torgersen\n6   3625.0         38.9   Torgersen\n7   4675.0         39.2   Torgersen\n  indep_var_4      dep_var\n0      Biscoe  4716.017964\n1       Dream  3712.903226\n2   Torgersen  3706.372549\n   dep_var  indep_var_1  indep_var_2  indep_var_3 indep_var_4  dep_var_mean  \\\n0   3750.0         39.1         18.7        181.0   Torgersen   3706.372549   \n1   3800.0         39.5         17.4        186.0   Torgersen   3706.372549   \n2   3250.0         40.3         18.0        195.0   Torgersen   3706.372549   \n3      NaN          NaN          NaN          NaN   Torgersen   3706.372549   \n4   3450.0         36.7         19.3        193.0   Torgersen   3706.372549   \n\n   dep_var_diff  \n0     43.627451  \n1     93.627451  \n2   -456.372549  \n3           NaN  \n4   -256.372549  \n                 dep_var\nindep_var_4             \nBiscoe       4716.017964\nDream        3712.903226\nTorgersen    3706.372549\n\n\nExplanation:\nThis final block prints the first few rows (head()) of various DataFrames, including the filtered data, selected columns, grouped data, merged data, and the pivot table. These print statements allow you to verify the results of each step in the analysis.",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "dataframe manipulation"
    ]
  },
  {
    "objectID": "pyws02-1-data-analysis.html#inferential-data-analysis",
    "href": "pyws02-1-data-analysis.html#inferential-data-analysis",
    "title": "dataframe manipulation",
    "section": "inferential data analysis",
    "text": "inferential data analysis\n\n# Import necessary libraries\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Load the penguins dataset\npenguins = sns.load_dataset('penguins')\n\nIn this step, we import the required libraries for data manipulation and visualization. We use seaborn to load the built-in ‘penguins’ dataset into a pandas DataFrame called penguins.\n\n\n# Display the first few rows to inspect the dataset\nprint(penguins.head())\n\n# Check for missing values\nprint(\"\\nMissing values in each column:\")\nprint(penguins.isnull().sum())\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen            39.1           18.7              181.0   \n1  Adelie  Torgersen            39.5           17.4              186.0   \n2  Adelie  Torgersen            40.3           18.0              195.0   \n3  Adelie  Torgersen             NaN            NaN                NaN   \n4  Adelie  Torgersen            36.7           19.3              193.0   \n\n   body_mass_g     sex  \n0       3750.0    Male  \n1       3800.0  Female  \n2       3250.0  Female  \n3          NaN     NaN  \n4       3450.0  Female  \n\nMissing values in each column:\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64\n\n\nWe use penguins.head() to preview the first few rows of the DataFrame, allowing us to understand the structure and contents of the data. The penguins.isnull().sum() operation checks each column for missing values by summing up the number of NaN entries, providing insight into data completeness.\n\n\n# Drop rows with missing values\npenguins_clean = penguins.dropna()\n\nTo handle missing data, we use dropna() to remove any rows that contain NaN values. This operation results in a new DataFrame penguins_clean that contains only complete cases, ensuring the integrity of subsequent analyses.\n\n\n# Descriptive statistics for numerical variables\nprint(\"\\nDescriptive statistics:\")\nprint(penguins_clean.describe())\n\n\nDescriptive statistics:\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount      333.000000     333.000000         333.000000   333.000000\nmean        43.992793      17.164865         200.966967  4207.057057\nstd          5.468668       1.969235          14.015765   805.215802\nmin         32.100000      13.100000         172.000000  2700.000000\n25%         39.500000      15.600000         190.000000  3550.000000\n50%         44.500000      17.300000         197.000000  4050.000000\n75%         48.600000      18.700000         213.000000  4775.000000\nmax         59.600000      21.500000         231.000000  6300.000000\n\n\nWe generate descriptive statistics using penguins_clean.describe(), which calculates summary metrics like mean, standard deviation, and quartiles for each numerical column. This helps in understanding the distribution and central tendencies of the data.\n\n\n# Histograms for numerical variables\nnumerical_vars = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\npenguins_clean[numerical_vars].hist(bins=15, figsize=(10, 6))\nplt.suptitle('Histograms of Numerical Variables')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe create histograms for numerical variables using DataFrame.hist(), which plots the frequency distribution of each variable in numerical_vars. The histograms help visualize the distribution shape, skewness, and potential outliers in the data.\n\n\n# Boxplots to detect outliers\nplt.figure(figsize=(12, 8))\nfor i, var in enumerate(numerical_vars, 1):\n    plt.subplot(2, 2, i)\n    sns.boxplot(y=penguins_clean[var])\n    plt.title(f'Boxplot of {var}')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTo detect outliers, we use sns.boxplot() for each numerical variable, plotting them in a grid layout. Boxplots display the median, quartiles, and potential outliers as individual points, making it easier to identify anomalies in the data.\n\n\n# Identify outliers using the IQR method\nQ1 = penguins_clean[numerical_vars].quantile(0.25)\nQ3 = penguins_clean[numerical_vars].quantile(0.75)\nIQR = Q3 - Q1\noutlier_condition = ((penguins_clean[numerical_vars] &lt; (Q1 - 1.5 * IQR)) | (penguins_clean[numerical_vars] &gt; (Q3 + 1.5 * IQR))).any(axis=1)\n\n# Remove outliers\npenguins_no_outliers = penguins_clean[~outlier_condition]\n\nWe calculate the Interquartile Range (IQR) for each numerical variable to identify outliers using the 1.5 * IQR rule. The condition outlier_condition flags rows with outliers, and we create a new DataFrame penguins_no_outliers by excluding these rows, thus cleaning the dataset.\n\n\n# Compare dataset sizes before and after removing outliers\nprint(\"\\nDataset size before removing outliers:\", penguins_clean.shape)\nprint(\"Dataset size after removing outliers:\", penguins_no_outliers.shape)\n\n\nDataset size before removing outliers: (333, 7)\nDataset size after removing outliers: (333, 7)\n\n\nWe use DataFrame.shape to compare the number of rows and columns before and after outlier removal. This helps assess the impact of outlier elimination on the dataset size.\n\n\n# Correlation matrix\nnumerical_cols = penguins_no_outliers.select_dtypes(include=[np.number]).columns\ncorr_matrix = penguins_no_outliers[numerical_cols].corr()\nprint(\"\\nCorrelation matrix:\")\nprint(corr_matrix)\n\n\nCorrelation matrix:\n                   bill_length_mm  bill_depth_mm  flipper_length_mm  \\\nbill_length_mm           1.000000      -0.228626           0.653096   \nbill_depth_mm           -0.228626       1.000000          -0.577792   \nflipper_length_mm        0.653096      -0.577792           1.000000   \nbody_mass_g              0.589451      -0.472016           0.872979   \n\n                   body_mass_g  \nbill_length_mm        0.589451  \nbill_depth_mm        -0.472016  \nflipper_length_mm     0.872979  \nbody_mass_g           1.000000  \n\n\nWe compute the correlation matrix using DataFrame.corr(), which calculates pairwise correlation coefficients between numerical variables. This identifies the strength and direction of linear relationships between variables.\n\n\n# Visualize the correlation matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\n\n\n\n\n\nWe visualize the correlation matrix using sns.heatmap(), which creates a color-coded matrix where each cell represents the correlation coefficient between variables. The annot=True parameter displays the numerical values within the cells for precise interpretation.\n\n\n# Select variables with the highest correlation to body_mass_g\nprint(\"\\nCorrelation with body_mass_g:\")\nprint(corr_matrix['body_mass_g'].sort_values(ascending=False))\n\n\nCorrelation with body_mass_g:\nbody_mass_g          1.000000\nflipper_length_mm    0.872979\nbill_length_mm       0.589451\nbill_depth_mm       -0.472016\nName: body_mass_g, dtype: float64\n\n\nWe extract the correlation coefficients of all variables with body_mass_g by accessing the corresponding column in the correlation matrix. Sorting these values helps identify which variables have the strongest linear relationship with body mass.\n\n\n# Prepare data for regression\n# Encode categorical variables\npenguins_encoded = pd.get_dummies(penguins_no_outliers, drop_first=True)\n\nWe use pd.get_dummies() to encode categorical variables into numerical format suitable for regression analysis. The drop_first=True parameter avoids multicollinearity by removing the first category in each encoded variable.\n\n\n# Prepare data for regression\n# Encode categorical variables\npenguins_encoded = pd.get_dummies(penguins_no_outliers, drop_first=True)\n\n# Define dependent and independent variables\nX = penguins_encoded.drop(['body_mass_g'], axis=1)\ny = penguins_encoded['body_mass_g']\n\n# Add a constant to the independent variables\nX = sm.add_constant(X)\n\nWe define the independent variables X by dropping the target variable body_mass_g and the year column from the encoded DataFrame. The dependent variable y is set as body_mass_g. We add a constant term to X using sm.add_constant() to include the intercept in the regression model.\n\n\n# Prepare data for regression\n# Encode categorical variables\npenguins_encoded = pd.get_dummies(penguins_no_outliers, drop_first=True)\n\n# Define dependent and independent variables\nX = penguins_encoded.drop(['body_mass_g'], axis=1)\ny = penguins_encoded['body_mass_g']\n\n# Convert boolean columns to int64\nbool_cols = X.select_dtypes(include=['bool']).columns\nX[bool_cols] = X[bool_cols].astype(int)\n\n# Now check data types\nprint(\"Data types of X after converting bools to int:\")\nprint(X.dtypes)\n\n# Add a constant to the independent variables\nX = sm.add_constant(X)\n\n# Fit the multiple linear regression model\nmodel = sm.OLS(y, X).fit()\n\n# Print the model summary\nprint(\"\\nRegression Model Summary:\")\nprint(model.summary())\n\nData types of X after converting bools to int:\nbill_length_mm       float64\nbill_depth_mm        float64\nflipper_length_mm    float64\nspecies_Chinstrap      int64\nspecies_Gentoo         int64\nisland_Dream           int64\nisland_Torgersen       int64\nsex_Male               int64\ndtype: object\n\nRegression Model Summary:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            body_mass_g   R-squared:                       0.875\nModel:                            OLS   Adj. R-squared:                  0.872\nMethod:                 Least Squares   F-statistic:                     284.1\nDate:                Tue, 17 Sep 2024   Prob (F-statistic):          1.85e-141\nTime:                        16:19:37   Log-Likelihood:                -2353.6\nNo. Observations:                 333   AIC:                             4725.\nDf Residuals:                     324   BIC:                             4760.\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n=====================================================================================\n                        coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nconst             -1500.0291    575.822     -2.605      0.010   -2632.852    -367.207\nbill_length_mm       18.1893      7.136      2.549      0.011       4.150      32.229\nbill_depth_mm        67.5754     19.821      3.409      0.001      28.581     106.570\nflipper_length_mm    16.2385      2.939      5.524      0.000      10.456      22.021\nspecies_Chinstrap  -260.3063     88.551     -2.940      0.004    -434.513     -86.100\nspecies_Gentoo      987.7614    137.238      7.197      0.000     717.771    1257.752\nisland_Dream        -13.1031     58.541     -0.224      0.823    -128.271     102.065\nisland_Torgersen    -48.0636     60.922     -0.789      0.431    -167.915      71.788\nsex_Male            387.2243     48.138      8.044      0.000     292.521     481.927\n==============================================================================\nOmnibus:                        1.114   Durbin-Watson:                   2.167\nProb(Omnibus):                  0.573   Jarque-Bera (JB):                1.101\nSkew:                           0.139   Prob(JB):                        0.577\nKurtosis:                       2.958   Cond. No.                     7.59e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 7.59e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nWe fit a multiple linear regression model using sm.OLS() from the statsmodels library and call .fit() to train the model. The model.summary() function provides a comprehensive summary of the regression results, including coefficients, p-values, and goodness-of-fit metrics.\n\n\n# Plot actual vs. predicted body mass\nplt.figure(figsize=(8, 6))\nplt.scatter(y, model.predict(X), alpha=0.7)\nplt.xlabel('Actual Body Mass (g)')\nplt.ylabel('Predicted Body Mass (g)')\nplt.title('Actual vs. Predicted Body Mass')\nplt.show()\n\n\n\n\n\n\n\n\nWe create a scatter plot to compare the actual and predicted body mass values using plt.scatter(). This visualization helps assess how well the model predictions align with the actual data, indicating the model’s predictive performance.\n\n\n# Residual plot to check for homoscedasticity\nplt.figure(figsize=(8, 6))\nsns.residplot(x=model.predict(X), y=model.resid, lowess=True, line_kws={'color': 'red'})\nplt.xlabel('Predicted Body Mass (g)')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()\n\n\n\n\n\n\n\n\nWe generate a residual plot using sns.residplot() to examine the homoscedasticity (constant variance) of residuals. The plot displays residuals against predicted values, and a red lowess line indicates any systematic patterns, which helps in diagnosing model assumptions.",
    "crumbs": [
      "workshop",
      "part 2: data analysis",
      "dataframe manipulation"
    ]
  },
  {
    "objectID": "pyws01-1-getting-started.html",
    "href": "pyws01-1-getting-started.html",
    "title": "python environments",
    "section": "",
    "text": "sign in to google account to connect colab cloud app\n\n\n\nhttps://colab.research.google.com/",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "python environments"
    ]
  },
  {
    "objectID": "pyws01-1-getting-started.html#google-colab",
    "href": "pyws01-1-getting-started.html#google-colab",
    "title": "python environments",
    "section": "",
    "text": "sign in to google account to connect colab cloud app\n\n\n\nhttps://colab.research.google.com/",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "python environments"
    ]
  },
  {
    "objectID": "pyws01-1-getting-started.html#visual-studio-code",
    "href": "pyws01-1-getting-started.html#visual-studio-code",
    "title": "python environments",
    "section": "visual studio code",
    "text": "visual studio code\n\n\n\ndownload and install vscode app on local computer\n\n\n\nhttps://code.visualstudio.com/download\nhttps://www.python.org/downloads/",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "python environments"
    ]
  },
  {
    "objectID": "pyws01-1-getting-started.html#anaconda",
    "href": "pyws01-1-getting-started.html#anaconda",
    "title": "python environments",
    "section": "anaconda",
    "text": "anaconda\n\n\n\ndownload and install anaconda on local computer\n\n\n\nhttps://www.anaconda.com/download",
    "crumbs": [
      "workshop",
      "part 1: getting started",
      "python environments"
    ]
  },
  {
    "objectID": "pyws02-0-data-analysis.html",
    "href": "pyws02-0-data-analysis.html",
    "title": "part 2: data analysis",
    "section": "",
    "text": "Dataframe operations in Python, particularly with the Pandas library, allow for highly flexible and efficient data manipulation, making it a cornerstone of data analysis in fields like social science. One of the most common operations involves loading data from various sources such as CSV files, databases, or APIs, transforming it into a structured dataframe for analysis. From there, you can perform cleaning operations, such as handling missing values, filtering rows or columns, and converting data types. These cleaning steps are essential in real-world data analysis, where datasets are rarely perfect. Once the data is clean, more advanced dataframe operations come into play, such as grouping and aggregating data by specific columns, which allows users to extract insights like means, counts, or sums based on key categories. Pandas also supports joining and merging data from multiple sources, enabling researchers to combine different datasets for more comprehensive analyses. These operations contrast with the more manual approaches used in tools like Excel, where data manipulation might involve repetitive tasks like copy-pasting or formula adjustments, which can be error-prone and less scalable.",
    "crumbs": [
      "workshop",
      "part 2: data analysis"
    ]
  },
  {
    "objectID": "pyws02-0-data-analysis.html#dataframe-manipulation",
    "href": "pyws02-0-data-analysis.html#dataframe-manipulation",
    "title": "part 2: data analysis",
    "section": "",
    "text": "Dataframe operations in Python, particularly with the Pandas library, allow for highly flexible and efficient data manipulation, making it a cornerstone of data analysis in fields like social science. One of the most common operations involves loading data from various sources such as CSV files, databases, or APIs, transforming it into a structured dataframe for analysis. From there, you can perform cleaning operations, such as handling missing values, filtering rows or columns, and converting data types. These cleaning steps are essential in real-world data analysis, where datasets are rarely perfect. Once the data is clean, more advanced dataframe operations come into play, such as grouping and aggregating data by specific columns, which allows users to extract insights like means, counts, or sums based on key categories. Pandas also supports joining and merging data from multiple sources, enabling researchers to combine different datasets for more comprehensive analyses. These operations contrast with the more manual approaches used in tools like Excel, where data manipulation might involve repetitive tasks like copy-pasting or formula adjustments, which can be error-prone and less scalable.",
    "crumbs": [
      "workshop",
      "part 2: data analysis"
    ]
  },
  {
    "objectID": "pyws02-0-data-analysis.html#result-visualizations",
    "href": "pyws02-0-data-analysis.html#result-visualizations",
    "title": "part 2: data analysis",
    "section": "result visualizations",
    "text": "result visualizations\nWhen it comes to data visualization, it’s important to distinguish between descriptive and inferential visualizations, both of which play a key role in data analysis. Descriptive visualizations, often created using libraries like Matplotlib and Seaborn, are used to summarize data in a visually intuitive way. Examples include bar charts, histograms, and scatter plots, which help the analyst understand the distribution, trends, or relationships within a dataset. These types of visualizations are valuable for exploratory data analysis, where the goal is to make sense of the data’s underlying patterns and features. On the other hand, inferential visualizations take things a step further by incorporating statistical models to make predictions or generalizations about a population based on sample data. Examples include confidence intervals, regression lines, or p-value plots, which are often layered on top of standard visualizations to highlight the uncertainty or significance of findings. While descriptive visualizations provide an overview of the data at hand, inferential visualizations are essential for drawing conclusions that extend beyond the immediate dataset, particularly in social science research, where researchers often need to infer trends across larger populations.",
    "crumbs": [
      "workshop",
      "part 2: data analysis"
    ]
  },
  {
    "objectID": "pyws04-1-image-analysis.html",
    "href": "pyws04-1-image-analysis.html",
    "title": "reading image content",
    "section": "",
    "text": "code examples pil, pillow\ncode examples opencv\ndownload jupyter notebook",
    "crumbs": [
      "workshop",
      "part 4: image analysis",
      "reading image content"
    ]
  }
]