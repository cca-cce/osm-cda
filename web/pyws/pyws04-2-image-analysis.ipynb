{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "  output: true\n",
        "title: \"object recognition\"\n",
        "---\n",
        "\n",
        "- code examples [huggingface](https://huggingface.co/tasks){target=\"_blank\"}\n",
        "- download [jupyter notebook](pyws04-2-image-analysis.ipynb){target=\"_blank\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# run inside google colab\n",
        "#!git clone https://github.com/cca-cce/osm-cca-cv.git\n",
        "#!pip install -q timm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## image classification "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import pipeline\n",
        "clf = pipeline(\"image-classification\")\n",
        "clf(\"/content/osm-cca-cv/res/img/image_prefix-000.png\")\n",
        "\n",
        "#[{'label': 'tabby cat', 'score': 0.731},\n",
        "#...\n",
        "#]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## object recognition "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model = pipeline(\"object-detection\")\n",
        "model(\"/content/osm-cca-cv/res/img/image_prefix-000.png\")\n",
        "# [{'label': 'blanket',\n",
        "#  'mask': mask_string,\n",
        "#  'score': 0.917},\n",
        "#...]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## image, video to text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "captioner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n",
        "#captioner(\"https://huggingface.co/datasets/Narsil/image_dummy/resolve/main/parrots.png\")\n",
        "captioner(\"https://raw.githubusercontent.com/cca-cce/osm-cca-cv/0a62207580cf63bc808ec825669af8d6f2eedf16/res/img/image_prefix-000.png\")\n",
        "## [{'generated_text': 'two birds are standing next to each other '}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pixtral"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# install dependencies\n",
        "#pip install --upgrade vllm\n",
        "#pip install --upgrade mistral_common"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from vllm import LLM\n",
        "from vllm.sampling_params import SamplingParams\n",
        "\n",
        "model_name = \"mistralai/Pixtral-12B-2409\"\n",
        "\n",
        "sampling_params = SamplingParams(max_tokens=8192)\n",
        "\n",
        "llm = LLM(model=model_name, tokenizer_mode=\"mistral\")\n",
        "\n",
        "prompt = \"Describe this image in one sentence.\"\n",
        "image_url = \"https://raw.githubusercontent.com/cca-cce/osm-cca-cv/0a62207580cf63bc808ec825669af8d6f2eedf16/res/img/image_prefix-000.png\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": prompt}, {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}]\n",
        "    },\n",
        "]\n",
        "\n",
        "outputs = llm.chat(messages, sampling_params=sampling_params)\n",
        "\n",
        "print(outputs[0].outputs[0].text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## testing.. on your own risk :)\n",
        "\n",
        "To summarize and describe the visual content of a short video without audio using Python, you can follow these main steps:\n",
        "\n",
        "1. **Extract frames from the video.**\n",
        "2. **Select key frames that represent the content effectively.**\n",
        "3. **Use an image captioning model to generate descriptions of the frames.**\n",
        "4. **Combine the frame descriptions into a coherent summary.**\n",
        "\n",
        "Below is a detailed explanation of each step along with the necessary code.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Extract Frames from the Video\n",
        "\n",
        "*We use the `OpenCV` library to extract frames from the video file. Alternatively, `moviepy` can also be used for frame extraction.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Define the path to the video file\n",
        "video_file = 'path_to_your_video.mp4'\n",
        "\n",
        "# Create a directory to store the extracted frames\n",
        "frames_dir = 'extracted_frames'\n",
        "os.makedirs(frames_dir, exist_ok=True)\n",
        "\n",
        "# Load the video using OpenCV\n",
        "cap = cv2.VideoCapture(video_file)\n",
        "\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Decide on the interval between frames to extract (e.g., every 1 second)\n",
        "frame_interval = int(fps)  # Adjust this value as needed\n",
        "\n",
        "frame_number = 0\n",
        "extracted_frames = []\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if frame_number % frame_interval == 0:\n",
        "        frame_path = os.path.join(frames_dir, f'frame_{frame_number}.jpg')\n",
        "        cv2.imwrite(frame_path, frame)\n",
        "        extracted_frames.append(frame_path)\n",
        "    \n",
        "    frame_number += 1\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(f\"Extracted {len(extracted_frames)} frames.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Notes:**\n",
        "\n",
        "- **Frame Extraction Interval:**\n",
        "  - The `frame_interval` determines how frequently frames are extracted. For example, extracting one frame per second.\n",
        "  - Adjust `frame_interval` based on the video's FPS and the level of detail you need.\n",
        "\n",
        "- **Saving Frames:**\n",
        "  - Frames are saved as JPEG images in the `extracted_frames` directory.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Use an Image Captioning Model to Generate Descriptions\n",
        "\n",
        "*We use the `transformers` library from Hugging Face along with a pre-trained image captioning model to generate descriptions for each extracted frame.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# Load the pre-trained image captioning model and processor\n",
        "model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
        "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set the device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Function to generate caption for an image\n",
        "def generate_caption(image_path):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = pixel_values.to(device)\n",
        "\n",
        "    # Generate caption\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(pixel_values, max_length=50, num_beams=5)\n",
        "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# Generate captions for all extracted frames\n",
        "frame_captions = []\n",
        "\n",
        "for frame_path in extracted_frames:\n",
        "    caption = generate_caption(frame_path)\n",
        "    frame_captions.append({'frame': frame_path, 'caption': caption})\n",
        "    print(f\"{frame_path}: {caption}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Notes:**\n",
        "\n",
        "- **Model Selection:**\n",
        "  - We use the `nlpconnect/vit-gpt2-image-captioning` model, which is a Vision Transformer (ViT) encoder and GPT-2 decoder.\n",
        "  - You can explore other models on Hugging Face Hub if needed.\n",
        "\n",
        "- **Device Configuration:**\n",
        "  - The code checks for GPU availability and uses it if possible for faster processing.\n",
        "\n",
        "- **Caption Generation:**\n",
        "  - The `generate_caption` function processes an image and generates a caption.\n",
        "  - Adjust parameters like `max_length` and `num_beams` for different results.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Combine Captions into a Coherent Summary\n",
        "\n",
        "*We combine the generated captions to form a coherent summary of the video's visual content.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Combine captions into a summary\n",
        "summary_text = ' '.join([item['caption'] for item in frame_captions])\n",
        "\n",
        "print(\"\\nVideo Summary:\")\n",
        "print(summary_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Notes:**\n",
        "\n",
        "- **Simple Concatenation:**\n",
        "  - We concatenate the captions to create a rough summary.\n",
        "  - This method may result in repetitive or disjointed text.\n",
        "\n",
        "- **Advanced Summarization (Optional):**\n",
        "  - For a more coherent summary, you can use a text summarization model to refine the combined captions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the summarization pipeline\n",
        "summarizer = pipeline('summarization')\n",
        "\n",
        "# Summarize the combined captions\n",
        "final_summary = summarizer(summary_text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
        "\n",
        "print(\"\\nFinal Summary:\")\n",
        "print(final_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Notes:**\n",
        "\n",
        "- **Summarization Model:**\n",
        "  - The `summarization` pipeline uses a pre-trained model to condense the text.\n",
        "\n",
        "- **Adjusting Parameters:**\n",
        "  - Modify `max_length` and `min_length` to control the length of the final summary.\n",
        "\n",
        "---\n",
        "\n",
        "### Complete Code Example\n",
        "\n",
        "Putting it all together, here's the complete code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer, pipeline\n",
        "\n",
        "# Step 1: Extract Frames from the Video\n",
        "video_file = 'path_to_your_video.mp4'\n",
        "frames_dir = 'extracted_frames'\n",
        "os.makedirs(frames_dir, exist_ok=True)\n",
        "\n",
        "cap = cv2.VideoCapture(video_file)\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_interval = int(fps)  # Extract one frame per second\n",
        "frame_number = 0\n",
        "extracted_frames = []\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if frame_number % frame_interval == 0:\n",
        "        frame_path = os.path.join(frames_dir, f'frame_{frame_number}.jpg')\n",
        "        cv2.imwrite(frame_path, frame)\n",
        "        extracted_frames.append(frame_path)\n",
        "    \n",
        "    frame_number += 1\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(f\"Extracted {len(extracted_frames)} frames.\")\n",
        "\n",
        "# Step 2: Generate Captions for Each Frame\n",
        "model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
        "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = pixel_values.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(pixel_values, max_length=50, num_beams=5)\n",
        "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "frame_captions = []\n",
        "\n",
        "for frame_path in extracted_frames:\n",
        "    caption = generate_caption(frame_path)\n",
        "    frame_captions.append({'frame': frame_path, 'caption': caption})\n",
        "    print(f\"{frame_path}: {caption}\")\n",
        "\n",
        "# Step 3: Combine Captions into a Summary\n",
        "summary_text = ' '.join([item['caption'] for item in frame_captions])\n",
        "\n",
        "# Optional: Summarize the Combined Captions\n",
        "summarizer = pipeline('summarization')\n",
        "final_summary = summarizer(summary_text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
        "\n",
        "print(\"\\nFinal Summary:\")\n",
        "print(final_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Additional Considerations\n",
        "\n",
        "- **Installing Dependencies:**\n",
        "\n",
        "  Ensure that you have all the required libraries installed:\n",
        "\n",
        "  ```bash\n",
        "  pip install opencv-python\n",
        "  pip install transformers\n",
        "  pip install torch  # For GPU support, install torch appropriate for your CUDA version\n",
        "  pip install pillow\n",
        "  ```\n",
        "\n",
        "- **Frame Selection:**\n",
        "\n",
        "  - **Dynamic Frame Selection:**\n",
        "    - Instead of extracting frames at fixed intervals, you can implement shot detection or scene change detection to extract frames when the content changes significantly.\n",
        "    - Libraries like `scenedetect` can help with this.\n",
        "\n",
        "- **Processing Time:**\n",
        "\n",
        "  - Image captioning models can be computationally intensive.\n",
        "  - Processing time will increase with the number of frames and the complexity of the model.\n",
        "  - Using a GPU significantly speeds up the process.\n",
        "\n",
        "- **Model Limitations:**\n",
        "\n",
        "  - The quality of the generated captions depends on the model's capabilities.\n",
        "  - Pre-trained models may not accurately describe all types of content.\n",
        "  - For domain-specific videos, consider fine-tuning a model on relevant datasets.\n",
        "\n",
        "- **Enhancing Summarization:**\n",
        "\n",
        "  - **Text Preprocessing:**\n",
        "    - Remove duplicate captions or captions with minimal information before summarization.\n",
        "  - **Semantic Understanding:**\n",
        "    - Use natural language understanding techniques to better combine captions.\n",
        "\n",
        "- **Privacy and Compliance:**\n",
        "\n",
        "  - Be cautious when processing videos containing personal or sensitive information.\n",
        "  - Ensure compliance with data protection regulations.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "By following these steps, you can extract frames from a video without audio, generate descriptions of the visual content using an image captioning model, and combine those descriptions into a textual summary. This approach leverages powerful pre-trained models available through the Hugging Face Transformers library and allows you to create summaries of visual content programmatically.\n",
        "\n",
        "**References:**\n",
        "\n",
        "- **OpenCV Documentation:** [https://docs.opencv.org/](https://docs.opencv.org/)\n",
        "- **Hugging Face Transformers Documentation:** [https://huggingface.co/transformers/](https://huggingface.co/transformers/)\n",
        "- **Image Captioning Models:** [https://huggingface.co/models?pipeline_tag=image-to-text](https://huggingface.co/models?pipeline_tag=image-to-text)\n",
        "- **Scene Detection with PySceneDetect:** [https://pyscenedetect.readthedocs.io/](https://pyscenedetect.readthedocs.io/)\n",
        "\n",
        "---\n",
        "\n",
        "**Example Output:**\n",
        "\n",
        "Assuming we have a video and we run the code, the output might look like:\n",
        "\n",
        "```\n",
        "Extracted 10 frames.\n",
        "extracted_frames/frame_0.jpg: a group of people standing in a room.\n",
        "extracted_frames/frame_30.jpg: a man holding a microphone.\n",
        "extracted_frames/frame_60.jpg: a woman sitting at a desk with a laptop.\n",
        "...\n",
        "Final Summary:\n",
        "A group of people are standing in a room. A man is holding a microphone. A woman is sitting at a desk with a laptop. ...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- Replace `'path_to_your_video.mp4'` with the actual path to your video file.\n",
        "- Adjust the `frame_interval` based on the video's length and desired granularity.\n",
        "- Ensure you have sufficient system resources to run the models, especially if processing longer videos or using higher-resolution frames.\n",
        "\n",
        "---\n",
        "\n",
        "By automating the process of generating textual descriptions from video frames, you can create summaries of videos without audio, which is particularly useful for silent surveillance footage, visual-only content, or scenarios where audio data is unavailable or unusable."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}