{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "  output: true\n",
        "title: \"reading text content\"\n",
        "---\n",
        "\n",
        "- code examples [nltk](https://www.nltk.org/howto.html)\n",
        "- code examples [spacy](https://spacy.io/usage/spacy-101)\n",
        "- download [jupyter notebook](pyws03-1-text-analysis.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# run inside google colab\n",
        "#!git clone https://github.com/cca-cce/osm-cca-nlp.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## recap reading data files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "#| echo: true\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# comma separated\n",
        "df = pd.read_csv('users.csv', sep=',', quotechar='\"', header=0)\n",
        "#df = pd.read_csv('users.csv', sep=',', quotechar='\"', header=None)\n",
        "#df = pd.read_csv('users.csv', sep=',', quotechar=\"'\", header=0)\n",
        "#df = pd.read_csv('users.csv', sep=',', quotechar=\"'\", header=None)\n",
        "#df = pd.read_csv('users.csv', sep='\\t', quotechar='\"', header=0)\n",
        "#df = pd.read_csv('users.csv', sep='\\t', quotechar='\"', header=None)\n",
        "#df = pd.read_csv('users.csv', sep='\\t', quotechar=\"'\", header=0)\n",
        "#df = pd.read_csv('users.csv', sep='\\t', quotechar=\"'\", header=None)\n",
        "\n",
        "# tab separated \n",
        "#df = pd.read_csv('users.tsv', sep=',', quotechar='\"', header=0)\n",
        "#df = pd.read_csv('users.tsv', sep=',', quotechar='\"', header=None)\n",
        "#df = pd.read_csv('users.tsv', sep=',', quotechar=\"'\", header=0)\n",
        "#df = pd.read_csv('users.tsv', sep=',', quotechar=\"'\", header=None)\n",
        "df = pd.read_csv('users.tsv', sep='\\t', quotechar='\"', header=0)\n",
        "#df = pd.read_csv('users.tsv', sep='\\t', quotechar='\"', header=None)\n",
        "#df = pd.read_csv('users.tsv', sep='\\t', quotechar=\"'\", header=0)\n",
        "#df = pd.read_csv('users.tsv', sep='\\t', quotechar=\"'\", header=None)\n",
        "\n",
        "# excel\n",
        "#df = pd.read_excel('users.xlsx', header=0, sheet_name=1)\n",
        "#df = pd.read_excel('users.xlsx', header=None, sheet_name=1)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## nltk and text corpora\n",
        "\n",
        "### Import Libraries and Download NLTK Data\n",
        "\n",
        "*In this step, we import the necessary libraries and download the required NLTK data packages. Specifically, we use NLTK's `download` function to ensure the 'gutenberg' corpus and the 'punkt' tokenizer are available for use. The 'punkt' tokenizer is essential for splitting text into sentences and words.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from IPython.display import display\n",
        "\n",
        "# Download necessary NLTK data files\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the Gutenberg Corpus\n",
        "\n",
        "*Here, we import the Gutenberg corpus from NLTK's corpus module. The Gutenberg corpus is a collection of literary texts that we will analyze. We retrieve the list of file IDs available in the corpus using `gutenberg.fileids()`, which provides us with the filenames of the texts in the corpus.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Get list of file IDs from the Gutenberg corpus\n",
        "file_ids = gutenberg.fileids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze Each Text in the Corpus\n",
        "\n",
        "*In this section, we iterate over each text in the Gutenberg corpus to compute various linguistic statistics. We use NLTK's `raw()` method to get the raw text, `word_tokenize()` to split the text into words, and `sent_tokenize()` to split the text into sentences. These NLTK tokenizers are essential for textual analysis.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize a list to store statistics\n",
        "stats_list = []\n",
        "\n",
        "# Analyze each text in the corpus\n",
        "for file_id in file_ids:\n",
        "    raw_text = gutenberg.raw(file_id)\n",
        "    words = nltk.word_tokenize(raw_text)\n",
        "    sentences = nltk.sent_tokenize(raw_text)\n",
        "    num_words = len(words)\n",
        "    num_sentences = len(sentences)\n",
        "    avg_word_length = sum(len(word) for word in words) / num_words\n",
        "    vocab_size = len(set(words))\n",
        "    lexical_diversity = vocab_size / num_words\n",
        "    stats_list.append({\n",
        "        'Title': file_id,\n",
        "        'Num_Words': num_words,\n",
        "        'Num_Sentences': num_sentences,\n",
        "        'Avg_Word_Length': avg_word_length,\n",
        "        'Vocab_Size': vocab_size,\n",
        "        'Lexical_Diversity': lexical_diversity\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create and Display the DataFrame\n",
        "\n",
        "*We create a pandas DataFrame from the collected statistics for easier analysis and display it within the notebook using `display()`. This allows us to view the computed statistics in a structured tabular format.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a DataFrame to hold the statistics\n",
        "stats_df = pd.DataFrame(stats_list)\n",
        "\n",
        "# Display the statistics table\n",
        "display(stats_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up the Output Directory\n",
        "\n",
        "*Here, we define the output path where we'll save the text files and figures. We use `os.makedirs()` with `exist_ok=True` to create the directory if it doesn't already exist, ensuring that our output files have a designated location.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define the output path for saving text files and figures\n",
        "output_path = \"/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/tmp\"\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_path, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate and Display Plots\n",
        "\n",
        "*In this step, we create various plots to visualize the text statistics using Seaborn and Matplotlib. We display these plots inline in the notebook using `plt.show()`. The plots include:*\n",
        "\n",
        "- *A bar plot of the number of words per text.*\n",
        "- *A bar plot of the average word length per text.*\n",
        "- *A scatter plot of vocabulary size versus the number of words.*\n",
        "\n",
        "*We utilize NLTK's tokenization outputs to extract the necessary values for plotting.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set up seaborn style\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "# Bar plot of number of words per text\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Title', y='Num_Words', data=stats_df)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Number of Words per Text')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_path, 'num_words_per_text.png'))\n",
        "plt.show()\n",
        "\n",
        "# Bar plot of average word length per text\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Title', y='Avg_Word_Length', data=stats_df)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Average Word Length per Text')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_path, 'avg_word_length_per_text.png'))\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot of vocabulary size vs. number of words\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Num_Words', y='Vocab_Size', data=stats_df, hue='Title')\n",
        "plt.title('Vocabulary Size vs. Number of Words')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_path, 'vocab_size_vs_num_words.png'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Texts to Disk\n",
        "\n",
        "*Finally, we save each text from the Gutenberg corpus as a plain text file to the specified output directory. We use NLTK's `raw()` method again to retrieve the full text of each file and write it to disk using standard file I/O operations.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save each text as a plain text file to the output path\n",
        "for file_id in file_ids:\n",
        "    raw_text = gutenberg.raw(file_id)\n",
        "    output_file_path = os.path.join(output_path, file_id)\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(raw_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Saved Texts into an NLTK Corpus\n",
        "\n",
        "*In this final step, we read the saved texts from the output directory back into an NLTK corpus using `PlaintextCorpusReader`. This allows us to treat the collection of saved texts as a corpus for further analysis. `PlaintextCorpusReader` is an NLTK class designed to read plain text files from a directory and create a corpus object.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from nltk.corpus import PlaintextCorpusReader\n",
        "\n",
        "# Define the corpus root directory\n",
        "corpus_root = output_path\n",
        "\n",
        "# Define the pattern to match the text files (e.g., all files with .txt extension)\n",
        "file_pattern = '.*'  # Matches all files\n",
        "# Matches only text files\n",
        "file_pattern = r'.*\\.txt'  # Matches all files ending with .txt\n",
        "\n",
        "# Create a PlaintextCorpusReader object\n",
        "new_corpus = PlaintextCorpusReader(corpus_root, file_pattern)\n",
        "\n",
        "# Access the file IDs in the new corpus\n",
        "new_file_ids = new_corpus.fileids()\n",
        "print(\"Files in the new corpus:\", new_file_ids)\n",
        "\n",
        "# Example: Read words from a specific file\n",
        "words_in_file = new_corpus.words(new_file_ids[0])\n",
        "print(\"First 20 words in\", new_file_ids[0], \":\", words_in_file[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*By using `PlaintextCorpusReader`, we can load all the saved text files into a new NLTK corpus. The `fileids()` method lists all the files in the corpus, and methods like `words()`, `sents()`, and `paras()` allow us to access words, sentences, and paragraphs, respectively. This demonstrates NLTK's capability to handle custom corpora built from local text files, enabling further text processing and analysis on the newly created corpus.*\n",
        "\n",
        "## text to pandas dataframe\n",
        "\n",
        "- example [sustainability communication](https://www.lunduniversity.lu.se/about-university/university-glance/mission-vision-and-values/sustainability)\n",
        "\n",
        "### Import Libraries and Define Text Cleaning Function\n",
        "\n",
        "*In this step, we import the necessary libraries and define a function to clean text data. We use the `os` module for file and directory operations, `re` for regular expressions, `pandas` for data manipulation, and `spacy` for natural language processing tasks.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Function to clean text by removing non-ASCII characters\n",
        "def clean_text(text):\n",
        "    # Remove non-ASCII characters (commented out to preserve UTF-8 text)\n",
        "    # cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "    cleaned_text = text\n",
        "    return cleaned_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*The `clean_text` function is intended to remove non-ASCII characters using `re.sub`. However, since we are dealing with UTF-8 encoded text (e.g., Swedish text data), we retain the original text by commenting out the removal line.*\n",
        "\n",
        "---\n",
        "\n",
        "### Set Directory Paths and Initialize Data Structures\n",
        "\n",
        "*Here, we specify the directory paths where the text files are located and initialize data structures for storing the text data. The `directory_path` variable holds the path to the directory containing the text files. We also initialize an empty list `data` to store the text information and a counter `unique_id` for assigning unique identifiers to each text.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Directory containing text files\n",
        "directory_path = '/content/osm-cca-nlp/res'\n",
        "directory_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/res'\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "data = []\n",
        "\n",
        "# Initialize a unique ID counter\n",
        "unique_id = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*The `os` module functions will later use `directory_path` to access the files. The `unique_id` will increment for each file, ensuring each text has a unique identifier.*\n",
        "\n",
        "---\n",
        "\n",
        "### Read and Clean Text Files\n",
        "\n",
        "*In this section, we iterate over the text files in the specified directory, read their contents, clean the text using the `clean_text` function, and store the data in the `data` list. The `os.listdir` function lists all files in the directory, and `os.path.join` constructs the full file path.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Iterate over the text files in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    # Consider only plain text files\n",
        "    if filename.endswith(\".txt\") or filename.endswith(\".md\"):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "        # Read the file content\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Clean the text\n",
        "        cleaned_text = clean_text(text)\n",
        "\n",
        "        # Append the data as a dictionary with a unique ID\n",
        "        data.append({\n",
        "            'id': unique_id,\n",
        "            'filename': filename,\n",
        "            'original_text': text,\n",
        "            'cleaned_text': cleaned_text\n",
        "        })\n",
        "\n",
        "        # Increment the unique ID\n",
        "        unique_id += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*We use `open` with `encoding='utf-8'` to read the files, ensuring that UTF-8 characters are handled correctly. The cleaned text and metadata are stored as dictionaries in the `data` list.*\n",
        "\n",
        "---\n",
        "\n",
        "### Create and Save DataFrame\n",
        "\n",
        "*We convert the collected data into a Pandas DataFrame for easier manipulation and analysis. We then save this DataFrame as a TSV (Tab-Separated Values) file using the `to_csv` method with `sep='\\t'`. The `index=False` parameter ensures that the DataFrame index is not included in the output file.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a Pandas DataFrame\n",
        "text_df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame as a TSV file in the 'csv' subdirectory\n",
        "output_file_path = '/content/osm-cca-nlp/csv/text_data.tsv'\n",
        "output_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/text_data.tsv'\n",
        "\n",
        "# Save the DataFrame to a TSV file\n",
        "text_df.to_csv(output_file_path, sep='\\t', index=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(text_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*This step utilizes Pandas' data handling capabilities to structure our text data effectively and save it for future use.*\n",
        "\n",
        "---\n",
        "\n",
        "### Load spaCy Model\n",
        "\n",
        "*We load a spaCy language model to perform natural language processing tasks. The `spacy.load` function loads the specified model into memory. In this case, we use the small English model `en_core_web_sm`.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the spaCy model (small English model is used here)\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*The loaded `nlp` object provides access to spaCy's powerful NLP features, including tokenization, part-of-speech tagging, and sentence segmentation.*\n",
        "\n",
        "---\n",
        "\n",
        "### Compute Text Statistics\n",
        "\n",
        "*We calculate word counts, character counts, and sentence counts for each cleaned text in the DataFrame. Pandas' `apply` function applies a lambda function to each row in the `cleaned_text` column. For sentence counting, we use spaCy's sentence segmentation by processing the text with `nlp` and accessing the `.sents` attribute.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Perform word count and character count on each cleaned text in the DataFrame\n",
        "text_df['word_count'] = text_df['cleaned_text'].apply(lambda x: len(x.split()))\n",
        "text_df['character_count'] = text_df['cleaned_text'].apply(lambda x: len(x))\n",
        "\n",
        "# Perform sentence count using spaCy\n",
        "text_df['sentence_count'] = text_df['cleaned_text'].apply(lambda x: len(list(nlp(x).sents)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*The `len(x.split())` calculates the number of words by splitting the text on whitespace. The character count is obtained with `len(x)`. For sentence count, we process the text with the spaCy model and convert the `sents` generator to a list to count the sentences.*\n",
        "\n",
        "---\n",
        "\n",
        "### Display DataFrame with Selected Columns\n",
        "\n",
        "*Finally, we display the DataFrame, excluding the 'original_text' and 'cleaned_text' columns for brevity. The `columns.difference` function identifies columns to exclude, and we use this to select the remaining columns for display.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select and print all columns except 'original_text' and 'cleaned_text'\n",
        "columns_to_display = text_df.columns.difference(['original_text', 'cleaned_text'])\n",
        "print(text_df[columns_to_display])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*This step showcases the metadata and statistical information we've gathered, such as the unique ID, filename, word count, character count, and sentence count, without displaying the potentially lengthy text content.*\n",
        "\n",
        "---\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "- **os module:** Used for interacting with the operating system, listing directory contents, and constructing file paths.\n",
        "- **re module:** Provides regular expression matching operations for text cleaning (though in this code, the regex is commented out).\n",
        "- **pandas:** Used for creating and manipulating the DataFrame to store text data and computed statistics.\n",
        "- **spacy:** Provides advanced NLP capabilities; we load a language model to perform sentence segmentation for counting sentences.\n",
        "- **apply and lambda functions in pandas:** Used to apply functions to DataFrame columns for calculating word counts, character counts, and sentence counts.\n",
        "\n",
        "This modular approach allows for easy understanding and maintenance of the code, with each section handling a specific part of the text processing pipeline.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}