{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "  output: true\n",
        "title: \"text tokenization\"\n",
        "---\n",
        "\n",
        "- code examples [nltk](https://www.nltk.org/howto.html)\n",
        "- code examples [spacy](https://spacy.io/usage/spacy-101)\n",
        "- download [jupyter notebook](pyws03-2-text-analysis.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# run inside google colab\n",
        "#!git clone https://github.com/cca-cce/osm-cca-nlp.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## natural language processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "\n",
        "# Process the sentence with spaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Print parts of speech tags\n",
        "print(\"Parts of Speech:\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text}: {token.pos_} ({token.tag_})\")\n",
        "\n",
        "# Print named entities\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text}: {ent.label_}\")\n",
        "\n",
        "# Render the dependency parse tree in Jupyter Notebook\n",
        "displacy.render(doc, style='dep', jupyter=True)\n",
        "\n",
        "# Render the named entity recognition visualization in Jupyter Notebook\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Instructions:**\n",
        "\n",
        "- **Imports:**\n",
        "  - `spacy` is imported for natural language processing tasks.\n",
        "  - `displacy` from `spacy` is imported for visualizations.\n",
        "\n",
        "- **Load Model:**\n",
        "  - The English language model `en_core_web_sm` is loaded using `spacy.load()`.\n",
        "\n",
        "- **Process Sentence:**\n",
        "  - The example sentence is processed to create a `Doc` object.\n",
        "\n",
        "- **Parts of Speech (POS) Tags:**\n",
        "  - Iterating over `doc`, each token's text and POS tags are printed.\n",
        "\n",
        "- **Named Entities:**\n",
        "  - Iterating over `doc.ents`, each entity's text and label are printed.\n",
        "\n",
        "- **Visualizations:**\n",
        "  - The dependency parse tree and named entity recognition (NER) are rendered using `displacy.render()` with `jupyter=True` to display within a Jupyter notebook.\n",
        "\n",
        "## spacy sentence processing\n",
        "\n",
        "### Import Libraries\n",
        "\n",
        "*In this step, we import the necessary libraries: pandas for data manipulation, spaCy for natural language processing tasks, and os for interacting with the operating system.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import os\n",
        "\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Load Data from TSV File\n",
        "\n",
        "*We load the DataFrame `text_df` from a TSV (Tab-Separated Values) file using pandas' `read_csv` function with the separator set to tab (`\\t`). The `input_file_path` variable specifies the path to the input file. Note that two file paths are provided; the second one overwrites the first, so only the last path is used.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load text_df from the TSV file\n",
        "input_file_path = '/content/osm-cca-nlp/csv/text_data.tsv'\n",
        "input_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/text_data.tsv'\n",
        "text_df = pd.read_csv(input_file_path, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Here, pandas reads the TSV file into a DataFrame, which allows for efficient data manipulation and analysis.*\n",
        "\n",
        "---\n",
        "\n",
        "### Load the spaCy Model\n",
        "\n",
        "*We load the spaCy English language model using `spacy.load`. The model `en_core_web_sm` is a small English model that includes vocabulary, syntax, entities, and word vectors.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the spaCy model (small English model is used here)\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*This model provides the necessary tools for tokenization, part-of-speech tagging, named entity recognition, and sentence segmentation, which are essential for processing and analyzing text data.*\n",
        "\n",
        "---\n",
        "\n",
        "### Initialize List to Store Sentence Data\n",
        "\n",
        "*We initialize an empty list `sentence_data` to store information about each sentence extracted from the texts. This list will be populated with dictionaries containing sentence-level data.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize an empty list to store sentence data\n",
        "sentence_data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*By using a list, we can dynamically append sentence data as we process each text, which will later be converted into a pandas DataFrame for further analysis.*\n",
        "\n",
        "---\n",
        "\n",
        "### Extract Sentences Using spaCy\n",
        "\n",
        "*In this step, we iterate over each row in the `text_df` DataFrame using `iterrows()`. For each text, we process the 'cleaned_text' column with the spaCy NLP pipeline to obtain a `Doc` object. We then iterate over the sentences in the `Doc` using `doc.sents` and collect the original text ID, sentence number, and the sentence text. Each sentence's data is appended to the `sentence_data` list as a dictionary.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Iterate over the cleaned text in the DataFrame\n",
        "for index, row in text_df.iterrows():\n",
        "    doc = nlp(row['cleaned_text'])  # Process the cleaned text with spaCy\n",
        "\n",
        "    # Iterate over the sentences in the document\n",
        "    for i, sentence in enumerate(doc.sents):\n",
        "        sentence_data.append({\n",
        "            'id': row['id'],           # Original text ID\n",
        "            'sentence_number': i + 1,  # Sentence number (starting from 1)\n",
        "            'sentence_text': sentence.text.strip()  # Sentence text\n",
        "        })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*This process leverages spaCy's sentence segmentation capabilities, which use linguistic rules and machine learning models to accurately split text into sentences.*\n",
        "\n",
        "---\n",
        "\n",
        "### Create DataFrame with Sentence Data\n",
        "\n",
        "*We create a new pandas DataFrame `sentence_df` from the `sentence_data` list. This DataFrame contains all the sentences extracted from the texts along with their corresponding IDs and sentence numbers.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a new DataFrame with the sentence data\n",
        "sentence_df = pd.DataFrame(sentence_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Using pandas allows us to efficiently organize and manipulate the sentence-level data for analysis or storage.*\n",
        "\n",
        "---\n",
        "\n",
        "### Save Sentence Data to TSV File\n",
        "\n",
        "*We save the `sentence_df` DataFrame to a TSV file using the `to_csv` method, specifying the tab separator (`\\t`) and setting `index=False` to exclude the DataFrame's index from the output file. Again, two output file paths are provided, with the second one overwriting the first.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save the sentence_df DataFrame as a TSV file\n",
        "output_file_path = '/content/osm-cca-nlp/csv/sentence_data.tsv'\n",
        "output_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/sentence_data.tsv'\n",
        "sentence_df.to_csv(output_file_path, sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*This step ensures that the extracted sentence data is saved in a structured format, which can be easily shared or used in subsequent analyses.*\n",
        "\n",
        "---\n",
        "\n",
        "### Display the Sentence DataFrame\n",
        "\n",
        "*Finally, we display the `sentence_df` DataFrame by printing it to the console. This allows us to verify the contents and ensure that the sentence extraction was successful.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display the sentence DataFrame\n",
        "print(sentence_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Viewing the DataFrame helps in quick validation of the data processing steps and provides an immediate look at the results of our sentence extraction.*\n",
        "\n",
        "---\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "- **pandas (`pd`):** Used for data manipulation and storage in DataFrames, providing efficient methods to read from and write to various file formats.\n",
        "- **spaCy (`spacy`):** Utilized for advanced natural language processing tasks, particularly for sentence segmentation in this code.\n",
        "- **os module:** Although imported, it isn't actively used in this code snippet. Typically, it would be used for file path manipulations or checking file existence.\n",
        "- **Data Processing Steps:**\n",
        "  - Loading text data from a TSV file.\n",
        "  - Processing text with spaCy to extract sentences.\n",
        "  - Storing the sentences along with metadata in a DataFrame.\n",
        "  - Saving the processed data back to a TSV file for future use.\n",
        "\n",
        "This modular approach breaks down the task into manageable steps, making the code easier to understand and maintain.\n",
        "\n",
        "## spacy token processing\n",
        "\n",
        "### Import Libraries\n",
        "\n",
        "*In this step, we import the necessary libraries: pandas for data manipulation, spaCy for natural language processing tasks, and os for interacting with the operating system.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Load Sentence Data from TSV File\n",
        "\n",
        "*We load the `sentence_df` DataFrame from a TSV (Tab-Separated Values) file using pandas' `read_csv` function with the separator set to tab (`\\t`). The `input_file_path` variable specifies the path to the input file. Note that the second assignment of `input_file_path` overwrites the first, so only the last path is used.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load sentence_df from the TSV file\n",
        "input_file_path = '/content/osm-cca-nlp/csv/sentence_data.tsv'\n",
        "input_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/sentence_data.tsv'\n",
        "sentence_df = pd.read_csv(input_file_path, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*By using pandas, we efficiently read the TSV file into a DataFrame for further processing.*\n",
        "\n",
        "---\n",
        "\n",
        "### Load the spaCy Model\n",
        "\n",
        "*We load the spaCy English language model `en_core_web_sm` using `spacy.load()`. This model provides tools for tokenization, part-of-speech tagging, named entity recognition, and more.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the spaCy model (small English model is used here)\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*The `nlp` object now contains the language model, which we'll use to process text data.*\n",
        "\n",
        "---\n",
        "\n",
        "### Initialize List to Store Token Data\n",
        "\n",
        "*We initialize an empty list `token_data` to store information about each token extracted from the sentences. This list will accumulate dictionaries containing token-level data.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize an empty list to store token data\n",
        "token_data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*This prepares us to collect detailed linguistic information from each sentence.*\n",
        "\n",
        "---\n",
        "\n",
        "### Extract Tokens from Sentences Using spaCy\n",
        "\n",
        "*We iterate over each sentence in `sentence_df` using `iterrows()`. For each sentence, we process it with spaCy to obtain a `Doc` object, which contains the tokens and their linguistic annotations. We then iterate over each token in the `Doc`, extracting attributes such as the token's text, lemma, part-of-speech tag, and named entity type. This information is stored in the `token_data` list as dictionaries.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Iterate over the sentences in the sentence_df DataFrame\n",
        "for index, row in sentence_df.iterrows():\n",
        "    doc = nlp(row['sentence_text'])  # Process the sentence text with spaCy\n",
        "\n",
        "    # Iterate over the tokens in the sentence\n",
        "    for j, token in enumerate(doc):\n",
        "        token_data.append({\n",
        "            'id': row['id'],                            # Original text ID\n",
        "            'sentence_number': row['sentence_number'],  # Sentence number\n",
        "            'token_number': j + 1,                      # Token number (starting from 1)\n",
        "            'token_text': token.text,                   # Token text\n",
        "            'token_lemma': token.lemma_,                # Token lemma\n",
        "            'token_pos': token.pos_,                    # Token part of speech\n",
        "            'token_entity': token.ent_type_             # Token entity type (if any)\n",
        "        })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*This step leverages spaCy's powerful NLP features to extract and annotate tokens, which is essential for detailed text analysis.*\n",
        "\n",
        "---\n",
        "\n",
        "### Create a DataFrame with Token Data\n",
        "\n",
        "*We convert the `token_data` list into a pandas DataFrame called `token_df`. This DataFrame organizes the token-level information in a tabular format, making it easier to analyze and manipulate.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a new DataFrame with the token data\n",
        "token_df = pd.DataFrame(token_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Using pandas allows us to handle large amounts of data efficiently and provides tools for data analysis.*\n",
        "\n",
        "---\n",
        "\n",
        "### Save Token Data to TSV File\n",
        "\n",
        "*We save the `token_df` DataFrame to a TSV file using pandas' `to_csv()` method. We set the separator to tab (`\\t`) and `index=False` to exclude the DataFrame's index from the output file. Similar to before, the second assignment of `output_file_path` overwrites the first.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save the token_df DataFrame as a TSV file\n",
        "output_file_path = '/content/osm-cca-nlp/csv/token_data.tsv'\n",
        "output_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/token_data.tsv'\n",
        "token_df.to_csv(output_file_path, sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*This saves our token-level data in a structured format that can be shared or used in future analyses.*\n",
        "\n",
        "---\n",
        "\n",
        "### Display the Token DataFrame\n",
        "\n",
        "*Finally, we print the `token_df` DataFrame to display the collected token data. This allows us to verify that the token extraction and annotation processes were successful.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display the token DataFrame\n",
        "print(token_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Viewing the DataFrame provides immediate feedback on the results of our processing pipeline.*\n",
        "\n",
        "---\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "- **pandas (`pd`):** Used for reading and writing TSV files and handling DataFrames.\n",
        "- **spaCy (`spacy`):** Utilized for processing text to extract tokens, lemmas, parts of speech, and named entities.\n",
        "- **Data Processing Steps:**\n",
        "  - Loading sentence-level data from a TSV file.\n",
        "  - Processing sentences with spaCy to extract token-level information.\n",
        "  - Storing and organizing token data in a pandas DataFrame.\n",
        "  - Saving the token data to a TSV file for future use or analysis.\n",
        "\n",
        "This modular approach ensures that each part of the code is focused on a specific task, making it easier to understand and maintain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Load token_df from the TSV file\n",
        "input_file_path = '/content/osm-cca-nlp/csv/token_data.tsv'\n",
        "input_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/csv/token_data.tsv'\n",
        "token_df = pd.read_csv(input_file_path, sep='\\t')\n",
        "\n",
        "# Filter the DataFrame to keep only rows where the part of speech is 'NOUN'\n",
        "noun_df = token_df[token_df['token_pos'] == 'NOUN']\n",
        "\n",
        "# Group by the lemma and count the occurrences of each lemma\n",
        "lemma_counts = noun_df['token_lemma'].value_counts().reset_index()\n",
        "\n",
        "# Rename the columns for clarity\n",
        "lemma_counts.columns = ['lemma', 'count']\n",
        "\n",
        "# Get the 20 most frequent lemmas\n",
        "top_lemmas = lemma_counts.head(20)\n",
        "\n",
        "# Plot the 20 most frequent nouns using Seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x='count', y='lemma', data=top_lemmas, palette='viridis')\n",
        "plt.title('Top 20 Most Frequent Nouns')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Lemma')\n",
        "\n",
        "# Save the figure to a PNG file\n",
        "output_file_path = '/content/osm-cca-nlp/fig/token_noun.png'\n",
        "output_file_path = '/home/sol-nhl/rnd/d/quarto/osm-cca-nlp/fig/token_noun.png'\n",
        "plt.savefig(output_file_path)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## text vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model.. or en_core_web_sm if you donâ€™t need word embeddings\n",
        "#nlp = spacy.load(\"en_core_web_md\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        # Remove stopwords, punctuation, and non-alphabetic tokens\n",
        "        if not token.is_stop and not token.is_punct and token.is_alpha:\n",
        "            tokens.append(token.lemma_.lower())  # Append lemmatized form and lowercase\n",
        "    return tokens\n",
        "\n",
        "# Generate news headlines with some words occurring twice in the same headline\n",
        "headlines = [\n",
        "    \"Apple launches new iPhone iPhone in September\",\n",
        "    \"Google announces AI advancements with AI in health sector\",\n",
        "    \"Tesla's electric cars revolutionize the electric car industry\",\n",
        "    \"Amazon announces new grocery delivery for grocery stores\",\n",
        "    \"Netflix announces new series based on new AI-based technology\",\n",
        "    \"Microsoft launches cloud services and cloud infrastructure\",\n",
        "    \"Facebook unveils privacy controls with enhanced privacy features\",\n",
        "    \"Pfizer launches vaccine trials for new vaccine prevention\",\n",
        "    \"Nike launches new eco-friendly shoe and shoe design\",\n",
        "    \"BMW announces electric car breakthrough in the electric vehicle market\"\n",
        "]\n",
        "\n",
        "# Preprocess the list of headlines\n",
        "preprocessed_headlines = [\" \".join(preprocess_text(headline)) for headline in headlines]\n",
        "print(preprocessed_headlines)  # Output preprocessed headlines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import TfidfModel\n",
        "\n",
        "# Tokenize preprocessed headlines\n",
        "tokenized_headlines = [headline.split() for headline in preprocessed_headlines]\n",
        "\n",
        "# Create a dictionary of words\n",
        "dictionary = corpora.Dictionary(tokenized_headlines)\n",
        "\n",
        "# Create a Bag of Words corpus\n",
        "bow_corpus = [dictionary.doc2bow(text) for text in tokenized_headlines]\n",
        "\n",
        "# Or, alternatively, create a TF-IDF corpus\n",
        "tfidf = TfidfModel(bow_corpus)\n",
        "tfidf_corpus = tfidf[bow_corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from gensim import corpora\n",
        "from gensim.models import TfidfModel\n",
        "\n",
        "# Assuming bow_corpus is already defined and dictionary is available\n",
        "# dictionary = corpora.Dictionary(tokenized_headlines)\n",
        "\n",
        "# Create a list of terms (vocabulary) from the dictionary\n",
        "terms = [dictionary[i] for i in range(len(dictionary))]\n",
        "\n",
        "# Create a document-term matrix for the Bag of Words (BoW) corpus\n",
        "bow_doc_term_matrix = pd.DataFrame([[dict(doc).get(i, 0) for doc in bow_corpus] for i in range(len(dictionary))], index=terms)\n",
        "\n",
        "# Display the BoW document-term matrix\n",
        "#tools.display_dataframe_to_user(name=\"BoW Document-Term Matrix\", dataframe=bow_doc_term_matrix)\n",
        "print(bow_doc_term_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}