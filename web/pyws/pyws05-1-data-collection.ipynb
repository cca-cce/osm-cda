{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "  output: true\n",
        "title: \"web scraping\"\n",
        "---\n",
        "\n",
        "- code examples [beatifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/){target=\"_blank\"}\n",
        "- code examples [scrapy](https://docs.scrapy.org/en/latest/intro/tutorial.html){target=\"_blank\"}\n",
        "- code examples [playwright](https://playwright.dev/python/docs/intro){target=\"_blank\"}\n",
        "- download [jupyter notebook](pyws05-1-data-collection.ipynb){target=\"_blank\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# run inside google colab\n",
        "#!git clone https://github.com/cca-cce/osm-cdc.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- example [sustainability communication](https://www.lunduniversity.lu.se/about-university/university-glance/mission-vision-and-values/sustainability){target=\"_blank\"}\n",
        "\n",
        "## ethical scraping, robots.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import re\n",
        "\n",
        "# Function to fetch and parse robots.txt into a DataFrame\n",
        "def parse_robots_txt(robots_url):\n",
        "    response = requests.get(robots_url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error fetching robots.txt from {robots_url}\")\n",
        "        return None\n",
        "    lines = response.text.splitlines()\n",
        "    data = []\n",
        "    current_user_agent = None\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith('#'):\n",
        "            continue  # Skip empty lines and comments\n",
        "        if line.lower().startswith('user-agent:'):\n",
        "            current_user_agent = line.split(':', 1)[1].strip()\n",
        "        elif line.lower().startswith(('disallow:', 'allow:')):\n",
        "            directive, path = line.split(':', 1)\n",
        "            directive = directive.strip()\n",
        "            path = path.strip()\n",
        "            data.append({\n",
        "                'User-agent': current_user_agent,\n",
        "                'Directive': directive.capitalize(),\n",
        "                'Path': path\n",
        "            })\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# Function to check if a URL is allowed or disallowed\n",
        "def is_url_allowed(url, df, user_agent='*'):\n",
        "    parsed_url = urlparse(url)\n",
        "    path = parsed_url.path\n",
        "    # Filter rules for the specific user-agent or '*'\n",
        "    user_agent_rules = df[df['User-agent'].isin([user_agent, '*'])]\n",
        "\n",
        "    # Initialize default permission\n",
        "    permission = 'Allow'\n",
        "\n",
        "    # Process rules in order\n",
        "    for _, row in user_agent_rules.iterrows():\n",
        "        rule_path = row['Path']\n",
        "        directive = row['Directive']\n",
        "        # Convert wildcard patterns to regular expressions\n",
        "        rule_regex = '^' + re.escape(rule_path).replace(r'\\*', '.*') + '$'\n",
        "        if re.match(rule_regex, path):\n",
        "            permission = directive\n",
        "    return permission == 'Allow'\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the URL of the robots.txt file\n",
        "    base_url = 'https://www.example.com'  # Replace with the base URL\n",
        "    base_url = 'https://www.lunduniversity.lu.se'  # Replace with the base URL\n",
        "    robots_url = urljoin(base_url, '/robots.txt')\n",
        "\n",
        "    # Parse the robots.txt file into a DataFrame\n",
        "    robots_df = parse_robots_txt(robots_url)\n",
        "\n",
        "    if robots_df is not None:\n",
        "        print(\"Robots.txt rules:\")\n",
        "        print(robots_df)\n",
        "\n",
        "        # URL to check\n",
        "        url_to_check = 'https://www.example.com/some/path'  # Replace with the URL you want to check\n",
        "        url_to_check = 'https://www.lunduniversity.lu.se/about-university/university-glance/mission-vision-and-values/sustainability'  # Replace with the URL you want to check\n",
        "\n",
        "        # Check if the URL is allowed\n",
        "        is_allowed = is_url_allowed(url_to_check, robots_df)\n",
        "\n",
        "        if is_allowed:\n",
        "            print(f\"The URL {url_to_check} is allowed to be scraped.\")\n",
        "        else:\n",
        "            print(f\"The URL {url_to_check} is disallowed to be scraped according to robots.txt.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## beautifulsoup, google colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "import re\n",
        "\n",
        "# Step 1: Define the URL to scrape\n",
        "url = 'https://www.example.com'  # Replace with the target URL\n",
        "\n",
        "# Step 2: Fetch the HTML content\n",
        "response = requests.get(url)\n",
        "if response.status_code != 200:\n",
        "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
        "    exit()\n",
        "\n",
        "html_content = response.text\n",
        "\n",
        "# Step 3: Parse the HTML content with BeautifulSoup\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Step 4: Extract all link URLs to a DataFrame called links_df\n",
        "links = []\n",
        "for link in soup.find_all('a', href=True):\n",
        "    href = link['href']\n",
        "    full_url = urljoin(url, href)\n",
        "    links.append(full_url)\n",
        "\n",
        "links_df = pd.DataFrame({'url': links})\n",
        "\n",
        "# Step 5: Extract all image URLs to a DataFrame called images_df\n",
        "images = []\n",
        "for img in soup.find_all('img', src=True):\n",
        "    src = img['src']\n",
        "    full_url = urljoin(url, src)\n",
        "    images.append(full_url)\n",
        "\n",
        "images_df = pd.DataFrame({'image_url': images})\n",
        "\n",
        "# Step 6: Extract the main text as markdown to texts_df\n",
        "# We'll try to exclude navigation, footer, and other UI elements\n",
        "\n",
        "# Attempt to find the main content\n",
        "main_content = soup.find('main')\n",
        "if not main_content:\n",
        "    # If there's no <main> tag, find the largest <div> or <article>\n",
        "    candidates = soup.find_all(['div', 'article'], recursive=True)\n",
        "    main_content = max(candidates, key=lambda el: len(el.get_text(strip=True)), default=None)\n",
        "\n",
        "if main_content:\n",
        "    # Remove script and style elements\n",
        "    for script_or_style in main_content(['script', 'style']):\n",
        "        script_or_style.decompose()\n",
        "\n",
        "    # Get text\n",
        "    text_content = main_content.get_text(separator='\\n', strip=True)\n",
        "\n",
        "    # Optionally, clean up the text\n",
        "    text_content = re.sub('\\n+', '\\n', text_content).strip()\n",
        "else:\n",
        "    text_content = ''\n",
        "\n",
        "texts_df = pd.DataFrame({'text': [text_content]})\n",
        "\n",
        "# Step 7: Save all DataFrames as TSV files\n",
        "links_df.to_csv('links.tsv', sep='\\t', index=False)\n",
        "images_df.to_csv('images.tsv', sep='\\t', index=False)\n",
        "texts_df.to_csv('texts.tsv', sep='\\t', index=False)\n",
        "\n",
        "print(\"DataFrames have been saved as TSV files.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### **Explanation:**\n",
        "\n",
        "**Step 1: Define the URL to Scrape**\n",
        "\n",
        "We start by specifying the URL of the page we want to scrape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "url = 'https://www.example.com'  # Replace with the target URL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2: Fetch the HTML Content**\n",
        "\n",
        "We use the `requests` library to send an HTTP GET request to the URL. If the request is successful (status code 200), we proceed; otherwise, we exit the script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "response = requests.get(url)\n",
        "if response.status_code != 200:\n",
        "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
        "    exit()\n",
        "\n",
        "html_content = response.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3: Parse the HTML Content with BeautifulSoup**\n",
        "\n",
        "We parse the HTML content using BeautifulSoup, which allows us to navigate and extract data from the HTML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "soup = BeautifulSoup(html_content, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 4: Extract All Link URLs to a DataFrame Called `links_df`**\n",
        "\n",
        "We find all `<a>` tags with an `href` attribute, resolve relative URLs to absolute URLs using `urljoin`, and store them in a list. We then create a DataFrame from this list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "links = []\n",
        "for link in soup.find_all('a', href=True):\n",
        "    href = link['href']\n",
        "    full_url = urljoin(url, href)\n",
        "    links.append(full_url)\n",
        "\n",
        "links_df = pd.DataFrame({'url': links})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 5: Extract All Image URLs to a DataFrame Called `images_df`**\n",
        "\n",
        "Similarly, we find all `<img>` tags with a `src` attribute, resolve URLs, and store them in a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "images = []\n",
        "for img in soup.find_all('img', src=True):\n",
        "    src = img['src']\n",
        "    full_url = urljoin(url, src)\n",
        "    images.append(full_url)\n",
        "\n",
        "images_df = pd.DataFrame({'image_url': images})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 6: Extract the Main Text as Markdown to `texts_df`**\n",
        "\n",
        "Extracting the main content while excluding navigation, footer, and other UI elements can be challenging. We attempt the following:\n",
        "\n",
        "- **Check for a `<main>` Tag:** Many modern websites use the `<main>` tag to wrap the main content.\n",
        "- **Fallback to the Largest `<div>` or `<article>` Tag:** If no `<main>` tag is found, we select the `<div>` or `<article>` with the most text content.\n",
        "- **Clean the Text:** We remove any `<script>` or `<style>` tags from the main content and extract the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Attempt to find the main content\n",
        "main_content = soup.find('main')\n",
        "if not main_content:\n",
        "    # If there's no <main> tag, find the largest <div> or <article>\n",
        "    candidates = soup.find_all(['div', 'article'], recursive=True)\n",
        "    main_content = max(candidates, key=lambda el: len(el.get_text(strip=True)), default=None)\n",
        "\n",
        "if main_content:\n",
        "    # Remove script and style elements\n",
        "    for script_or_style in main_content(['script', 'style']):\n",
        "        script_or_style.decompose()\n",
        "\n",
        "    # Get text\n",
        "    text_content = main_content.get_text(separator='\\n', strip=True)\n",
        "\n",
        "    # Optionally, clean up the text\n",
        "    text_content = re.sub('\\n+', '\\n', text_content).strip()\n",
        "else:\n",
        "    text_content = ''\n",
        "\n",
        "texts_df = pd.DataFrame({'text': [text_content]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** This method provides an approximation. For more accurate extraction, consider using libraries like **Readability** or **Boilerpipe** that are designed to extract the main content from web pages.\n",
        "\n",
        "**Step 7: Save All DataFrames as TSV Files**\n",
        "\n",
        "We save each DataFrame to a TSV (Tab-Separated Values) file using pandas' `to_csv` method with `sep='\\t'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "links_df.to_csv('links.tsv', sep='\\t', index=False)\n",
        "images_df.to_csv('images.tsv', sep='\\t', index=False)\n",
        "texts_df.to_csv('texts.tsv', sep='\\t', index=False)\n",
        "\n",
        "print(\"DataFrames have been saved as TSV files.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### **Additional Notes:**\n",
        "\n",
        "- **Dependencies:**\n",
        "  - Ensure you have the required libraries installed:\n",
        "\n",
        "    ```bash\n",
        "    pip install requests beautifulsoup4 pandas\n",
        "    ```\n",
        "\n",
        "- **URL Replacement:**\n",
        "  - Replace `'https://www.example.com'` with the actual URL you wish to scrape.\n",
        "\n",
        "- **Handling Relative URLs:**\n",
        "  - The `urljoin` function from `urllib.parse` resolves relative URLs based on the base URL.\n",
        "\n",
        "- **Data Cleaning:**\n",
        "  - The code removes extra newline characters in the extracted text.\n",
        "  - For more advanced text cleaning and markdown conversion, consider using additional libraries like `markdownify`.\n",
        "\n",
        "- **Legal and Ethical Considerations:**\n",
        "  - **Respect Robots.txt:** Ensure that you are allowed to scrape the website by checking its `robots.txt` file.\n",
        "  - **Terms of Service:** Review the website's terms of service to make sure scraping is permitted.\n",
        "  - **Rate Limiting:** Be considerate and do not overwhelm the server with too many requests in a short period.\n",
        "\n",
        "- **Error Handling:**\n",
        "  - The script checks if the page was successfully retrieved before proceeding.\n",
        "  - You can add more error handling as needed.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Usage:**\n",
        "\n",
        "Suppose we set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "url = 'https://www.python.org/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After running the script, you would get:\n",
        "\n",
        "- **links.tsv:** A TSV file containing all the links found on the Python homepage.\n",
        "- **images.tsv:** A TSV file containing all the image URLs.\n",
        "- **texts.tsv:** A TSV file containing the main textual content of the page.\n",
        "\n",
        "---\n",
        "\n",
        "**Feel free to customize the script further to suit your specific needs!**\n",
        "\n",
        "## scrapy, github codespaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# scrapy_spider.py\n",
        "\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from urllib.parse import urljoin\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "class MySpider(scrapy.Spider):\n",
        "    name = \"my_spider\"\n",
        "\n",
        "    # Start URL\n",
        "    start_urls = ['https://www.example.com']  # Replace with your target URL\n",
        "\n",
        "    # Initialize data storage\n",
        "    links = []\n",
        "    resources = []\n",
        "    texts = []\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Extract all link URLs\n",
        "        link_urls = response.css('a::attr(href)').getall()\n",
        "        for href in link_urls:\n",
        "            full_url = urljoin(response.url, href)\n",
        "            self.links.append(full_url)\n",
        "\n",
        "        # Extract all binary file URLs (images, .pdf, .docx)\n",
        "        # Define patterns for binary files\n",
        "        binary_patterns = re.compile(r'.*\\.(jpg|jpeg|png|gif|bmp|tiff|pdf|docx|doc)$', re.IGNORECASE)\n",
        "        resource_urls = []\n",
        "\n",
        "        # Extract image URLs\n",
        "        img_urls = response.css('img::attr(src)').getall()\n",
        "        resource_urls.extend(img_urls)\n",
        "\n",
        "        # Include URLs from link tags that point to binary files\n",
        "        for href in link_urls:\n",
        "            if binary_patterns.match(href):\n",
        "                resource_urls.append(href)\n",
        "\n",
        "        # Resolve and store resource URLs\n",
        "        for res_url in resource_urls:\n",
        "            full_res_url = urljoin(response.url, res_url)\n",
        "            if binary_patterns.match(full_res_url):\n",
        "                self.resources.append(full_res_url)\n",
        "\n",
        "        # Extract main text\n",
        "        main_text = self.extract_main_text(response)\n",
        "        self.texts.append(main_text)\n",
        "\n",
        "    def extract_main_text(self, response):\n",
        "        # Try to find <main> tag content\n",
        "        main_content = response.css('main')\n",
        "        if not main_content:\n",
        "            # Fallback to the largest <div> or <article> tag\n",
        "            candidates = response.css('div, article')\n",
        "            if candidates:\n",
        "                main_content = max(candidates, key=lambda el: len(el.get()), default=None)\n",
        "            else:\n",
        "                main_content = None\n",
        "        else:\n",
        "            main_content = main_content[0]\n",
        "\n",
        "        if main_content:\n",
        "            # Remove script and style tags\n",
        "            main_content = main_content.xpath('.//*[not(self::script or self::style)]')\n",
        "            text = main_content.xpath('.//text()').getall()\n",
        "            text = [t.strip() for t in text if t.strip()]\n",
        "            text_content = '\\n'.join(text)\n",
        "        else:\n",
        "            text_content = ''\n",
        "\n",
        "        return text_content\n",
        "\n",
        "    def closed(self, reason):\n",
        "        # Save data to dataframes\n",
        "        links_df = pd.DataFrame({'url': self.links})\n",
        "        resources_df = pd.DataFrame({'resource_url': self.resources})\n",
        "        texts_df = pd.DataFrame({'text': self.texts})\n",
        "\n",
        "        # Save dataframes to TSV files\n",
        "        links_df.to_csv('links.tsv', sep='\\t', index=False)\n",
        "        resources_df.to_csv('resources.tsv', sep='\\t', index=False)\n",
        "        texts_df.to_csv('texts.tsv', sep='\\t', index=False)\n",
        "\n",
        "        print(\"DataFrames have been saved as TSV files.\")\n",
        "\n",
        "# Run the spider\n",
        "if __name__ == \"__main__\":\n",
        "    process = CrawlerProcess({\n",
        "        'ROBOTSTXT_OBEY': True,\n",
        "    })\n",
        "    process.crawl(MySpider)\n",
        "    process.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation:\n",
        "\n",
        "**Imports and Setup:**\n",
        "\n",
        "- **scrapy**: The main library for web scraping.\n",
        "- **CrawlerProcess**: To run the spider without using the command line.\n",
        "- **urljoin**: To resolve relative URLs.\n",
        "- **pandas**: To create and manipulate DataFrames.\n",
        "- **re**: For regular expressions.\n",
        "\n",
        "**Define the Spider Class:**\n",
        "\n",
        "- **name**: Name of the spider.\n",
        "- **start_urls**: List containing the starting URL(s) for the spider.\n",
        "- **Data Storage**: Initialize lists to store links, resources, and texts.\n",
        "\n",
        "**Parsing the Response:**\n",
        "\n",
        "- **Extract All Link URLs:**\n",
        "\n",
        "  - Use CSS selectors to get all `<a>` tags with `href` attributes.\n",
        "  - Resolve relative URLs using `urljoin`.\n",
        "  - Store the full URLs in the `links` list.\n",
        "\n",
        "- **Extract All Binary File URLs:**\n",
        "\n",
        "  - Define a regular expression pattern to match image files and documents.\n",
        "  - Extract `src` attributes from `<img>` tags.\n",
        "  - Include URLs from `<a>` tags that point directly to binary files.\n",
        "  - Resolve and store the full resource URLs in the `resources` list.\n",
        "\n",
        "- **Extract Main Text:**\n",
        "\n",
        "  - Use the `extract_main_text` method.\n",
        "  - Attempt to extract content from the `<main>` tag.\n",
        "  - If `<main>` tag is not found, fallback to the largest `<div>` or `<article>` tag.\n",
        "  - Remove `<script>` and `<style>` tags.\n",
        "  - Extract text content.\n",
        "  - Store the text in the `texts` list.\n",
        "\n",
        "**Closing the Spider:**\n",
        "\n",
        "- When the spider finishes, the `closed` method is called.\n",
        "- Convert the lists to pandas DataFrames.\n",
        "- Save each DataFrame to a TSV file.\n",
        "\n",
        "**Running the Spider:**\n",
        "\n",
        "- Use `CrawlerProcess` to run the spider programmatically.\n",
        "- Set `ROBOTSTXT_OBEY` to `True` to respect `robots.txt`.\n",
        "- Call `process.start()` to begin crawling.\n",
        "\n",
        "### How to Use:\n",
        "\n",
        "1. **Install Scrapy and Pandas:**\n",
        "\n",
        "   ```bash\n",
        "   pip install scrapy pandas\n",
        "   ```\n",
        "\n",
        "2. **Save the Script:**\n",
        "\n",
        "   - Save the code above in a file named `scrapy_spider.py`.\n",
        "\n",
        "3. **Replace the URL:**\n",
        "\n",
        "   - In the `start_urls` list, replace `'https://www.example.com'` with the URL you wish to scrape.\n",
        "\n",
        "4. **Run the Script:**\n",
        "\n",
        "   ```bash\n",
        "   python scrapy_spider.py\n",
        "   ```\n",
        "\n",
        "5. **Check the Output:**\n",
        "\n",
        "   - After the spider finishes, you will find three TSV files in the same directory:\n",
        "     - `links.tsv`\n",
        "     - `resources.tsv`\n",
        "     - `texts.tsv`\n",
        "\n",
        "### Additional Notes:\n",
        "\n",
        "- **Legal and Ethical Considerations:**\n",
        "\n",
        "  - **Respect Robots.txt:** The code sets `ROBOTSTXT_OBEY` to `True` to ensure compliance with the website's `robots.txt` policies.\n",
        "\n",
        "  - **Terms of Service:** Before scraping, check the website's terms of service to ensure that scraping is permitted.\n",
        "\n",
        "  - **Rate Limiting:** Be considerate and do not overload the server with too many requests. You can set a download delay:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "    "
      },
      "source": [
        "process = CrawlerProcess({\n",
        "    'ROBOTSTXT_OBEY': True,\n",
        "    'DOWNLOAD_DELAY': 1,  # Delay of 1 second between requests\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Scrapy Settings:**\n",
        "\n",
        "  - You can customize the Scrapy settings by passing a dictionary to `CrawlerProcess`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "    "
      },
      "source": [
        "process = CrawlerProcess({\n",
        "    'USER_AGENT': 'my-scraper (https://www.mywebsite.com)',\n",
        "    'ROBOTSTXT_OBEY': True,\n",
        "    'DOWNLOAD_DELAY': 1,\n",
        "    'LOG_LEVEL': 'INFO',  # Set to 'DEBUG' for more details\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Handling JavaScript-rendered Content:**\n",
        "\n",
        "  - Scrapy does not execute JavaScript. If the page content is loaded via JavaScript, consider using tools like **Scrapy Splash** or **Selenium** with Scrapy.\n",
        "\n",
        "- **Improving Text Extraction:**\n",
        "\n",
        "  - For better extraction of the main text, consider using libraries like `readability-lxml`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "    "
      },
      "source": [
        "from readability import Document\n",
        "\n",
        "def extract_main_text(self, response):\n",
        "    doc = Document(response.text)\n",
        "    main_html = doc.summary()\n",
        "    main_soup = scrapy.Selector(text=main_html)\n",
        "    text = main_soup.xpath('//text()').getall()\n",
        "    text = [t.strip() for t in text if t.strip()]\n",
        "    text_content = '\\n'.join(text)\n",
        "    return text_content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  - **Note:** You'll need to install `readability-lxml`:\n",
        "\n",
        "    ```bash\n",
        "    pip install readability-lxml\n",
        "    ```\n",
        "\n",
        "- **Single Page Crawl:**\n",
        "\n",
        "  - The spider as written only crawls the single page specified in `start_urls`. If you want to follow links and crawl multiple pages, you need to adjust the `parse` method to yield new requests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "    "
      },
      "source": [
        "for href in link_urls:\n",
        "    full_url = urljoin(response.url, href)\n",
        "    yield scrapy.Request(full_url, callback=self.parse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  - Be cautious with recursive crawling to avoid crawling the entire website unintentionally.\n",
        "\n",
        "- **Avoid Duplicates:**\n",
        "\n",
        "  - Ensure that you're not collecting duplicate URLs by checking if a URL is already in your list before adding it.\n",
        "\n",
        "- **Data Cleaning:**\n",
        "\n",
        "  - You may need to clean the extracted data further, depending on your requirements.\n",
        "\n",
        "### Important Considerations:\n",
        "\n",
        "- **Ethical Scraping:**\n",
        "\n",
        "  - Always scrape responsibly by following the website's `robots.txt` rules and terms of service.\n",
        "  - Use a meaningful `User-Agent` string to identify your scraper.\n",
        "\n",
        "- **Robustness:**\n",
        "\n",
        "  - Add error handling to manage network issues or unexpected HTML structures.\n",
        "\n",
        "- **Testing:**\n",
        "\n",
        "  - Test your scraper on a small scale before running it on larger sites.\n",
        "\n",
        "**Disclaimer:**\n",
        "\n",
        "- **Use Responsibly:** Ensure that you use this code responsibly and ethically, complying with all applicable laws and website policies.\n",
        "\n",
        "---\n",
        "\n",
        "**Feel free to ask if you need further assistance or modifications to the code!**\n",
        "\n",
        "## web browser automation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!pip install pytest-playwright\n",
        "#!playwright install\n",
        "#!pip install pytest-playwright playwright -U\n",
        "\n",
        "#!pip install playwright\n",
        "#!playwright install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# screenshot.py\n",
        "\n",
        "from playwright.sync_api import sync_playwright\n",
        "\n",
        "def take_full_page_screenshot(url, output_path):\n",
        "    with sync_playwright() as p:\n",
        "        # Launch the browser (Chromium in headless mode)\n",
        "        browser = p.chromium.launch()\n",
        "        page = browser.new_page()\n",
        "        # Navigate to the specified URL\n",
        "        page.goto(url)\n",
        "        # Wait until the page is fully loaded\n",
        "        page.wait_for_load_state('networkidle')\n",
        "        # Take a full-page screenshot\n",
        "        page.screenshot(path=output_path, full_page=True)\n",
        "        # Close the browser\n",
        "        browser.close()\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    url = 'https://www.example.com'  # Replace with your target URL\n",
        "    output_path = 'full_screenshot.png'  # Output file path\n",
        "    take_full_page_screenshot(url, output_path)\n",
        "    print(f\"Screenshot saved to {output_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### **Explanation**\n",
        "\n",
        "#### **Imports and Setup**\n",
        "\n",
        "- **Import `sync_playwright`**: We import `sync_playwright` from `playwright.sync_api` to use Playwright in synchronous mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "  "
      },
      "source": [
        "from playwright.sync_api import sync_playwright"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Define the Function**\n",
        "\n",
        "- **Function `take_full_page_screenshot`**: This function takes a URL and an output file path as arguments and captures a full-page screenshot of the specified URL.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "  "
      },
      "source": [
        "def take_full_page_screenshot(url, output_path):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Start Playwright and Launch the Browser**\n",
        "\n",
        "- **Use `sync_playwright()` Context Manager**: This ensures that Playwright starts and stops correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "  "
      },
      "source": [
        "with sync_playwright() as p:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Launch the Browser**: We use Chromium in headless mode by default. You can choose `'chromium'`, `'firefox'`, or `'webkit'`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "  "
      },
      "source": [
        "browser = p.chromium.launch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Create a New Page**: Open a new browser page (tab).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "  "
      },
      "source": [
        "page = browser.new_page()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Navigate to the URL**\n",
        "\n",
        "- **Go to the Specified URL**: Navigate the browser to the target URL.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "  "
      },
      "source": [
        "page.goto(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Wait for the Page to Load**: Ensure the page is fully loaded before taking the screenshot.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "  "
      },
      "source": [
        "page.wait_for_load_state('networkidle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Take the Full-Page Screenshot**\n",
        "\n",
        "- **Capture the Screenshot**: Use the `screenshot` method with `full_page=True` to capture the entire page.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "  "
      },
      "source": [
        "page.screenshot(path=output_path, full_page=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Close the Browser**\n",
        "\n",
        "- **Close the Browser**: Clean up by closing the browser instance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "  "
      },
      "source": [
        "browser.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Usage Example**\n",
        "\n",
        "- **Provide an Example Usage**: Demonstrate how to call the function with a specific URL and output path.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "  "
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    url = 'https://www.example.com'  # Replace with your target URL\n",
        "    output_path = 'full_screenshot.png'  # Output file path\n",
        "    take_full_page_screenshot(url, output_path)\n",
        "    print(f\"Screenshot saved to {output_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### **Step-by-Step Guide**\n",
        "\n",
        "#### **1. Install Playwright**\n",
        "\n",
        "First, you need to install Playwright and its dependencies.\n",
        "\n",
        "```bash\n",
        "pip install playwright\n",
        "```\n",
        "\n",
        "Then, install the browser binaries:\n",
        "\n",
        "```bash\n",
        "playwright install\n",
        "```\n",
        "\n",
        "This command installs Chromium, Firefox, and WebKit browsers needed by Playwright.\n",
        "\n",
        "#### **2. Save the Script**\n",
        "\n",
        "Save the code above in a file named `screenshot.py`.\n",
        "\n",
        "#### **3. Replace the URL**\n",
        "\n",
        "In the script, replace `'https://www.example.com'` with the URL of the page you want to capture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "url = 'https://www.your-target-website.com'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **4. Run the Script**\n",
        "\n",
        "Execute the script from the command line:\n",
        "\n",
        "```bash\n",
        "python screenshot.py\n",
        "```\n",
        "\n",
        "#### **5. Check the Output**\n",
        "\n",
        "After running the script, you should find a file named `full_screenshot.png` in the same directory. This image is the full-page screenshot of the specified URL.\n",
        "\n",
        "---\n",
        "\n",
        "### **Additional Options**\n",
        "\n",
        "#### **Specify Browser Options**\n",
        "\n",
        "You can customize the browser launch options, such as running in headless or headful mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "browser = p.chromium.launch(headless=False)  # Set headless=False to see the browser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Set Viewport Size**\n",
        "\n",
        "If you need to set a specific viewport size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "page = browser.new_page(viewport={'width': 1280, 'height': 720})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Authentication and Headers**\n",
        "\n",
        "If the page requires authentication or specific headers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "page.goto(url, headers={'Authorization': 'Bearer YOUR_TOKEN'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Delay for Dynamic Content**\n",
        "\n",
        "If the page loads content dynamically, you may need to wait before taking the screenshot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "page.goto(url)\n",
        "page.wait_for_timeout(5000)  # Wait for 5 seconds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or wait for a specific element:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "page.goto(url)\n",
        "page.wait_for_selector('#content')  # Wait for an element with id 'content'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### **Notes and Best Practices**\n",
        "\n",
        "- **Error Handling**: You can add try-except blocks to handle exceptions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "  "
      },
      "source": [
        "try:\n",
        "    # Your code\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Headless vs. Headful**: Running in headless mode (`headless=True`) means the browser runs in the background without a GUI. Setting `headless=False` allows you to see the browser actions.\n",
        "\n",
        "- **Resource Cleanup**: Using the context manager ensures resources are properly cleaned up after execution.\n",
        "\n",
        "- **Playwright Documentation**: For more advanced usage, refer to the official Playwright Python documentation: [Playwright for Python](https://playwright.dev/python/docs/intro)\n",
        "\n",
        "---\n",
        "\n",
        "### **Example with Advanced Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from playwright.sync_api import sync_playwright\n",
        "\n",
        "def take_full_page_screenshot(url, output_path):\n",
        "    with sync_playwright() as p:\n",
        "        # Launch the browser in headless mode\n",
        "        browser = p.chromium.launch(headless=True)\n",
        "        # Create a new browser context with a specific viewport size\n",
        "        context = browser.new_context(\n",
        "            viewport={'width': 1920, 'height': 1080}\n",
        "        )\n",
        "        page = context.new_page()\n",
        "        # Navigate to the URL\n",
        "        page.goto(url)\n",
        "        # Wait for network to be idle\n",
        "        page.wait_for_load_state('networkidle')\n",
        "        # Wait for any animations to finish\n",
        "        page.wait_for_timeout(1000)  # Wait for 1 second\n",
        "        # Hide any unwanted elements (e.g., cookie banners)\n",
        "        page.evaluate(\"() => { const banners = document.querySelectorAll('.cookie-banner'); banners.forEach(b => b.style.display = 'none'); }\")\n",
        "        # Take a full-page screenshot\n",
        "        page.screenshot(path=output_path, full_page=True)\n",
        "        # Close the browser\n",
        "        browser.close()\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    url = 'https://www.example.com'  # Replace with your target URL\n",
        "    output_path = 'full_screenshot.png'  # Output file path\n",
        "    take_full_page_screenshot(url, output_path)\n",
        "    print(f\"Screenshot saved to {output_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation of Additional Features:**\n",
        "\n",
        "- **Viewport Size**: Setting the viewport to a larger size can capture more content.\n",
        "\n",
        "- **Waiting for Animations**: Adding a timeout after page load can ensure that any animations or dynamic content have completed.\n",
        "\n",
        "- **Hiding Elements**: Use `evaluate` to run JavaScript in the page context to hide unwanted elements like pop-ups or banners.\n",
        "\n",
        "---\n",
        "\n",
        "### **Common Issues and Troubleshooting**\n",
        "\n",
        "- **Timeout Errors**: If the page takes too long to load, you might encounter timeout errors. Increase the timeout using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "  "
      },
      "source": [
        "page.goto(url, timeout=60000)  # Wait up to 60 seconds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **SSL Certificate Errors**: For pages with SSL issues, you can ignore HTTPS errors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "  "
      },
      "source": [
        "browser = p.chromium.launch(args=['--ignore-certificate-errors'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Authentication Required**: For pages behind authentication, provide credentials:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "  "
      },
      "source": [
        "context = browser.new_context(\n",
        "    http_credentials={'username': 'user', 'password': 'pass'}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "This script provides a simple way to use Playwright for Python to visit a URL and take a full-page screenshot, saving it as a PNG file. Playwright is a powerful tool for browser automation and supports advanced features that can be leveraged for more complex tasks.\n",
        "\n",
        "---\n",
        "\n",
        "**Remember to replace `'https://www.example.com'` with the actual URL you want to capture and ensure you comply with the website's terms of service and robots.txt policies when automating interactions.**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}