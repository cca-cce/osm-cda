{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  eval: true\n",
        "  echo: true\n",
        "  output: true\n",
        "title: \"object recognition\"\n",
        "---\n",
        "\n",
        "- code examples [huggingface](https://huggingface.co/tasks){target=\"_blank\"}\n",
        "- download [jupyter notebook](pyws04-2-image-analysis.ipynb){target=\"_blank\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# get github repo.. run inside google colab\n",
        "#!git clone https://github.com/cca-cce/osm-cca-cv.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# install dependencies\n",
        "#!pip install -q timm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## image classification "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import pipeline\n",
        "clf = pipeline(\"image-classification\")\n",
        "clf(\"/content/osm-cca-cv/res/img/image_prefix-000.png\")\n",
        "\n",
        "#[{'label': 'tabby cat', 'score': 0.731},\n",
        "#...\n",
        "#]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## object recognition "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model = pipeline(\"object-detection\")\n",
        "model(\"/content/osm-cca-cv/res/img/image_prefix-000.png\")\n",
        "# [{'label': 'blanket',\n",
        "#  'mask': mask_string,\n",
        "#  'score': 0.917},\n",
        "#...]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Original image path\n",
        "image_path = \"/content/osm-cca-cv/res/img/image_prefix-000.png\"\n",
        "\n",
        "# Read the image\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Check if the image was successfully loaded\n",
        "if image is None:\n",
        "    print(\"Error: Could not read image.\")\n",
        "else:\n",
        "    # List of detections\n",
        "    detections = [\n",
        "        {\n",
        "            'score': 0.8914425373077393,\n",
        "            'label': 'dog',\n",
        "            'box': {'xmin': 265, 'ymin': 187, 'xmax': 282, 'ymax': 204}\n",
        "        },\n",
        "        {\n",
        "            'score': 0.999186098575592,\n",
        "            'label': 'person',\n",
        "            'box': {'xmin': 237, 'ymin': 136, 'xmax': 303, 'ymax': 218}\n",
        "        },\n",
        "        {\n",
        "            'score': 0.9989272952079773,\n",
        "            'label': 'person',\n",
        "            'box': {'xmin': 352, 'ymin': 126, 'xmax': 385, 'ymax': 184}\n",
        "        },\n",
        "        {\n",
        "            'score': 0.999570906162262,\n",
        "            'label': 'person',\n",
        "            'box': {'xmin': 481, 'ymin': 150, 'xmax': 589, 'ymax': 344}\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Iterate over detections and draw bounding boxes\n",
        "    for detection in detections:\n",
        "        score = detection['score']\n",
        "        label = detection['label']\n",
        "        box = detection['box']\n",
        "        xmin = box['xmin']\n",
        "        ymin = box['ymin']\n",
        "        xmax = box['xmax']\n",
        "        ymax = box['ymax']\n",
        "\n",
        "        # Draw the bounding box\n",
        "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color=(0, 255, 0), thickness=2)\n",
        "\n",
        "        # Prepare label with score\n",
        "        text = f\"{label}: {score:.2f}\"\n",
        "\n",
        "        # Calculate text size for background rectangle\n",
        "        (text_width, text_height), baseline = cv2.getTextSize(\n",
        "            text, cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, thickness=1\n",
        "        )\n",
        "\n",
        "        # Position the text above the bounding box if possible\n",
        "        text_x = xmin\n",
        "        text_y = ymin - 10 if ymin - 10 > text_height else ymin + text_height + 10\n",
        "\n",
        "        # Draw background rectangle for text\n",
        "        cv2.rectangle(\n",
        "            image,\n",
        "            (text_x, text_y - text_height - baseline),\n",
        "            (text_x + text_width, text_y + baseline),\n",
        "            color=(0, 255, 0),\n",
        "            thickness=-1\n",
        "        )\n",
        "\n",
        "        # Put the text on the image\n",
        "        cv2.putText(\n",
        "            image,\n",
        "            text,\n",
        "            (text_x, text_y),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            fontScale=0.5,\n",
        "            color=(0, 0, 0),\n",
        "            thickness=1\n",
        "        )\n",
        "\n",
        "    # Convert BGR to RGB for displaying with matplotlib\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Display the image using matplotlib\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(image_rgb)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Alternatively, save the image to a file\n",
        "    output_path = \"/content/osm-cca-cv/res/img/image_with_detections.png\"\n",
        "    cv2.imwrite(output_path, image)\n",
        "    print(f\"Output image saved to {output_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:**\n",
        "\n",
        "- **Import Libraries:**\n",
        "  - `cv2` for image processing.\n",
        "  - `matplotlib.pyplot` for displaying the image inline if using a Jupyter notebook or Google Colab.\n",
        "\n",
        "- **Read the Original Image:**\n",
        "  - Use `cv2.imread()` to read the image from the specified path.\n",
        "  - Check if the image was loaded successfully.\n",
        "\n",
        "- **Define the Detections List:**\n",
        "  - This is the provided list containing detection results, each with a score, label, and bounding box coordinates.\n",
        "\n",
        "- **Iterate Over Detections and Draw Bounding Boxes:**\n",
        "  - Loop through each detection in the list.\n",
        "  - Extract the `score`, `label`, and `box` coordinates.\n",
        "  - Use `cv2.rectangle()` to draw the bounding box on the image.\n",
        "    - Parameters:\n",
        "      - Image to draw on.\n",
        "      - Starting point `(xmin, ymin)`.\n",
        "      - Ending point `(xmax, ymax)`.\n",
        "      - `color`: Set to green `(0, 255, 0)`.\n",
        "      - `thickness`: Set to `2`.\n",
        "  - Prepare the text to display the label and score.\n",
        "  - Calculate the size of the text using `cv2.getTextSize()` to create a background rectangle for better visibility.\n",
        "  - Draw a filled rectangle behind the text using `cv2.rectangle()`.\n",
        "  - Put the text on the image using `cv2.putText()`.\n",
        "\n",
        "- **Display or Save the Image:**\n",
        "  - Convert the image from BGR to RGB color space for correct color representation in Matplotlib.\n",
        "  - Use `plt.imshow()` to display the image inline.\n",
        "  - Alternatively, save the image with detections to a file using `cv2.imwrite()`.\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "- **Color Spaces:**\n",
        "  - OpenCV uses BGR color order, while Matplotlib uses RGB. Conversion is necessary for correct color display.\n",
        "\n",
        "- **Text Positioning:**\n",
        "  - The code attempts to place the label above the bounding box if there's enough space; otherwise, it places it below.\n",
        "\n",
        "- **Font and Text Styling:**\n",
        "  - Font: `cv2.FONT_HERSHEY_SIMPLEX`.\n",
        "  - Font scale and thickness are set for readability.\n",
        "\n",
        "- **Saving the Image:**\n",
        "  - The output image is saved to the specified path.\n",
        "\n",
        "**Dependencies:**\n",
        "\n",
        "- Ensure you have OpenCV installed:\n",
        "\n",
        "  ```bash\n",
        "  pip install opencv-python\n",
        "  ```\n",
        "\n",
        "- If using Jupyter Notebook or Google Colab, Matplotlib is typically pre-installed.\n",
        "\n",
        "**Example Output:**\n",
        "\n",
        "- The resulting image will display the original image with green bounding boxes around detected objects.\n",
        "- Labels and scores are displayed near each bounding box for identification.\n",
        "\n",
        "**Adjustments:**\n",
        "\n",
        "- **Image Path:**\n",
        "  - Ensure that `image_path` correctly points to your image file.\n",
        "  - If the image is not found, you'll receive an error message.\n",
        "\n",
        "- **Output Path:**\n",
        "  - The `output_path` should be adjusted if you want to save the image to a different location.\n",
        "\n",
        "- **Display Method:**\n",
        "  - If you're running this code outside of a notebook environment, you might need to use `cv2.imshow()` instead of Matplotlib to display the image.\n",
        "\n",
        "**Display with OpenCV (Alternative):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display the image using OpenCV's imshow (only works in local environments)\n",
        "cv2.imshow('Image with Detections', image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Note that `cv2.imshow()` may not work in notebook environments like Google Colab.\n",
        "\n",
        "## image, video to text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "captioner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n",
        "#captioner(\"https://huggingface.co/datasets/Narsil/image_dummy/resolve/main/parrots.png\")\n",
        "captioner(\"https://raw.githubusercontent.com/cca-cce/osm-cca-cv/0a62207580cf63bc808ec825669af8d6f2eedf16/res/img/image_prefix-000.png\")\n",
        "## [{'generated_text': 'two birds are standing next to each other '}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pixtral, cutting edge..\n",
        "\n",
        "[![huggingface account and gpu resources required](https://img.youtube.com/vi/7aGTKJJMb5w/0.jpg)](https://www.youtube.com/watch?v=7aGTKJJMb5w)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# install dependencies\n",
        "#!pip install -q --upgrade vllm\n",
        "#!pip install -q --upgrade mistral_common"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from vllm import LLM\n",
        "from vllm.sampling_params import SamplingParams\n",
        "\n",
        "model_name = \"mistralai/Pixtral-12B-2409\"\n",
        "\n",
        "sampling_params = SamplingParams(max_tokens=8192)\n",
        "\n",
        "llm = LLM(model=model_name, tokenizer_mode=\"mistral\")\n",
        "\n",
        "prompt = \"Describe this image in one sentence.\"\n",
        "image_url = \"https://raw.githubusercontent.com/cca-cce/osm-cca-cv/0a62207580cf63bc808ec825669af8d6f2eedf16/res/img/image_prefix-000.png\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": prompt}, {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}]\n",
        "    },\n",
        "]\n",
        "\n",
        "outputs = llm.chat(messages, sampling_params=sampling_params)\n",
        "\n",
        "print(outputs[0].outputs[0].text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## testing.. on your own risk :)\n",
        "\n",
        "To summarize and describe the visual content of a short video without audio using Python, you can follow these main steps:\n",
        "\n",
        "1. **Extract frames from the video.**\n",
        "2. **Select key frames that represent the content effectively.**\n",
        "3. **Use an image captioning model to generate descriptions of the frames.**\n",
        "4. **Combine the frame descriptions into a coherent summary.**\n",
        "\n",
        "Below is a detailed explanation of each step along with the necessary code.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Extract Frames from the Video\n",
        "\n",
        "*We use the `OpenCV` library to extract frames from the video file. Alternatively, `moviepy` can also be used for frame extraction.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Define the path to the video file\n",
        "video_file = 'path_to_your_video.mp4'\n",
        "\n",
        "# Create a directory to store the extracted frames\n",
        "frames_dir = 'extracted_frames'\n",
        "os.makedirs(frames_dir, exist_ok=True)\n",
        "\n",
        "# Load the video using OpenCV\n",
        "cap = cv2.VideoCapture(video_file)\n",
        "\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Decide on the interval between frames to extract (e.g., every 1 second)\n",
        "frame_interval = int(fps)  # Adjust this value as needed\n",
        "\n",
        "frame_number = 0\n",
        "extracted_frames = []\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if frame_number % frame_interval == 0:\n",
        "        frame_path = os.path.join(frames_dir, f'frame_{frame_number}.jpg')\n",
        "        cv2.imwrite(frame_path, frame)\n",
        "        extracted_frames.append(frame_path)\n",
        "    \n",
        "    frame_number += 1\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(f\"Extracted {len(extracted_frames)} frames.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Notes:**\n",
        "\n",
        "- **Frame Extraction Interval:**\n",
        "  - The `frame_interval` determines how frequently frames are extracted. For example, extracting one frame per second.\n",
        "  - Adjust `frame_interval` based on the video's FPS and the level of detail you need.\n",
        "\n",
        "- **Saving Frames:**\n",
        "  - Frames are saved as JPEG images in the `extracted_frames` directory.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Use an Image Captioning Model to Generate Descriptions\n",
        "\n",
        "*We use the `transformers` library from Hugging Face along with a pre-trained image captioning model to generate descriptions for each extracted frame.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# Load the pre-trained image captioning model and processor\n",
        "model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
        "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set the device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Function to generate caption for an image\n",
        "def generate_caption(image_path):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = pixel_values.to(device)\n",
        "\n",
        "    # Generate caption\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(pixel_values, max_length=50, num_beams=5)\n",
        "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# Generate captions for all extracted frames\n",
        "frame_captions = []\n",
        "\n",
        "for frame_path in extracted_frames:\n",
        "    caption = generate_caption(frame_path)\n",
        "    frame_captions.append({'frame': frame_path, 'caption': caption})\n",
        "    print(f\"{frame_path}: {caption}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Notes:**\n",
        "\n",
        "- **Model Selection:**\n",
        "  - We use the `nlpconnect/vit-gpt2-image-captioning` model, which is a Vision Transformer (ViT) encoder and GPT-2 decoder.\n",
        "  - You can explore other models on Hugging Face Hub if needed.\n",
        "\n",
        "- **Device Configuration:**\n",
        "  - The code checks for GPU availability and uses it if possible for faster processing.\n",
        "\n",
        "- **Caption Generation:**\n",
        "  - The `generate_caption` function processes an image and generates a caption.\n",
        "  - Adjust parameters like `max_length` and `num_beams` for different results.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Combine Captions into a Coherent Summary\n",
        "\n",
        "*We combine the generated captions to form a coherent summary of the video's visual content.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Combine captions into a summary\n",
        "summary_text = ' '.join([item['caption'] for item in frame_captions])\n",
        "\n",
        "print(\"\\nVideo Summary:\")\n",
        "print(summary_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Notes:**\n",
        "\n",
        "- **Simple Concatenation:**\n",
        "  - We concatenate the captions to create a rough summary.\n",
        "  - This method may result in repetitive or disjointed text.\n",
        "\n",
        "- **Advanced Summarization (Optional):**\n",
        "  - For a more coherent summary, you can use a text summarization model to refine the combined captions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the summarization pipeline\n",
        "summarizer = pipeline('summarization')\n",
        "\n",
        "# Summarize the combined captions\n",
        "final_summary = summarizer(summary_text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
        "\n",
        "print(\"\\nFinal Summary:\")\n",
        "print(final_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Notes:**\n",
        "\n",
        "- **Summarization Model:**\n",
        "  - The `summarization` pipeline uses a pre-trained model to condense the text.\n",
        "\n",
        "- **Adjusting Parameters:**\n",
        "  - Modify `max_length` and `min_length` to control the length of the final summary.\n",
        "\n",
        "---\n",
        "\n",
        "### Complete Code Example\n",
        "\n",
        "Putting it all together, here's the complete code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer, pipeline\n",
        "\n",
        "# Step 1: Extract Frames from the Video\n",
        "video_file = 'path_to_your_video.mp4'\n",
        "frames_dir = 'extracted_frames'\n",
        "os.makedirs(frames_dir, exist_ok=True)\n",
        "\n",
        "cap = cv2.VideoCapture(video_file)\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_interval = int(fps)  # Extract one frame per second\n",
        "frame_number = 0\n",
        "extracted_frames = []\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if frame_number % frame_interval == 0:\n",
        "        frame_path = os.path.join(frames_dir, f'frame_{frame_number}.jpg')\n",
        "        cv2.imwrite(frame_path, frame)\n",
        "        extracted_frames.append(frame_path)\n",
        "    \n",
        "    frame_number += 1\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(f\"Extracted {len(extracted_frames)} frames.\")\n",
        "\n",
        "# Step 2: Generate Captions for Each Frame\n",
        "model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
        "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def generate_caption(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = pixel_values.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(pixel_values, max_length=50, num_beams=5)\n",
        "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "frame_captions = []\n",
        "\n",
        "for frame_path in extracted_frames:\n",
        "    caption = generate_caption(frame_path)\n",
        "    frame_captions.append({'frame': frame_path, 'caption': caption})\n",
        "    print(f\"{frame_path}: {caption}\")\n",
        "\n",
        "# Step 3: Combine Captions into a Summary\n",
        "summary_text = ' '.join([item['caption'] for item in frame_captions])\n",
        "\n",
        "# Optional: Summarize the Combined Captions\n",
        "summarizer = pipeline('summarization')\n",
        "final_summary = summarizer(summary_text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
        "\n",
        "print(\"\\nFinal Summary:\")\n",
        "print(final_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Additional Considerations\n",
        "\n",
        "- **Installing Dependencies:**\n",
        "\n",
        "  Ensure that you have all the required libraries installed:\n",
        "\n",
        "  ```bash\n",
        "  pip install opencv-python\n",
        "  pip install transformers\n",
        "  pip install torch  # For GPU support, install torch appropriate for your CUDA version\n",
        "  pip install pillow\n",
        "  ```\n",
        "\n",
        "- **Frame Selection:**\n",
        "\n",
        "  - **Dynamic Frame Selection:**\n",
        "    - Instead of extracting frames at fixed intervals, you can implement shot detection or scene change detection to extract frames when the content changes significantly.\n",
        "    - Libraries like `scenedetect` can help with this.\n",
        "\n",
        "- **Processing Time:**\n",
        "\n",
        "  - Image captioning models can be computationally intensive.\n",
        "  - Processing time will increase with the number of frames and the complexity of the model.\n",
        "  - Using a GPU significantly speeds up the process.\n",
        "\n",
        "- **Model Limitations:**\n",
        "\n",
        "  - The quality of the generated captions depends on the model's capabilities.\n",
        "  - Pre-trained models may not accurately describe all types of content.\n",
        "  - For domain-specific videos, consider fine-tuning a model on relevant datasets.\n",
        "\n",
        "- **Enhancing Summarization:**\n",
        "\n",
        "  - **Text Preprocessing:**\n",
        "    - Remove duplicate captions or captions with minimal information before summarization.\n",
        "  - **Semantic Understanding:**\n",
        "    - Use natural language understanding techniques to better combine captions.\n",
        "\n",
        "- **Privacy and Compliance:**\n",
        "\n",
        "  - Be cautious when processing videos containing personal or sensitive information.\n",
        "  - Ensure compliance with data protection regulations.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "By following these steps, you can extract frames from a video without audio, generate descriptions of the visual content using an image captioning model, and combine those descriptions into a textual summary. This approach leverages powerful pre-trained models available through the Hugging Face Transformers library and allows you to create summaries of visual content programmatically.\n",
        "\n",
        "**References:**\n",
        "\n",
        "- **OpenCV Documentation:** [https://docs.opencv.org/](https://docs.opencv.org/)\n",
        "- **Hugging Face Transformers Documentation:** [https://huggingface.co/transformers/](https://huggingface.co/transformers/)\n",
        "- **Image Captioning Models:** [https://huggingface.co/models?pipeline_tag=image-to-text](https://huggingface.co/models?pipeline_tag=image-to-text)\n",
        "- **Scene Detection with PySceneDetect:** [https://pyscenedetect.readthedocs.io/](https://pyscenedetect.readthedocs.io/)\n",
        "\n",
        "---\n",
        "\n",
        "**Example Output:**\n",
        "\n",
        "Assuming we have a video and we run the code, the output might look like:\n",
        "\n",
        "```\n",
        "Extracted 10 frames.\n",
        "extracted_frames/frame_0.jpg: a group of people standing in a room.\n",
        "extracted_frames/frame_30.jpg: a man holding a microphone.\n",
        "extracted_frames/frame_60.jpg: a woman sitting at a desk with a laptop.\n",
        "...\n",
        "Final Summary:\n",
        "A group of people are standing in a room. A man is holding a microphone. A woman is sitting at a desk with a laptop. ...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- Replace `'path_to_your_video.mp4'` with the actual path to your video file.\n",
        "- Adjust the `frame_interval` based on the video's length and desired granularity.\n",
        "- Ensure you have sufficient system resources to run the models, especially if processing longer videos or using higher-resolution frames.\n",
        "\n",
        "---\n",
        "\n",
        "By automating the process of generating textual descriptions from video frames, you can create summaries of videos without audio, which is particularly useful for silent surveillance footage, visual-only content, or scenarios where audio data is unavailable or unusable."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}